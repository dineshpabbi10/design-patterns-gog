{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Gang of Four Design Patterns","text":"<p>Welcome to an interactive guide to design patterns tailored for your domain: microservices, React frontends, Spark data pipelines, and custom Python clients.</p>"},{"location":"#about-this-guide","title":"About This Guide","text":"<p>This repository implements the 23 Gang of Four design patterns with real-world examples from your development experience:</p> <ul> <li>Microservices: Building resilient distributed systems</li> <li>Frontend Integration: React components communicating with backend services</li> <li>Data Pipelines: Spark ETL jobs and orchestration</li> <li>API Clients: Custom Python clients for third-party systems (like <code>ParagoNClient</code>)</li> </ul> <p>Each pattern includes: - \ud83d\udcd6 Detailed explanation and motivation - \ud83d\udcbb Complete, runnable code examples - \ud83c\udfaf Real scenarios from your domain - \u26a1 Practical considerations and trade-offs - \ud83e\uddea Testing strategies</p>"},{"location":"#pattern-categories","title":"Pattern Categories","text":""},{"location":"#creational-patterns","title":"Creational Patterns","text":"<p>Deal with object creation mechanisms. Useful for managing dependencies, abstracting instantiation, and building complex configurations.</p> <ul> <li>Singleton: Single shared instance per process (e.g., <code>ParagoNClient</code> manager)</li> <li>Factory Method: Choose implementation based on configuration</li> <li>Abstract Factory: Create families of related objects</li> <li>Builder: Assemble complex objects step-by-step (e.g., <code>SparkJobBuilder</code>)</li> <li>Prototype: Clone and customize objects rapidly</li> </ul>"},{"location":"#structural-patterns","title":"Structural Patterns","text":"<p>Deal with object composition and how classes/objects relate. Useful for adapting interfaces, adding behavior, and managing hierarchies.</p> <ul> <li>Adapter: Map incompatible interfaces (e.g., ParagoN API \u2192 internal DTOs)</li> <li>Bridge: Decouple abstraction from implementation (e.g., ingestion + storage backends)</li> <li>Composite: Build tree structures of operations</li> <li>Decorator: Add behavior dynamically (e.g., retries, tracing, metrics)</li> <li>Facade: Simplify complex subsystems (e.g., user onboarding)</li> <li>Flyweight: Share immutable objects efficiently (e.g., Spark metadata)</li> <li>Proxy: Control access with policies (e.g., caching, rate-limiting)</li> </ul>"},{"location":"#behavioral-patterns","title":"Behavioral Patterns","text":"<p>Deal with object collaboration and responsibility distribution.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Explore a pattern: Pick one that matches your current challenge</li> <li>Read the problem: Understand the real-world scenario</li> <li>Study the solution: See the complete implementation</li> <li>Try it: Copy, adapt, and use in your code</li> <li>Extend it: Customize for your specific needs</li> </ol>"},{"location":"#example-builder-pattern","title":"Example: Builder Pattern","text":"<p>Looking to simplify Spark job configuration?</p> <pre><code>job_spec = (SparkJobBuilder()\n    .from_s3(\"prod-bucket\", \"data/customers/\")\n    .with_filter(\"status = 'active'\")\n    .with_windowing(\"tumbling\", duration=3600)\n    .with_resources(executors=10, memory_gb=4)\n    .build())\n</code></pre> <p>Learn more about the Builder pattern \u2192</p> <p>Last Updated: December 2025 | Version: 1.0</p>"},{"location":"behavioral/","title":"Behavioral Patterns","text":"<p>Behavioral patterns deal with object collaboration and responsibility distribution. They focus on how objects interact, communicate, and distribute work.</p>"},{"location":"behavioral/#overview","title":"Overview","text":"<p>In microservices, event-driven systems, and orchestration, behavioral patterns help: - Handle requests through chains: Pass requests along a chain of handlers (Chain of Responsibility) - Encapsulate operations: Queue, retry, and undo operations as objects (Command) - Define domain-specific languages: Parse and execute DSL scripts (Interpreter) - Iterate over collections: Traverse elements without exposing structure (Iterator) - Coordinate interactions: Centralize communication between components (Mediator) - Capture state: Save and restore state for undo/replay (Memento) - Notify dependents: React to state changes (Observer) - Alter behavior by state: Change behavior based on internal state (State) - Select algorithms: Switch algorithms at runtime (Strategy) - Define algorithm skeletons: Let subclasses override steps (Template Method) - Apply operations to elements: Perform operations without modifying objects (Visitor)</p>"},{"location":"behavioral/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Chain of Responsibility Pipeline of handlers Event validation \u2192 enrichment \u2192 routing Command Encapsulate operations Queue complex actions for async execution Interpreter Parse domain languages DSL for pipeline definitions Iterator Traverse collections Page through API results seamlessly Mediator Centralize coordination Orchestrate multi-service workflows Memento Capture/restore state Pipeline checkpoints and rollback Observer Notify dependents UI updates on backend state changes State Alter behavior by state Order lifecycle state machine Strategy Select algorithms Pluggable retry/backoff strategies Template Method Define algorithm skeleton ETL job template with customizable steps Visitor Apply operations Transform/validate config trees"},{"location":"behavioral/#when-to-use","title":"When to Use","text":"<p>Choose behavioral patterns when you need to: - Decouple senders from receivers - Encapsulate requests as objects - Define families of algorithms - Change object behavior at runtime - Coordinate complex interactions between components - Provide extensibility without modifying existing code</p>"},{"location":"behavioral/strategy/","title":"Strategy Pattern","text":""},{"location":"behavioral/strategy/#problem","title":"Problem","text":"<p>Your clients must support different retry and backoff strategies depending on the endpoint (idempotent vs non-idempotent) and environment (dev vs prod). Implement a <code>Strategy</code> abstraction for pluggable retry/backoff and serialization strategies used across clients and pipeline connectors.</p>"},{"location":"behavioral/strategy/#solution","title":"Solution","text":"<p>Define an abstract <code>RetryStrategy</code> class with a <code>get_delay</code> method, then implement concrete strategies like <code>FixedIntervalStrategy</code> and <code>ExponentialBackoffWithJitterStrategy</code>. Integrate into a client class like <code>HttpClient</code> for automatic retries.</p> <pre><code>import time\nimport random\nfrom abc import ABC, abstractmethod\n\nclass RetryStrategy(ABC):\n    @abstractmethod\n    def get_delay(self, attempt: int) -&gt; float:\n        \"\"\"Calculate the delay before the next retry based on the attempt number.\"\"\"\n        pass\n\nclass FixedIntervalStrategy(RetryStrategy):\n    def __init__(self, interval: float):\n        self.interval = interval\n\n    def get_delay(self, attempt: int) -&gt; float:\n        return self.interval\n\nclass ExponentialBackoffWithJitterStrategy(RetryStrategy):\n    def __init__(self, base_delay: float, max_delay: float):\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n\n    def get_delay(self, attempt: int) -&gt; float:\n        exp_delay = min(self.base_delay * (2 ** attempt), self.max_delay)\n        jitter = random.uniform(0, exp_delay * 0.1)  # 10% jitter\n        return exp_delay + jitter\n\n\nclass HttpClient:\n    def __init__(self, strategy: RetryStrategy, max_attempts: int):\n        self.strategy = strategy\n        self.max_attempts = max_attempts\n\n    def get(self, url: str):\n        attempt = 0\n        while attempt &lt; self.max_attempts:\n            try:\n                # Simulate an HTTP GET request (replace with actual request logic)\n                print(f\"Attempt {attempt + 1} to GET {url}\")\n                if random.random() &lt; 0.7:  # Simulate a failure 70% of the time\n                    raise Exception(\"Simulated request failure\")\n                print(\"Request succeeded\")\n                return \"Response data\"\n            except Exception as e:\n                print(f\"Request failed: {e}\")\n                attempt += 1\n                if attempt &lt; self.max_attempts:\n                    delay = self.strategy.get_delay(attempt)\n                    print(f\"Retrying in {delay:.2f} seconds...\")\n                    time.sleep(delay)\n                else:\n                    print(\"Max attempts reached. Giving up.\")\n                    raise\n</code></pre>"},{"location":"behavioral/strategy/#key-features","title":"Key Features","text":"<ul> <li>Pluggable: Strategies can be swapped via configuration or runtime.</li> <li>Composable: Strategies can be extended or combined (e.g., add jitter to exponential backoff).</li> <li>Decoupled: Core client logic unchanged; reliability tuned via strategies.</li> </ul>"},{"location":"behavioral/strategy/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Example usage\nif __name__ == \"__main__\":\n    strategy = ExponentialBackoffWithJitterStrategy(base_delay=1.0, max_delay=10.0)\n    client = HttpClient(strategy=strategy, max_attempts=5)\n    try:\n        response = client.get('https://api.example.com/data')\n        print(f\"Received response: {response}\")\n    except Exception as e:\n        print(f\"Final failure: {e}\")\n</code></pre>"},{"location":"behavioral/strategy/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Easy to swap strategies Requires careful design to avoid tight coupling Composable implementations Potential for strategy explosion if not managed Improves reliability without changing core logic Adds abstraction overhead"},{"location":"behavioral/strategy/#testing-tip","title":"Testing Tip","text":"<p>Mock the strategy in unit tests to isolate client behavior.</p> <pre><code>from unittest.mock import Mock\n\n# Mock strategy for testing\nmock_strategy = Mock()\nmock_strategy.get_delay.return_value = 0.1\nclient = HttpClient(strategy=mock_strategy, max_attempts=3)\n# Test client logic without real delays\n</code></pre>"},{"location":"creational/","title":"Creational Patterns","text":"<p>Creational patterns deal with object creation mechanisms, trying to create objects in a way that is suitable to the situation. They abstract the instantiation process.</p>"},{"location":"creational/#overview","title":"Overview","text":"<p>In microservices and data pipelines, creational patterns help: - Manage dependencies: Ensure single instances of clients (Singleton) - Abstract creation logic: Different client types for different environments (Factory Method) - Build complex configurations: Step-by-step assembly of job specs (Builder) - Clone and customize: Rapidly create variations of pipeline configs (Prototype)</p>"},{"location":"creational/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Singleton One global instance per process ParagoNClient manager Factory Method Choose implementation based on config Create prod/test/async clients Abstract Factory Create families of related objects Ecosystem-specific client sets Builder Assemble complex objects step-by-step SparkJobBuilder Prototype Clone and customize objects rapidly Pipeline spec cloning"},{"location":"creational/#when-to-use","title":"When to Use","text":"<p>Choose creational patterns when you need to: - Decouple object creation from your business logic - Support multiple ways to construct similar objects - Enforce constraints on how objects are instantiated - Improve testability through dependency injection</p>"},{"location":"creational/abstract_factory/","title":"Abstract Factory Pattern","text":""},{"location":"creational/abstract_factory/#problem","title":"Problem","text":"<p>Your platform integrates with multiple third-party ecosystems (ParagoN, another API provider). Each provider requires a set of cooperating client objects:</p> <ul> <li>ParagoN ecosystem: <code>AuthClient</code>, <code>DataClient</code>, <code>WebhookClient</code></li> <li>Another API: Different implementations of the same interfaces but different protocols/auth</li> </ul> <p>Without Abstract Factory, you'd end up with messy conditional logic everywhere:</p> <pre><code># \u274c Messy without pattern\nif provider == \"paragon\":\n    auth = ParagoNAuthClient()\n    data = ParagoNDataClient()\n    webhook = ParagoNWebhookClient()\nelif provider == \"another\":\n    auth = AnotherAuthClient()\n    data = AnotherDataClient()\n    webhook = AnotherWebhookClient()\n</code></pre>"},{"location":"creational/abstract_factory/#solution","title":"Solution","text":"<p>Define abstract interfaces and provider-specific factories:</p> <pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\n\nclass Provider(Enum):\n    PARAGON = \"paragon\"\n    ANOTHER_API = \"another_api\"\n    MOCK = \"mock\"\n\nclass BaseAuthClient(ABC):\n    @abstractmethod\n    def authenticate(self, credentials: dict):\n        raise NotImplementedError\n\nclass BaseDataClient(ABC):\n    @abstractmethod\n    def fetch_data(self, query: str) -&gt; dict:\n        raise NotImplementedError\n\nclass BaseWebhookClient(ABC):\n    @abstractmethod\n    def send_webhook(self, payload: dict) -&gt; None:\n        raise NotImplementedError\n\nclass BaseClientFactory(ABC):\n    @abstractmethod\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_data_client(self) -&gt; BaseDataClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        raise NotImplementedError\n\n# ParagoN implementations\nclass ParagoNAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass ParagoNDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass ParagoNWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass ParagoNClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return ParagoNAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return ParagoNDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return ParagoNWebhookClient()\n\n# Mock implementations for testing\nclass MockAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass MockDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass MockWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass MockClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return MockAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return MockDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return MockWebhookClient()\n\ndef load_factory(provider_name: Provider) -&gt; BaseClientFactory:\n    if provider_name == Provider.PARAGON:\n        return ParagoNClientFactory()\n    if provider_name == Provider.MOCK:\n        return MockClientFactory()\n    raise ValueError(f\"Unknown provider: {provider_name.value}\")\n</code></pre>"},{"location":"creational/abstract_factory/#usage","title":"Usage","text":"<pre><code># Configuration-driven ecosystem setup\nprovider = Provider.PARAGON\nfactory = load_factory(provider)\n\n# Create a coordinated set of clients\nauth_client = factory.create_auth_client()\ndata_client = factory.create_data_client()\nwebhook_client = factory.create_webhook_client()\n\n# All three work together seamlessly\nauth_client.authenticate({\"api_key\": \"...\"})\nwebhook_client.send_webhook({\"event\": \"user_created\"})\ndata = data_client.fetch_data(\"SELECT * FROM users\")\n\n# Easy to switch to testing\ntest_factory = load_factory(Provider.MOCK)\ntest_auth = test_factory.create_auth_client()\ntest_data = test_factory.create_data_client()\n</code></pre>"},{"location":"creational/abstract_factory/#benefits","title":"Benefits","text":"<ul> <li>Consistency: All clients from one factory are compatible</li> <li>Easy provider switching: Change config, not code</li> <li>Testability: Create a mock factory for integration tests</li> <li>Extensibility: Adding a new provider requires only one new factory class</li> </ul>"},{"location":"creational/abstract_factory/#testing-example","title":"Testing Example","text":"<pre><code>def test_ecosystem_integration():\n    # Use mock factory for testing\n    factory = load_factory(Provider.MOCK)\n    auth_client = factory.create_auth_client()\n    data_client = factory.create_data_client()\n    webhook_client = factory.create_webhook_client()\n\n    # All clients are now mocked\n    auth_client.authenticate({})\n    webhook_client.send_webhook({})\n    data = data_client.fetch_data(\"\")\n</code></pre>"},{"location":"creational/builder/","title":"Builder Pattern","text":""},{"location":"creational/builder/#problem","title":"Problem","text":"<p>Constructing a complex Spark job configuration requires many optional parameters: - Input sources (S3, Kafka, database) - Transforms (filters, maps, joins) - Windowing and triggers - Resource settings (executors, memory) - Monitoring hooks</p> <p>Without a builder, you'd either: 1. Use a giant constructor with 20 parameters 2. Pass a loose dictionary around 3. Use <code>**kwargs</code> and lose type safety</p> <pre><code># \u274c Messy constructor\njob = SparkJob(\n    source, format, schema_path,\n    filter_fn, map_fn, join_key,\n    window_type, window_duration, trigger_interval,\n    executor_count, executor_memory, num_cores,\n    shuffle_partitions, log_level, metrics_enabled,\n    # ... and 10 more parameters\n)\n</code></pre>"},{"location":"creational/builder/#solution","title":"Solution","text":"<p>Use a builder with fluent chaining to assemble the spec step-by-step:</p> <pre><code>from copy import deepcopy\n\nclass SparkJobSpec:\n    __slots__ = (\n        \"input_source\",\n        \"transforms\",\n        \"windowing\",\n        \"triggers\",\n        \"resources\",\n        \"monitoring_hooks\",\n        \"spec_version\",\n    )\n\n    def __init__(self, *, input_source, transforms, windowing, triggers, resources, monitoring_hooks, spec_version=\"1.0\"):\n        object.__setattr__(self, \"input_source\", input_source)\n        object.__setattr__(self, \"transforms\", tuple(transforms))\n        object.__setattr__(self, \"windowing\", windowing)\n        object.__setattr__(self, \"triggers\", triggers)\n        object.__setattr__(self, \"resources\", deepcopy(resources))\n        object.__setattr__(self, \"monitoring_hooks\", tuple(monitoring_hooks))\n        object.__setattr__(self, \"spec_version\", spec_version)\n\n    def __setattr__(self, key, value):\n        if hasattr(self, key):\n            raise AttributeError(\"Cannot modify immutable SparkJobSpec\")\n        super().__setattr__(key, value)\n\n    def __delattr__(self, key):\n        raise AttributeError(\"SparkJobSpec is immutable\")\n\n    def serialize(self):\n        return {\n            \"input_source\": self.input_source,\n            \"transforms\": self.transforms,\n            \"windowing\": self.windowing,\n            \"triggers\": self.triggers,\n            \"resources\": self.resources,\n            \"monitoring_hooks\": self.monitoring_hooks,\n            \"spec_version\": self.spec_version,\n        }\n\nclass SparkJobBuilder:\n    def __init__(self):\n        self._input_source = None\n        self._transforms = []\n        self._windowing = None\n        self._triggers = None\n        self._resources = {}\n        self._monitoring_hooks = []\n        self._spec_version = \"1.0\"\n\n    def input_source(self, source):\n        self._input_source = source\n        return self\n\n    def set_transforms(self, transforms):\n        self._transforms = transforms\n        return self\n\n    def windowing(self, windowing):\n        self._windowing = windowing\n        return self\n\n    def triggers(self, triggers):\n        self._triggers = triggers\n        return self\n\n    def resources(self, resources):\n        self._resources = resources\n        return self\n\n    def monitoring_hooks(self, hooks):\n        self._monitoring_hooks = hooks\n        return self\n\n    def spec_version(self, version):\n        self._spec_version = version\n        return self\n\n    def build(self):\n        return SparkJobSpec(\n            input_source=self._input_source,\n            transforms=self._transforms,\n            windowing=self._windowing,\n            triggers=self._triggers,\n            resources=self._resources,\n            monitoring_hooks=self._monitoring_hooks,\n            spec_version=self._spec_version\n        )\n</code></pre>"},{"location":"creational/builder/#usage-examples","title":"Usage Examples","text":"<pre><code># Build a job spec fluently\nbuilder = SparkJobBuilder()\njob_spec = (builder\n    .input_source({\"type\": \"s3\", \"bucket\": \"prod-bucket\", \"path\": \"data/\"})\n    .set_transforms([\n        {\"type\": \"filter\", \"condition\": \"status = 'active'\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"}\n    ])\n    .windowing({\"type\": \"tumbling\", \"duration\": 3600})\n    .triggers({\"type\": \"micro-batch\", \"interval\": 60})\n    .resources({\"executor_count\": 10, \"executor_memory_gb\": 4})\n    .monitoring_hooks([{\"type\": \"prometheus\"}])\n    .build())\n\n# Serialize for storage\nconfig = job_spec.serialize()\n\n# Immutability guaranteed\ntry:\n    job_spec.input_source = \"new_value\"  # Raises AttributeError\nexcept AttributeError as e:\n    print(f\"Cannot modify: {e}\")\n</code></pre>"},{"location":"creational/builder/#advantages","title":"Advantages","text":"Pros Cons Readable fluent interface Extra classes/code Type-safe method signatures Slightly more overhead Immutable final spec Not suitable for tiny objects Easy to extend with new methods Requires careful implementation Versioning built-in"},{"location":"creational/builder/#advanced-builder-cloning","title":"Advanced: Builder Cloning","text":"<pre><code># Clone an existing job and modify\nmodified_job = (SparkJobBuilder()\n    # Copy settings from existing spec\n    # then override specific fields\n    .from_s3(\"test-bucket\", \"data/customers/\")\n    .with_resources(executors=2)  # Fewer resources for testing\n    .build())\n</code></pre>"},{"location":"creational/factory_method/","title":"Factory Method Pattern","text":""},{"location":"creational/factory_method/#problem","title":"Problem","text":"<p>Your system needs to create API clients for different runtime contexts: - Production: Real <code>ParagoNClient</code> connecting to live API - Testing: Mock client returning fixtures - Async workers: Async-aware client variant</p> <p>Without a factory, you'd hardcode client selection scattered throughout your code. The Factory Method centralizes this logic.</p>"},{"location":"creational/factory_method/#solution","title":"Solution","text":"<p>Define a factory method that returns the appropriate client based on configuration:</p> <pre><code>from enum import Enum\nfrom typing import Union\n\nclass ClientType(Enum):\n    PRODUCTION = \"production\"\n    TESTING = \"testing\"\n    ASYNC = \"async\"\n\nclass HttpMethod(Enum):\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n\nclass ParagonClientConfig:\n    def __init__(self, client_type: ClientType, client_id: str = None, client_secret: str = None):\n        self.client_type = client_type\n        self.client_id = client_id or \"default_id\"\n        self.client_secret = client_secret or \"default_secret\"\n\nclass BaseClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    def request(self, method, url, headers, data, query_params):\n        raise NotImplementedError\n\nclass BaseAsyncClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        raise NotImplementedError\n\nclass ParagonSyncClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"SyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonAsyncClient(BaseAsyncClient):\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"AsyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonMockClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"MockClient: {method} {url} with {data} and {query_params}\"\n\ndef create_client(config: ParagonClientConfig) -&gt; Union[BaseClient, BaseAsyncClient]:\n    if config.client_type == ClientType.PRODUCTION:\n        return ParagonSyncClient(config)\n    elif config.client_type == ClientType.TESTING:\n        return ParagonMockClient(config)\n    elif config.client_type == ClientType.ASYNC:\n        return ParagonAsyncClient(config)\n    else:\n        raise ValueError(f\"Unknown client type: {config.client_type}\")\n</code></pre>"},{"location":"creational/factory_method/#usage","title":"Usage","text":"<pre><code># Configuration-driven client creation\nconfig_prod = ParagonClientConfig(ClientType.PRODUCTION, client_id=\"prod_123\")\nprod_client = create_client(config_prod)\n\nconfig_test = ParagonClientConfig(ClientType.TESTING)\ntest_client = create_client(config_test)\n\nconfig_async = ParagonClientConfig(ClientType.ASYNC, client_id=\"async_123\")\nasync_client = create_client(config_async)\n\n# Use the appropriate client\nresult_prod = prod_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\nresult_test = test_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\n</code></pre>"},{"location":"creational/factory_method/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Decouples client selection from usage Extra indirection for simple cases Easy to add new client types Factory method becomes complex with many types Testability: swap with mock effortlessly String-based selection is fragile Configuration-driven behavior"},{"location":"creational/factory_method/#advanced-factory-with-config-objects","title":"Advanced: Factory with Config Objects","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ClientConfig:\n    env: str\n    api_key: str = None\n    timeout: int = 30\n    retries: int = 3\n\ndef create_client_from_config(config: ClientConfig) -&gt; ClientBase:\n    if config.env == \"prod\":\n        return ParagoNClientProd(api_key=config.api_key, \n                                 timeout=config.timeout, \n                                 retries=config.retries)\n    elif config.env == \"test\":\n        return ParagoNClientMock()\n    # ... etc\n</code></pre>"},{"location":"creational/prototype/","title":"Prototype Pattern","text":""},{"location":"creational/prototype/#problem","title":"Problem","text":"<p>In your ETL orchestration, you often reuse pipeline configurations with minor variations:</p> <pre><code># Base template: daily customer data pipeline\nbase_config = {\n    \"name\": \"customer_daily\",\n    \"source\": {\"s3\": \"s3://prod-data/customers/\"},\n    \"transforms\": [\n        {\"type\": \"filter\", \"condition\": \"is_active=true\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"},\n        {\"type\": \"aggregate\", \"window\": \"1d\"},\n    ],\n    \"resources\": {\"executors\": 10, \"memory_gb\": 4},\n}\n</code></pre> <p>Now you need variations for: - Testing: Same config but with test data and fewer resources - Weekly rollup: Different transforms and schedule - A/B test variant: Modified enrichment logic</p> <p>Without Prototype, you rebuild each from scratch, leading to duplication and inconsistency.</p>"},{"location":"creational/prototype/#solution","title":"Solution","text":"<p>Implement a cloning mechanism with provenance tracking:</p> <pre><code>from copy import deepcopy, copy\n\nclass PipelineSpec:\n    \"\"\"Pipeline specification with cloning and provenance.\"\"\"\n\n    def __init__(self, *, name: str, input_source: str, \n                 transforms: list, resources: dict, \n                 metadata: dict = None):\n        self.name = name\n        self.input_source = input_source\n        self.transforms = transforms\n        self.resources = resources\n        self.metadata = metadata or {}\n\n    def clone(self, deep: bool = True, **overrides):\n        \"\"\"Clone the spec, optionally overriding fields.\n\n        Args:\n            deep: If True, recursively copy nested structures\n            **overrides: Fields to override in the cloned spec\n\n        Returns:\n            PipelineSpec: New independent instance\n        \"\"\"\n        if deep:\n            # Deep copy: all nested structures are independent\n            cloned_transforms = deepcopy(self.transforms)\n            cloned_resources = deepcopy(self.resources)\n            cloned_metadata = deepcopy(self.metadata)\n        else:\n            # Shallow copy: nested objects are shared\n            cloned_transforms = copy(self.transforms)\n            cloned_resources = copy(self.resources)\n            cloned_metadata = copy(self.metadata)\n\n        # Apply overrides\n        for key, value in overrides.items():\n            if key == \"transforms\":\n                cloned_transforms = value\n            elif key == \"resources\":\n                cloned_resources = value\n            elif key == \"metadata\":\n                cloned_metadata = value\n\n        # Create new spec\n        new_spec = PipelineSpec(\n            name=overrides.get(\"name\", self.name),\n            input_source=overrides.get(\"input_source\", self.input_source),\n            transforms=cloned_transforms,\n            resources=cloned_resources,\n            metadata=cloned_metadata,\n        )\n\n        # Update provenance\n        new_spec.metadata['cloned_from'] = self.name\n\n        return new_spec\n</code></pre>"},{"location":"creational/prototype/#usage-examples","title":"Usage Examples","text":""},{"location":"creational/prototype/#basic-cloning-with-deep-copy","title":"Basic Cloning with Deep Copy","text":"<pre><code># Create a base pipeline spec\nbase_spec = PipelineSpec(\n    name=\"base_etl\",\n    input_source=\"s3://input/data\",\n    transforms=[\"parse\", \"clean\", \"enrich\"],\n    resources={\"executor_memory\": \"4G\", \"partitions\": 100},\n)\n\n# Clone it for a new use case (deep copy by default)\n# All nested structures are independent\ntest_spec = base_spec.clone(\n    name=\"test_etl\",\n    input_source=\"s3://test/data\",\n)\n\n# Modify test_spec's transforms - does NOT affect base_spec\ntest_spec.transforms.append(\"validate\")\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Still [\"parse\", \"clean\", \"enrich\"]\nprint(f\"test_spec transforms: {test_spec.transforms}\")  # [\"parse\", \"clean\", \"enrich\", \"validate\"]\nprint(f\"test_spec provenance: {test_spec.metadata.get('cloned_from')}\")  # \"base_etl\"\n</code></pre>"},{"location":"creational/prototype/#shallow-copy-for-shared-resources","title":"Shallow Copy for Shared Resources","text":"<pre><code># When you want nested structures to share references\n# (e.g., shared resource pool configuration)\nshared_spec = base_spec.clone(deep=False)\nshared_spec.resources[\"executor_memory\"] = \"8G\"\n\n# With shallow copy, the list reference is shared\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Changes are reflected\n</code></pre>"},{"location":"creational/prototype/#templating-with-overrides","title":"Templating with Overrides","text":"<pre><code># Template for different environments\nprod_spec = base_spec.clone(\n    name=\"prod_etl\",\n    input_source=\"s3://prod/data\",\n    resources={\"executor_memory\": \"16G\", \"partitions\": 500},\n)\n\n# Create a debug variant\ndebug_spec = prod_spec.clone(\n    name=\"prod_etl_debug\",\n    transforms=[\"parse\", \"clean\"],  # Skip expensive enrich\n    resources={\"executor_memory\": \"2G\", \"partitions\": 10},\n)\n</code></pre>"},{"location":"creational/prototype/#shallow-vs-deep-clone","title":"Shallow vs Deep Clone","text":"Shallow Copy Deep Copy <code>cloned_transforms = copy(self.transforms)</code> <code>cloned_transforms = deepcopy(self.transforms)</code> New list, but items still shared Completely independent copy Faster, less memory Safer, prevents accidental mutations Use when: only changing top-level fields Use when: modifying nested structures <pre><code># Shallow clone example\nbase_transforms = [{\"type\": \"filter\"}]\nclone1 = copy(base_transforms)\nclone1[0][\"condition\"] = \"active=true\"\n# base_transforms ALSO changed! \u26a0\ufe0f\n\n# Deep clone example\nclone2 = deepcopy(base_transforms)\nclone2[0][\"condition\"] = \"active=true\"\n# base_transforms unchanged \u2713\n</code></pre>"},{"location":"creational/prototype/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Avoid duplication of complex configs Deep copying can be slow/memory-intensive Track config evolution via provenance Extra book-keeping required Support templates and variants Shallow copy gotchas if not careful Faster than manual rebuilding Complex graphs may need custom cloning"},{"location":"creational/singleton/","title":"Singleton Pattern","text":""},{"location":"creational/singleton/#problem","title":"Problem","text":"<p>When you run microservices with multiple threads or async workers, you need a single, shared instance of expensive resources like database connections or API clients. The <code>ParagoNClientManager</code> singleton ensures all threads in a process use the same cached client instance, avoiding redundant credentials and connection overhead.</p>"},{"location":"creational/singleton/#solution","title":"Solution","text":"<p>Create a class that enforces single instantiation:</p> <pre><code>from threading import Lock\n\nclass ParagonNSingleton:\n    _instance = None\n    _lock = Lock()\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.token = 0\n\n    def _fetch_token(self) -&gt; int:\n        # Simulate fetching a token using the API key\n        with self._lock:\n            self.token += 1\n            return self.token\n\n    def refresh_token(self):\n        self.token = self._fetch_token()\n        return self.token\n\n\nclass ParagonNSingletonManager:\n    _instance = None \n    _lock = Lock()\n\n    def __new__(cls, *args, **kwargs):\n        raise NotImplementedError(\"Use get_instance() method to get the singleton instance.\")\n\n    @classmethod \n    def get_instance(cls, api_key: str) -&gt; \"ParagonNSingletonManager\":\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(ParagonNSingletonManager)\n                    cls._instance.client = ParagonNSingleton(api_key)\n        return cls._instance\n\n    @classmethod\n    def get_client(cls, api_key: str) -&gt; ParagonNSingleton:\n        instance = cls.get_instance(api_key)\n        return instance.client\n</code></pre>"},{"location":"creational/singleton/#key-features","title":"Key Features","text":"<ul> <li>Thread-safe: Uses double-checked locking to prevent race conditions</li> <li>Lazy initialization: Client created only on first use</li> <li>Global access: <code>ParagoNClientManager().get_client()</code> from anywhere</li> <li>Credentials managed: API keys loaded once per process</li> </ul>"},{"location":"creational/singleton/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Get singleton client instance\na = ParagonNSingletonManager.get_client(\"my_api_key\")\nb = ParagonNSingletonManager.get_client(\"my_api_key\")\n\nprint(a is b)  # True - both are the same instance\n\n# Thread-safe access\ndef access_client():\n    client = ParagonNSingletonManager.get_client(\"my_api_key\")\n    client.refresh_token()\n    print(f\"Token: {client.token}\")\n\nimport threading\nthreads = [threading.Thread(target=access_client) for _ in range(5)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"creational/singleton/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Single shared instance reduces overhead Hard to test (global state) Thread-safe token refresh Difficult to mock in unit tests Credentials loaded once Tight coupling to singleton Easy global access Hidden dependencies"},{"location":"creational/singleton/#testing-tip","title":"Testing Tip","text":"<p>For unit tests, consider a test-friendly alternative: use dependency injection or a registry pattern instead of a pure singleton.</p> <pre><code># Testable version\nclient_manager = ParagoNClientManager()\n# Pass to handlers as dependency\ndef handle_user_request(user_id, client_manager=client_manager):\n    client = client_manager.get_client()\n    # ...\n</code></pre>"},{"location":"structural/","title":"Structural Patterns","text":"<p>Structural patterns deal with object composition, creating larger structures from classes and objects through inheritance and composition.</p>"},{"location":"structural/#overview","title":"Overview","text":"<p>In microservices, React integrations, and data pipelines, structural patterns help: - Adapt incompatible interfaces: Hide third-party API quirks (Adapter) - Decouple abstraction from implementation: Support multiple backends (Bridge) - Compose hierarchies: Build tree structures of operations (Composite) - Add behavior dynamically: Wrap objects with cross-cutting concerns (Decorator) - Simplify complex subsystems: Provide unified high-level interfaces (Facade) - Share expensive objects: Reduce memory footprint of large datasets (Flyweight) - Control access: Add layers like caching and rate-limiting (Proxy)</p>"},{"location":"structural/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Adapter Make incompatible interfaces work together Map ParagoN API to internal User DTO Bridge Decouple abstraction from implementation Ingestion logic + multiple storage backends Composite Treat individual and composite objects uniformly Tree of bulk operations across services Decorator Add responsibilities dynamically Wrap clients with retry/tracing/metrics Facade Provide simplified interface to complex subsystem Onboarding orchestration Flyweight Share fine-grained objects efficiently Shared metadata across Spark tasks Proxy Control access to another object Client wrapper with caching + rate-limiting"},{"location":"structural/#when-to-use","title":"When to Use","text":"<p>Choose structural patterns when you need to: - Work with external systems that have incompatible interfaces - Support multiple implementations transparently - Build flexible hierarchies of objects - Reduce memory usage for large numbers of similar objects - Add or modify behavior without changing class definitions</p>"},{"location":"structural/adapter/","title":"Adapter Pattern","text":""},{"location":"structural/adapter/#problem","title":"Problem","text":"<p>Your microservices and React frontend rely on consistent internal data models (DTOs). However, the ParagoN API (which you interact with via <code>ParagoNClient</code>) returns deeply nested JSON with inconsistent field names, optional fields, and different data types:</p>"},{"location":"structural/adapter/#paragon-api-response","title":"ParagoN API Response","text":"<pre><code>{\n  \"user_id\": \"12345\",\n  \"personal_info\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"contact\": {\n      \"email_addr\": \"john.doe@example.com\",\n      \"phone_num\": \"+1234567890\"\n    }\n  },\n  \"account_status\": \"ACTIVE\",\n  \"created_at\": \"2023-10-01T12:00:00Z\",\n  \"metadata\": {\n    \"tags\": [\"premium\", \"verified\"],\n    \"preferences\": {\"notifications\": true}\n  }\n}\n</code></pre>"},{"location":"structural/adapter/#your-internal-user-model","title":"Your Internal User Model","text":"<pre><code>{\n  \"id\": \"12345\",\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"phone\": \"+1234567890\",\n  \"status\": \"active\",\n  \"createdAt\": \"2023-10-01T12:00:00Z\",\n  \"tags\": [\"premium\", \"verified\"],\n  \"preferences\": {\"notifications\": true}\n}\n</code></pre> <p>Discrepancies: - Field name variations (<code>user_id</code> \u2192 <code>id</code>, <code>email_addr</code> \u2192 <code>email</code>, <code>account_status</code> \u2192 <code>status</code>) - Nested structure flattening - Type conversions (string status to lowercase) - Optional fields may be missing</p>"},{"location":"structural/adapter/#solution","title":"Solution","text":"<p>Implement bidirectional adapters that hide the complexity:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseAdapterModel(ABC):\n    \"\"\"Abstract adapter interface.\"\"\"\n\n    def __init__(self, external_data: dict):\n        self.external_data = external_data\n\n    @abstractmethod\n    def to_internal(self) -&gt; dict:\n        \"\"\"Convert external API response to internal model.\"\"\"\n        raise NotImplementedError(\"to_internal method not implemented\")\n\n    @abstractmethod\n    def to_external(self) -&gt; dict:\n        \"\"\"Convert internal model back to external API format.\"\"\"\n        raise NotImplementedError(\"to_external method not implemented\")\n\n\nclass ParagoNUserAdapter(BaseAdapterModel):\n    \"\"\"Maps ParagoN API user responses to internal User DTOs.\"\"\"\n\n    def to_internal(self) -&gt; dict:\n        \"\"\"Flatten ParagoN's nested response to internal User model.\"\"\"\n        data = self.external_data\n        internal_data = {\n            \"id\": data.get(\"user_id\"),\n            \"firstName\": data.get(\"personal_info\", {}).get(\"firstName\"),\n            \"lastName\": data.get(\"personal_info\", {}).get(\"lastName\"),\n            \"email\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"email_addr\"),\n            \"phone\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"phone_num\"),\n            \"status\": data.get(\"account_status\", \"\").lower(),\n            \"createdAt\": data.get(\"created_at\"),\n            \"tags\": data.get(\"metadata\", {}).get(\"tags\", []),\n            \"preferences\": data.get(\"metadata\", {}).get(\"preferences\", {}),\n        }\n        return internal_data\n\n    def to_external(self) -&gt; dict:\n        \"\"\"Map internal User model back to ParagoN format.\"\"\"\n        internal_data = self.external_data\n        external_data = {\n            \"user_id\": internal_data.get(\"id\"),\n            \"personal_info\": {\n                \"firstName\": internal_data.get(\"firstName\"),\n                \"lastName\": internal_data.get(\"lastName\"),\n                \"contact\": {\n                    \"email_addr\": internal_data.get(\"email\"),\n                    \"phone_num\": internal_data.get(\"phone\"),\n                },\n            },\n            \"account_status\": internal_data.get(\"status\", \"\").upper(),\n            \"created_at\": internal_data.get(\"createdAt\"),\n            \"metadata\": {\n                \"tags\": internal_data.get(\"tags\", []),\n                \"preferences\": internal_data.get(\"preferences\", {}),\n            },\n        }\n        return external_data\n</code></pre>"},{"location":"structural/adapter/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/adapter/#basic-adaptation-paragon-response-to-internal-model","title":"Basic Adaptation: ParagoN Response to Internal Model","text":"<pre><code># Receive data from ParagoN API\nparagon_response = {\n    \"user_id\": \"12345\",\n    \"personal_info\": {\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"contact\": {\n            \"email_addr\": \"john@example.com\",\n            \"phone_num\": \"+1234567890\"\n        }\n    },\n    \"account_status\": \"ACTIVE\",\n    \"created_at\": \"2023-10-01T12:00:00Z\",\n    \"metadata\": {\n        \"tags\": [\"premium\", \"verified\"],\n        \"preferences\": {\"notifications\": True}\n    }\n}\n\n# Adapt to internal model\nadapter = ParagoNUserAdapter(paragon_response)\ninternal_user = adapter.to_internal()\n\nprint(internal_user)\n# {\n#     \"id\": \"12345\",\n#     \"firstName\": \"John\",\n#     \"lastName\": \"Doe\",\n#     \"email\": \"john@example.com\",\n#     \"phone\": \"+1234567890\",\n#     \"status\": \"active\",  # lowercased\n#     \"createdAt\": \"2023-10-01T12:00:00Z\",\n#     \"tags\": [\"premium\", \"verified\"],\n#     \"preferences\": {\"notifications\": True}\n# }\n</code></pre>"},{"location":"structural/adapter/#reverse-adaptation-internal-model-to-paragon-format","title":"Reverse Adaptation: Internal Model to ParagoN Format","text":"<pre><code># Your internal user model (from React or database)\ninternal_user = {\n    \"id\": \"12345\",\n    \"firstName\": \"Jane\",\n    \"lastName\": \"Smith\",\n    \"email\": \"jane@example.com\",\n    \"phone\": \"+9876543210\",\n    \"status\": \"active\",\n    \"createdAt\": \"2023-10-01T12:00:00Z\",\n    \"tags\": [\"vip\"],\n    \"preferences\": {\"notifications\": False}\n}\n\n# Adapt back to ParagoN format for API request\nadapter = ParagoNUserAdapter(internal_user)\nparagon_format = adapter.to_external()\n\nprint(paragon_format)\n# {\n#     \"user_id\": \"12345\",\n#     \"personal_info\": {\n#         \"firstName\": \"Jane\",\n#         \"lastName\": \"Smith\",\n#         \"contact\": {\n#             \"email_addr\": \"jane@example.com\",\n#             \"phone_num\": \"+9876543210\"\n#         }\n#     },\n#     \"account_status\": \"ACTIVE\",  # uppercased\n#     \"created_at\": \"2023-10-01T12:00:00Z\",\n#     \"metadata\": {\n#         \"tags\": [\"vip\"],\n#         \"preferences\": {\"notifications\": False}\n#     }\n# }\n</code></pre>"},{"location":"structural/adapter/#handling-missing-fields","title":"Handling Missing Fields","text":"<pre><code># Incomplete ParagoN response (missing optional fields)\nincomplete_response = {\n    \"user_id\": \"99999\",\n    \"personal_info\": {\n        \"firstName\": \"Bob\"\n        # No lastName, contact info, etc.\n    },\n    \"account_status\": \"PENDING\"\n    # No created_at, metadata\n}\n\n# Adapter safely handles missing nested fields\nadapter = ParagoNUserAdapter(incomplete_response)\ninternal = adapter.to_internal()\n\nprint(internal)\n# {\n#     \"id\": \"99999\",\n#     \"firstName\": \"Bob\",\n#     \"lastName\": None,  # Missing field\n#     \"email\": None,\n#     \"phone\": None,\n#     \"status\": \"pending\",\n#     \"createdAt\": None,\n#     \"tags\": [],  # Default empty list\n#     \"preferences\": {}  # Default empty dict\n# }\n</code></pre>"},{"location":"structural/adapter/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Isolates your code from API changes Extra layer of indirection Type-safe internal models Maintenance overhead for each new API Easy to test and mock May add performance overhead Composable for complex transformations Debugging can be harder Consistent data contracts"},{"location":"structural/adapter/#performance-considerations","title":"Performance Considerations","text":"<p>For high-throughput scenarios: - Cache adapter instances if stateless - Minimize deep copying - Consider using <code>__slots__</code> for adapter classes - Profile serialization/deserialization overhead</p>"},{"location":"structural/bridge/","title":"Bridge Pattern","text":""},{"location":"structural/bridge/#problem","title":"Problem","text":"<p>Your ingestion logic must support multiple storage backends: - Production: S3 for large-scale data storage - Testing/Dev: Local filesystem for quick iteration - Multi-cloud: GCS for Google Cloud deployments</p> <p>Without Bridge, you'd have conditional logic sprinkled everywhere:</p> <pre><code># \u274c Tight coupling to specific backends\ndef ingest_data(backend_type, data):\n    if backend_type == \"s3\":\n        s3 = boto3.client(\"s3\")\n        s3.put_object(Bucket=\"my-bucket\", Key=\"data\", Body=data)\n    elif backend_type == \"local\":\n        with open(\"/data/local\", \"wb\") as f:\n            f.write(data)\n    elif backend_type == \"gcs\":\n        gcs = storage.Client()\n        bucket = gcs.bucket(\"my-bucket\")\n        blob = bucket.blob(\"data\")\n        blob.upload_from_string(data)\n</code></pre> <p>Problem: Ingestion logic is tightly coupled to storage implementation.</p>"},{"location":"structural/bridge/#solution","title":"Solution","text":"<p>Implement the bridge by separating the abstraction (ingestion logic) from implementations (storage backends):</p> <pre><code>from abc import ABC, abstractmethod\nfrom threading import Lock\n\n# Implementation interface - storage backends\nclass StorageBackend(ABC):\n    \"\"\"Abstract interface for storage operations.\"\"\"\n\n    @abstractmethod\n    def write(self, key: str, data: bytes):\n        \"\"\"Write data (idempotent - skips if exists).\"\"\"\n        raise NotImplementedError(\"write method not implemented\")\n\n    @abstractmethod\n    def update(self, key: str, data: bytes):\n        \"\"\"Update existing data (raises KeyError if not exists).\"\"\"\n        raise NotImplementedError(\"update method not implemented\")\n\n    @abstractmethod\n    def read(self, key: str) -&gt; bytes:\n        \"\"\"Read data from the backend.\"\"\"\n        raise NotImplementedError(\"read method not implemented\")\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if data exists in the backend.\"\"\"\n        raise NotImplementedError(\"exists method not implemented\")\n\n    @abstractmethod\n    def delete(self, key: str):\n        \"\"\"Delete data from the backend.\"\"\"\n        raise NotImplementedError(\"delete method not implemented\")\n\n\n# Concrete Implementation: S3\nclass S3Storage(StorageBackend):\n    \"\"\"AWS S3 storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        import boto3\n        self.s3_client = boto3.client(\"s3\")\n        self.bucket_name = bucket_name\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                print(f\"Data with key {key} already exists in S3. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in S3. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n        return response['Body'].read()\n\n    def exists(self, key: str) -&gt; bool:\n        try:\n            self.s3_client.head_object(Bucket=self.bucket_name, Key=key)\n            return True\n        except self.s3_client.exceptions.NoSuchKey:\n            return False\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.delete_object(Bucket=self.bucket_name, Key=key)\n\n\n# Concrete Implementation: Local Filesystem\nclass LocalStorage(StorageBackend):\n    \"\"\"Local filesystem storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, base_path: str):\n        import os\n        self.base_path = base_path\n        os.makedirs(base_path, exist_ok=True)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                print(f\"Data with key {key} already exists in Local Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in Local Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        with open(f\"{self.base_path}/{key}\", \"rb\") as f:\n            return f.read()\n\n    def exists(self, key: str) -&gt; bool:\n        import os\n        return os.path.exists(f\"{self.base_path}/{key}\")\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                import os\n                os.remove(f\"{self.base_path}/{key}\")\n\n\n# Concrete Implementation: Google Cloud Storage\nclass GCSStorage(StorageBackend):\n    \"\"\"Google Cloud Storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        from google.cloud import storage\n        self.client = storage.Client()\n        self.bucket = self.client.bucket(bucket_name)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                print(f\"Data with key {key} already exists in GCS Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in GCS Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        blob = self.bucket.blob(key)\n        return blob.download_as_bytes()\n\n    def exists(self, key: str) -&gt; bool:\n        blob = self.bucket.blob(key)\n        return blob.exists()\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.delete()\n\n\n# Concrete Implementation: In-memory Mock (for testing)\nclass MockStorage(StorageBackend):\n    \"\"\"In-memory mock storage for testing.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self):\n        self.storage = {}\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.storage[key] = data\n            else:\n                print(f\"Data with key {key} already exists in Mock Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.storage[key] = data\n            else:\n                raise KeyError(f\"Key {key} does not exist in Mock Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        return self.storage[key]\n\n    def exists(self, key: str) -&gt; bool:\n        return key in self.storage\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                del self.storage[key]\n\n\n# Abstraction: Ingestion Logic (independent of storage backend)\nclass IngestJob:\n    \"\"\"Ingestion job that works with any storage backend.\"\"\"\n\n    def __init__(self, backend: StorageBackend):\n        self.backend = backend\n\n    def execute(self, data_key: str, data: bytes):\n        \"\"\"Execute ingestion with idempotency check.\"\"\"\n        if not self.backend.exists(data_key):\n            self.backend.write(data_key, data)\n        else:\n            print(f\"Data with key {data_key} already exists. Skipping write.\")\n</code></pre>"},{"location":"structural/bridge/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/bridge/#basic-usage-production-testing-and-development","title":"Basic Usage: Production, Testing, and Development","text":"<pre><code># Production: use S3\ns3_backend = S3Storage(bucket_name=\"prod-data-lake\")\ningest_job = IngestJob(s3_backend)\ningest_job.execute(\"2025-12-16/customer_data.parquet\", b\"production data\")\n\n# Local Development: use local filesystem\nlocal_backend = LocalStorage(\"/tmp/data\")\ningest_job_local = IngestJob(local_backend)\ningest_job_local.execute(\"example_key\", b\"sample data\")\n\n# Testing: use in-memory mock storage\nmock_backend = MockStorage()\ningest_job_test = IngestJob(mock_backend)\ningest_job_test.execute(\"example_key\", b\"sample data in mock\")\n\n# Multi-cloud: use Google Cloud Storage\ngcs_backend = GCSStorage(bucket_name=\"multi-cloud-data\")\ningest_job_gcs = IngestJob(gcs_backend)\ningest_job_gcs.execute(\"2025-12-16/customer_data.parquet\", b\"gcs data\")\n</code></pre>"},{"location":"structural/bridge/#demonstrating-the-bridge-same-logic-different-backends","title":"Demonstrating the Bridge: Same Logic, Different Backends","text":"<pre><code># The key benefit: IngestJob logic is identical regardless of backend\ndef run_ingestion_pipeline(backend: StorageBackend, data_items: list):\n    \"\"\"Generic pipeline - works with ANY storage backend.\"\"\"\n    job = IngestJob(backend)\n    for item in data_items:\n        job.execute(item[\"key\"], item[\"data\"])\n    print(\"Pipeline complete!\")\n\n# Use the same function with different backends\ndata = [\n    {\"key\": \"file1.txt\", \"data\": b\"content1\"},\n    {\"key\": \"file2.txt\", \"data\": b\"content2\"},\n]\n\n# Try with mock first\nmock_backend = MockStorage()\nrun_ingestion_pipeline(mock_backend, data)\n\n# Deploy with S3\ns3_backend = S3Storage(bucket_name=\"my-bucket\")\nrun_ingestion_pipeline(s3_backend, data)\n\n# No code changes! Just swap the backend.\n</code></pre>"},{"location":"structural/bridge/#thread-safe-updates-with-write-and-update-methods","title":"Thread-Safe Updates with Write and Update Methods","text":"<pre><code># Write: idempotent, skips if exists (used for initial writes)\nbackend = MockStorage()\nbackend.write(\"user:123\", b'{\"name\": \"Alice\"}')  # Succeeds\nbackend.write(\"user:123\", b'{\"name\": \"Bob\"}')    # Skips (already exists)\n\n# Update: requires existing key (used for updates/patches)\ntry:\n    backend.update(\"user:123\", b'{\"name\": \"Bob\"}')  # Succeeds\n    backend.update(\"user:999\", b'{\"name\": \"Charlie\"}')  # Raises KeyError\nexcept KeyError as e:\n    print(f\"Update failed: {e}\")\n</code></pre>"},{"location":"structural/bridge/#concurrent-access-with-lock-protection","title":"Concurrent Access with Lock Protection","text":"<pre><code>import threading\n\ndef ingest_in_thread(backend: StorageBackend, key: str, data: bytes):\n    job = IngestJob(backend)\n    job.execute(key, data)\n\n# Test concurrent access with mock backend\nmock_backend = MockStorage()\nthreads = []\n\nfor i in range(5):\n    t = threading.Thread(\n        target=ingest_in_thread, \n        args=(mock_backend, \"concurrent_key\", f\"data_{i}\".encode())\n    )\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"Final data in mock: {mock_backend.read('concurrent_key')}\")\n# All threads are synchronized via Lock - no race conditions\n</code></pre>"},{"location":"structural/bridge/#testing-with-mock-backend","title":"Testing with Mock Backend","text":"<pre><code># Unit test example - no need for real AWS/GCS credentials\ndef test_ingest_job_with_mock_storage():\n    backend = MockStorage()\n    ingest_job = IngestJob(backend)\n\n    key = \"ingest_key\"\n    data = b\"ingest data\"\n\n    # Execute ingestion\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Verify data integrity\n    read_data = backend.read(key)\n    assert read_data == data\n\n    # Test idempotency: second execute should skip\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Cleanup\n    backend.delete(key)\n    assert backend.exists(key) == False\n</code></pre>"},{"location":"structural/bridge/#benefits","title":"Benefits","text":"<p>// Add performance considerations | Pros | Cons | |------|------| | Decouple job logic from storage | Extra abstraction layers | | Add new backends without changing job code | More code upfront | | Easy to test with mock storage | Potential performance overhead | | Swap backends at runtime | Interface mismatches between backends | | Support multi-cloud deployments | | |------|------| | Decouple job logic from storage | Extra abstraction layers | | Add new backends without changing job code | More code upfront | | Easy to test with mock storage | Potential performance overhead | | Swap backends at runtime | Interface mismatches between backends | | Support multi-cloud deployments | |</p>"},{"location":"structural/bridge/#advanced-storage-factory","title":"Advanced: Storage Factory","text":"<pre><code>def create_storage(backend_type: str, **config) -&gt; StorageImplementation:\n    \"\"\"Factory for creating storage backends.\"\"\"\n    backends = {\n        \"s3\": lambda: S3Storage(**config),\n        \"local\": lambda: LocalStorage(**config),\n        \"gcs\": lambda: GCSStorage(**config),\n    }\n\n    if backend_type not in backends:\n        raise ValueError(f\"Unknown storage backend: {backend_type}\")\n\n    return backends[backend_type]()\n\n# Configuration-driven backend selection\nbackend_type = os.getenv(\"STORAGE_BACKEND\", \"s3\")\nstorage = create_storage(backend_type, bucket_name=\"my-bucket\")\njob = IngestJob(storage, {\"name\": \"my_job\"})\n</code></pre>"},{"location":"structural/composite/","title":"Composite Pattern","text":""},{"location":"structural/composite/#problem","title":"Problem","text":"<p>Your orchestration layer must execute hierarchical operations composed of many smaller sub-operations:</p> <ul> <li>Bulk updates spanning multiple microservices</li> <li>Fan-out workflows (user-service \u2192 billing \u2192 inventory \u2192 notifications)</li> <li>Long-running batches with partial progress tracking</li> <li>Parallel or sequential execution depending on the operation</li> </ul> <p>Each operation may:</p> <ul> <li>Succeed or fail independently</li> <li>Run in parallel or sequence</li> <li>Be cancelled mid-flight</li> <li>Report partial progress and aggregated errors</li> </ul>"},{"location":"structural/composite/#without-composite","title":"Without Composite","text":"<p>You end up with special-case orchestration logic everywhere:</p> <pre><code># \u274c Hard-coded orchestration logic\ndef bulk_update():\n    user_result = update_user()\n    if not user_result.success:\n        return failure()\n\n    inventory_result = update_inventory()\n    billing_result = update_billing()\n\n    if inventory_result.failed or billing_result.failed:\n        return partial_failure()\n</code></pre> <p>Problems:</p> <ul> <li>Tight coupling between orchestration and execution logic</li> <li>No recursive composition</li> <li>Hard to parallelize</li> <li>Difficult to track progress or aggregate failures</li> <li>Adding nested workflows becomes unmanageable</li> </ul>"},{"location":"structural/composite/#solution","title":"Solution","text":"<p>Use the Composite Pattern to model operations as a tree:</p> <ul> <li>LeafOperation \u2192 Executes a single RPC / task</li> <li>CompositeOperation \u2192 Executes and aggregates child operations</li> <li>Operation interface \u2192 Uniform execution, cancellation, and status tracking</li> </ul> <p>This allows:</p> <ul> <li>Recursive composition</li> <li>Parallel or sequential execution</li> <li>Aggregated success/failure</li> <li>Partial execution and progress reporting</li> <li>Transparent orchestration</li> </ul>"},{"location":"structural/composite/#core-design","title":"Core Design","text":"<pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import List, Optional\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n</code></pre>"},{"location":"structural/composite/#operation-status","title":"Operation Status","text":"<pre><code>class OperationStatus(Enum):\n    PENDING = \"PENDING\"\n    IN_PROGRESS = \"IN_PROGRESS\"\n    SUCCESS = \"SUCCESS\"\n    FAILURE = \"FAILURE\"\n</code></pre>"},{"location":"structural/composite/#operation-result","title":"Operation Result","text":"<pre><code>class OperationResult:\n    def __init__(self, status=OperationStatus.PENDING, errors=None):\n        self.status = status\n        self.errors = errors if errors else []\n\n    @property\n    def is_complete(self):\n        return self.status in {OperationStatus.SUCCESS, OperationStatus.FAILURE}\n</code></pre>"},{"location":"structural/composite/#component-interface","title":"Component Interface","text":"<pre><code>class Operation(ABC):\n    @abstractmethod\n    def execute(self) -&gt; OperationResult:\n        pass\n\n    @abstractmethod\n    def cancel(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_status(self) -&gt; OperationStatus:\n        pass\n</code></pre>"},{"location":"structural/composite/#leaf-operation","title":"Leaf Operation","text":"<p>Represents a single RPC / microservice call.</p> <pre><code>class LeafOperation(Operation):\n    def __init__(self, name: str, payload: dict, fail: bool = False):\n        self.name = name\n        self.payload = payload\n        self.fail = fail\n        self.result = OperationResult()\n\n    def execute(self) -&gt; OperationResult:\n        try:\n            self.result.status = OperationStatus.IN_PROGRESS\n            time.sleep(0.5)  # simulate RPC latency\n\n            if self.fail:\n                raise Exception(f\"{self.name} failed\")\n\n            self.result.status = OperationStatus.SUCCESS\n            return self.result\n\n        except Exception as e:\n            self.result.status = OperationStatus.FAILURE\n            self.result.errors.append(e)\n            return self.result\n\n    def cancel(self):\n        if self.result.status == OperationStatus.IN_PROGRESS:\n            self.result.status = OperationStatus.FAILURE\n\n    def get_status(self):\n        return self.result.status\n</code></pre>"},{"location":"structural/composite/#composite-operation","title":"Composite Operation","text":"<p>Executes and aggregates child operations.</p> <pre><code>class CompositeOperation(Operation):\n    def __init__(self, name: str, children: Optional[List[Operation]] = None, parallel=False):\n        self.name = name\n        self.children = children or []\n        self.parallel = parallel\n        self.result = OperationResult()\n\n    def add_operation(self, operation: Operation):\n        self.children.append(operation)\n</code></pre>"},{"location":"structural/composite/#sequential-execution","title":"Sequential Execution","text":"<pre><code>    def _execute_sequential(self):\n        self.result.status = OperationStatus.IN_PROGRESS\n        success = True\n\n        for child in self.children:\n            result = child.execute()\n            if result.status == OperationStatus.FAILURE:\n                success = False\n                self.result.errors.extend(result.errors)\n\n        self.result.status = OperationStatus.SUCCESS if success else OperationStatus.FAILURE\n        return self.result\n</code></pre>"},{"location":"structural/composite/#parallel-execution","title":"Parallel Execution","text":"<pre><code>    def _execute_parallel(self):\n        self.result.status = OperationStatus.IN_PROGRESS\n        success = True\n\n        with ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [executor.submit(child.execute) for child in self.children]\n\n            for future in as_completed(futures):\n                result = future.result()\n                if result.status == OperationStatus.FAILURE:\n                    success = False\n                    self.result.errors.extend(result.errors)\n\n        self.result.status = OperationStatus.SUCCESS if success else OperationStatus.FAILURE\n        return self.result\n</code></pre>"},{"location":"structural/composite/#unified-execution-api","title":"Unified Execution API","text":"<pre><code>    def execute(self):\n        return (\n            self._execute_parallel()\n            if self.parallel\n            else self._execute_sequential()\n        )\n\n    def cancel(self):\n        for child in self.children:\n            child.cancel()\n        self.result.status = OperationStatus.FAILURE\n\n    def get_status(self):\n        if all(c.get_status() == OperationStatus.SUCCESS for c in self.children):\n            return OperationStatus.SUCCESS\n        if any(c.get_status() == OperationStatus.FAILURE for c in self.children):\n            return OperationStatus.FAILURE\n        if any(c.get_status() == OperationStatus.IN_PROGRESS for c in self.children):\n            return OperationStatus.IN_PROGRESS\n        return OperationStatus.PENDING\n</code></pre>"},{"location":"structural/composite/#progress-tracking","title":"Progress Tracking","text":"<pre><code>    def get_progress(self) -&gt; float:\n        if not self.children:\n            return 100.0\n        completed = sum(1 for c in self.children if c.result.is_complete)\n        return (completed / len(self.children)) * 100\n</code></pre>"},{"location":"structural/composite/#usage-example-bulk-update-workflow","title":"Usage Example: Bulk Update Workflow","text":"<pre><code>bulk_update = CompositeOperation(\n    name=\"bulk-update\",\n    parallel=True,\n    children=[\n        LeafOperation(\"user-service\", {\"user_id\": 1}),\n        LeafOperation(\"inventory-service\", {\"sku\": \"A1\"}, fail=True),\n        LeafOperation(\"billing-service\", {\"invoice\": 123}),\n    ],\n)\n\nbulk_update.execute()\n\nprint(\"Status:\", bulk_update.get_status().value)\nprint(\"Progress:\", bulk_update.get_progress(), \"%\")\nprint(\"Errors:\", bulk_update.result.errors)\n</code></pre>"},{"location":"structural/composite/#recursive-composition-nested-workflows","title":"Recursive Composition (Nested Workflows)","text":"<pre><code>billing_flow = CompositeOperation(\n    \"billing-flow\",\n    children=[\n        LeafOperation(\"invoice\"),\n        LeafOperation(\"payment\"),\n    ]\n)\n\nroot = CompositeOperation(\n    \"root-workflow\",\n    parallel=True,\n    children=[\n        LeafOperation(\"user-service\"),\n        billing_flow,\n        LeafOperation(\"notification-service\"),\n    ],\n)\n\nroot.execute()\n</code></pre>"},{"location":"structural/composite/#concurrency-orchestration","title":"Concurrency &amp; Orchestration","text":"<ul> <li>Parallel execution via <code>ThreadPoolExecutor</code></li> <li> <p>Compatible with:</p> </li> <li> <p>Worker pools</p> </li> <li>Background job schedulers</li> <li>Async orchestration layers</li> <li>Supports best-effort execution (all children run even if some fail)</li> </ul>"},{"location":"structural/composite/#testing-example","title":"Testing Example","text":"<pre><code>def test_partial_failure():\n    op = CompositeOperation(\n        \"test\",\n        children=[\n            LeafOperation(\"ok\"),\n            LeafOperation(\"fail\", fail=True),\n        ],\n    )\n\n    result = op.execute()\n    assert result.status == OperationStatus.FAILURE\n    assert len(result.errors) == 1\n</code></pre>"},{"location":"structural/composite/#benefits","title":"Benefits","text":"Pros Cons Uniform interface for simple &amp; complex operations More abstraction Recursive composition Requires careful state handling Parallel &amp; sequential execution Thread cancellation is cooperative Aggregated errors Retry logic not built-in Progress tracking"},{"location":"structural/composite/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Retry Decorator for operations</li> <li>Fail-fast vs best-effort policies</li> <li>Compensation / rollback operations</li> <li>Timeout-aware execution</li> <li>Tracing &amp; metrics decorators</li> <li>Async (<code>asyncio</code>) implementation</li> </ul>"},{"location":"structural/composite/#when-to-use-composite","title":"When to Use Composite","text":"<p>\u2705 Use when:</p> <ul> <li>Operations naturally form trees</li> <li>You need uniform handling of single and grouped actions</li> <li>Orchestration logic must remain clean</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Execution order is strictly linear</li> <li>No hierarchical grouping exists</li> </ul>"},{"location":"structural/decorator/","title":"Decorator Pattern \u2013 Cross-Cutting Features for API Clients","text":""},{"location":"structural/decorator/#problem","title":"Problem","text":"<p>You want to add cross-cutting features to API clients like <code>ParagoNClient</code> without modifying their code. Typical features include:</p> <ul> <li>\ud83d\udd01 Retries</li> <li>\ud83e\uddf5 Tracing</li> <li>\ud83d\udcca Metrics / Logging</li> </ul>"},{"location":"structural/decorator/#challenges","title":"Challenges","text":"<ul> <li>Features must be chainable and order-sensitive</li> <li>Wrapper overhead must be minimal for high-frequency calls</li> <li>Ability to enable/disable features via configuration</li> </ul>"},{"location":"structural/decorator/#without-decorators","title":"\u274c Without Decorators","text":"<p>Embedding retries or tracing directly in the client:</p> <pre><code>class ParagoNClient:\n    def get_user(self, user_id):\n        print(\"Tracing start\")\n        for attempt in range(3):\n            try:\n                print(\"Calling API\")\n                return {\"user_id\": user_id}\n            except Exception:\n                print(\"Retrying...\")\n        print(\"Tracing end\")\n</code></pre> <p>Problems:</p> <ul> <li>Violates Single Responsibility Principle</li> <li>Hard to test or extend</li> <li>Cannot selectively enable/disable features</li> </ul>"},{"location":"structural/decorator/#solution-decorator-pattern","title":"Solution: Decorator Pattern","text":"<p>Wrap the client with decorators for each feature:</p> <pre><code>ParagoNClient\n     \u2193\nRetryingClient\n     \u2193\nTracingClient\n     \u2193\nApplication Code\n</code></pre>"},{"location":"structural/decorator/#advantages","title":"Advantages:","text":"<ul> <li>Each feature is isolated</li> <li>Decorators are composable</li> <li>Order can change behavior (e.g., trace every retry vs retry traced calls)</li> <li>Supports configuration-driven enable/disable</li> </ul>"},{"location":"structural/decorator/#implementation","title":"Implementation","text":""},{"location":"structural/decorator/#base-client","title":"Base Client","text":"<pre><code>class ParagoNClient:\n    def get_user(self, user_id: str) -&gt; dict:\n        print(f\"Fetching user {user_id}\")\n        return {\"user_id\": user_id, \"name\": \"John Doe\"}\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        print(f\"Updating user {user_id}\")\n        return True\n</code></pre>"},{"location":"structural/decorator/#base-decorator","title":"Base Decorator","text":"<pre><code>class BaseDecorator:\n    def __init__(self, client: ParagoNClient):\n        self.client = client\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.client.get_user(user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.client.update_user(user_id, data)\n</code></pre>"},{"location":"structural/decorator/#retrying-decorator","title":"Retrying Decorator","text":"<pre><code>class RetryingClient(BaseDecorator):\n    def __init__(self, client: ParagoNClient, retries: int = 3):\n        super().__init__(client)\n        self.retries = retries\n\n    def retry_func(self, func, *args, **kwargs):\n        for attempt in range(self.retries):\n            try:\n                print(f\"Attempt {attempt+1} for {func.__name__}\")\n                if random.random() &lt; 0.3:  # simulate failure\n                    raise Exception(\"Simulated network error\")\n                return func(*args, **kwargs)\n            except Exception as e:\n                print(f\"Error on attempt {attempt+1}: {e}\")\n                if attempt == self.retries - 1:\n                    raise e\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.retry_func(super().get_user, user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.retry_func(super().update_user, user_id, data)\n</code></pre>"},{"location":"structural/decorator/#tracing-decorator","title":"Tracing Decorator","text":"<pre><code>class TracingClient(BaseDecorator):\n    def __init__(self, client: ParagoNClient):\n        super().__init__(client)\n\n    def trace_func(self, func, *args, **kwargs):\n        print(f\"Tracing start: {func.__name__} with args: {args}, kwargs: {kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"Tracing end: {func.__name__} with result: {result}\")\n        return result\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.trace_func(super().get_user, user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.trace_func(super().update_user, user_id, data)\n</code></pre>"},{"location":"structural/decorator/#configuration-driven-client","title":"Configuration-Driven Client","text":"<pre><code>class ParagonClientConfig:\n    def __init__(self, enable_retries=True, enable_tracing=True, retries=3):\n        self.enable_retries = enable_retries\n        self.enable_tracing = enable_tracing\n        self.retries = retries\n\n    def get_client(self) -&gt; ParagoNClient:\n        client = ParagoNClient()\n        if self.enable_retries:\n            client = RetryingClient(client, retries=self.retries)\n        if self.enable_tracing:\n            client = TracingClient(client)\n        return client\n</code></pre>"},{"location":"structural/decorator/#usage-example","title":"Usage Example","text":"<pre><code>config = ParagonClientConfig(enable_retries=True, enable_tracing=True, retries=3)\nclient = config.get_client()\n\nclient.get_user(\"12345\")\nclient.update_user(\"12345\", {\"name\": \"Jane Doe\"})\n</code></pre> <p>Order-sensitive behavior:</p> <pre><code># Trace every retry\nclient = TracingClient(RetryingClient(ParagoNClient()))\n\n# Retry traced calls\nclient = RetryingClient(TracingClient(ParagoNClient()))\n</code></pre>"},{"location":"structural/decorator/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_decorated_client():\n    config = ParagonClientConfig(True, True, retries=2)\n    client = config.get_client()\n    user_data = client.get_user(\"12345\")\n    assert user_data[\"user_id\"] == \"12345\"\n    update_status = client.update_user(\"12345\", {\"name\": \"Jane Doe\"})\n    assert update_status is True\n</code></pre> <pre><code>def test_tracing_logic(capfd):\n    client = TracingClient(ParagoNClient())\n    client.get_user(\"12345\")\n    out, _ = capfd.readouterr()\n    assert \"Tracing start\" in out\n    assert \"Tracing end\" in out\n</code></pre>"},{"location":"structural/decorator/#benefits","title":"Benefits","text":"Pros Cons No changes to core client Slight overhead per call Features are composable Some method duplication Order-sensitive behavior Requires interface consistency Easy to test Supports configuration-driven enable/disable"},{"location":"structural/decorator/#summary","title":"Summary","text":"<p>This implementation provides a clean, extensible, and testable approach to adding cross-cutting features to API clients using the Decorator Pattern.</p> <p>It allows runtime composition, configurable behavior, and ensures SRP adherence.</p>"},{"location":"structural/facade/","title":"Facade Pattern","text":""},{"location":"structural/facade/#problem","title":"Problem","text":"<p>Your application includes a complex onboarding subsystem that must coordinate multiple internal and external services:</p> <ul> <li>Identity service (user creation)</li> <li>Billing service (subscription setup)</li> <li>Third-party provider (ParagoN) for provisioning</li> <li>Error handling and compensation when partial failures occur</li> <li>Retry safety for frontend and service retries</li> </ul> <p>Each onboarding attempt may:</p> <ul> <li>Call multiple subsystems in a strict order</li> <li>Partially succeed and then fail</li> <li>Be retried due to network issues or client timeouts</li> <li>Require rollback of previously completed steps</li> </ul>"},{"location":"structural/facade/#without-facade","title":"Without Facade","text":"<p>Controllers and callers end up containing orchestration logic:</p> <pre><code># \u274c Orchestration leaks into controllers\ndef onboard_user_controller(req):\n    try:\n        create_identity(req.user_id, req.email)\n        create_subscription(req.user_id, req.plan_id)\n        provision_paragon(req.user_id)\n        return success()\n    except Exception:\n        rollback_billing(req.user_id)\n        rollback_paragon(req.user_id)\n        return failure()\n</code></pre> <p>Problems:</p> <ul> <li>Controllers become complex and fragile</li> <li>Orchestration logic is duplicated across callers</li> <li>Error recovery is inconsistent</li> <li>Retry behavior is unsafe without idempotency</li> <li>Changing onboarding steps requires touching many callers</li> </ul>"},{"location":"structural/facade/#solution","title":"Solution","text":"<p>Use the Facade Pattern to provide a single high-level API for onboarding:</p> <ul> <li>OnboardingFacade \u2192 Orchestrates the full onboarding workflow</li> <li>Subsystem services \u2192 Identity, Billing, ParagoN</li> <li>Idempotency store \u2192 Ensures safe retries</li> </ul> <p>The Facade:</p> <ul> <li>Hides orchestration details</li> <li>Centralizes error handling and rollback</li> <li>Returns a simple success/failure result</li> <li>Keeps controllers and frontends thin</li> </ul>"},{"location":"structural/facade/#core-design","title":"Core Design","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Optional, Dict\n</code></pre>"},{"location":"structural/facade/#request-dto","title":"Request DTO","text":"<p>Encapsulates all data needed for onboarding.</p> <pre><code>@dataclass(frozen=True)\nclass OnboardUserRequest:\n    user_id: str\n    email: str\n    plan_id: str\n    idempotency_key: str\n</code></pre>"},{"location":"structural/facade/#result-dto","title":"Result DTO","text":"<p>Simple outcome returned to callers.</p> <pre><code>@dataclass\nclass OnboardUserResult:\n    success: bool\n    user_id: Optional[str] = None\n    error: Optional[str] = None\n</code></pre>"},{"location":"structural/facade/#subsystem-services-hidden-behind-the-facade","title":"Subsystem Services (Hidden Behind the Facade)","text":"<p>These represent internal or external dependencies. The Facade coordinates them but callers never interact with them directly.</p> <pre><code>class IdentityService:\n    def create_user(self, user_id: str, email: str) -&gt; None:\n        ...\n</code></pre> <pre><code>class BillingService:\n    def create_subscription(self, user_id: str, plan_id: str) -&gt; None:\n        ...\n    def cancel_subscription(self, user_id: str) -&gt; None:\n        ...\n</code></pre> <pre><code>class ParagoNClient:\n    def provision_account(self, user_id: str) -&gt; None:\n        ...\n    def deprovision_account(self, user_id: str) -&gt; None:\n        ...\n</code></pre>"},{"location":"structural/facade/#idempotency-store","title":"Idempotency Store","text":"<p>Ensures safe retries by caching onboarding results.</p> <pre><code>class IdempotencyStore:\n    def __init__(self):\n        self._store: Dict[str, OnboardUserResult] = {}\n\n    def get(self, key: str) -&gt; Optional[OnboardUserResult]:\n        return self._store.get(key)\n\n    def save(self, key: str, result: OnboardUserResult) -&gt; None:\n        self._store[key] = result\n</code></pre>"},{"location":"structural/facade/#facade","title":"Facade","text":"<p>Provides a single entry point for onboarding.</p> <pre><code>class OnboardingFacade:\n    \"\"\"\n    Facade that hides onboarding orchestration, retries, and recovery\n    behind a single high-level API.\n    \"\"\"\n</code></pre>"},{"location":"structural/facade/#orchestration-flow","title":"Orchestration Flow","text":"<pre><code>    def onboard_user(self, request: OnboardUserRequest) -&gt; OnboardUserResult:\n</code></pre>"},{"location":"structural/facade/#1-idempotency-check","title":"1. Idempotency Check","text":"<pre><code>cached = self.idempotency_store.get(request.idempotency_key)\nif cached:\n    return cached\n</code></pre> <ul> <li>Prevents duplicate onboarding</li> <li>Allows safe retries from frontend or services</li> </ul>"},{"location":"structural/facade/#2-execute-subsystems-in-order","title":"2. Execute Subsystems in Order","text":"<pre><code>self.identity.create_user(request.user_id, request.email)\nself.billing.create_subscription(request.user_id, request.plan_id)\nself.paragon.provision_account(request.user_id)\n</code></pre> <ul> <li>Strict sequencing</li> <li>All orchestration is hidden from callers</li> </ul>"},{"location":"structural/facade/#3-error-handling-compensation","title":"3. Error Handling &amp; Compensation","text":"<pre><code>except Exception as e:\n    self._rollback(request)\n</code></pre> <p>Rollback is best-effort and isolated:</p> <pre><code>def _rollback(self, request: OnboardUserRequest) -&gt; None:\n    self.paragon.deprovision_account(request.user_id)\n    self.billing.cancel_subscription(request.user_id)\n</code></pre>"},{"location":"structural/facade/#4-persist-result","title":"4. Persist Result","text":"<pre><code>self.idempotency_store.save(request.idempotency_key, result)\n</code></pre> <ul> <li>Guarantees consistent results on retries</li> </ul>"},{"location":"structural/facade/#usage-example","title":"Usage Example","text":""},{"location":"structural/facade/#controller-caller","title":"Controller / Caller","text":"<pre><code>result = facade.onboard_user(\n    OnboardUserRequest(\n        user_id=\"user-123\",\n        email=\"user@example.com\",\n        plan_id=\"pro\",\n        idempotency_key=\"req-001\",\n    )\n)\n\nif result.success:\n    return 200\nreturn 500\n</code></pre>"},{"location":"structural/facade/#idempotent-retry","title":"Idempotent Retry","text":"<pre><code>result = facade.onboard_user(request)  # returns cached result\n</code></pre>"},{"location":"structural/facade/#execution-characteristics","title":"Execution Characteristics","text":"<ul> <li>Sequential execution of subsystems</li> <li>Centralized rollback logic</li> <li>Idempotent retries</li> <li>Single return type for callers</li> </ul>"},{"location":"structural/facade/#testing-example","title":"Testing Example","text":"<pre><code>def test_idempotent_onboarding():\n    result1 = facade.onboard_user(request)\n    result2 = facade.onboard_user(request)\n    assert result1 == result2\n</code></pre>"},{"location":"structural/facade/#benefits","title":"Benefits","text":"Pros Cons Simple API for callers Facade can grow large Centralized orchestration Requires careful design Consistent error handling May hide subsystem details Idempotent retry support Rollbacks are best-effort Keeps controllers thin"},{"location":"structural/facade/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Persistent onboarding state machine</li> <li>Async / event-driven onboarding</li> <li>Saga-based compensation</li> <li>Retry &amp; circuit breaker decorators</li> <li>Metrics and tracing inside Facade</li> </ul>"},{"location":"structural/facade/#when-to-use-facade","title":"When to Use Facade","text":"<p>\u2705 Use when:</p> <ul> <li>Multiple subsystems must be coordinated</li> <li>Callers only care about success/failure</li> <li>Orchestration logic is growing in controllers</li> <li>You need a stable API over volatile internals</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Logic is trivial or single-service</li> <li>Callers need fine-grained control over steps</li> </ul>"},{"location":"structural/flyweight/","title":"Flyweight Pattern: Metadata Sharing in Spark","text":""},{"location":"structural/flyweight/#problem","title":"Problem","text":"<p>Your Spark cluster processes millions of records that reference a small set of schema descriptors and lookup metadata:</p> <ul> <li>Each record may carry references to the same descriptors repeatedly</li> <li>Large-scale duplication of metadata increases memory usage</li> <li>Serialization of redundant metadata adds overhead during task shipping across executors</li> <li>Immutable metadata can be shared safely, but naive instantiation wastes resources</li> </ul>"},{"location":"structural/flyweight/#without-flyweight","title":"Without Flyweight","text":"<pre><code># \u274c Every record creates a new metadata object\nfor row in rows:\n    descriptor = MetadataDescriptor(id=row.descriptor_id, fields=row.fields)\n    process(row, descriptor)\n</code></pre> <p>Problems:</p> <ul> <li>High memory consumption</li> <li>Frequent serialization/deserialization in distributed tasks</li> <li>Redundant object creation for identical descriptors</li> <li>Difficult to maintain consistent references</li> </ul>"},{"location":"structural/flyweight/#solution","title":"Solution","text":"<p>Use the Flyweight Pattern to share immutable metadata objects across tasks:</p> <ul> <li>MetadataFlyweight \u2192 Represents a single, immutable descriptor object</li> <li>Central registry \u2192 Ensures one instance per descriptor id</li> <li>Thread-safe creation \u2192 Prevents race conditions when registering flyweights</li> <li>Distributed-safe serialization \u2192 Reuse the same metadata instance during pickling where possible</li> </ul> <p>This allows:</p> <ul> <li>Minimal memory footprint</li> <li>Fast serialization</li> <li>Safe sharing across threads and tasks</li> <li>Centralized management of metadata objects</li> </ul>"},{"location":"structural/flyweight/#core-design","title":"Core Design","text":"<pre><code>from threading import Lock\n</code></pre>"},{"location":"structural/flyweight/#flyweight-class","title":"Flyweight Class","text":"<pre><code>class MetadataFlyweight:\n    _registry = {}\n    _lock = Lock()\n</code></pre> <ul> <li><code>_registry</code> \u2192 stores unique flyweight instances keyed by <code>descriptor_id</code></li> <li><code>_lock</code> \u2192 ensures thread-safe creation</li> </ul>"},{"location":"structural/flyweight/#object-creation-new","title":"Object Creation (new)","text":"<pre><code>def __new__(cls, descriptor_id: str, **attributes):\n    with cls._lock:\n        if descriptor_id not in cls._registry:\n            instance = super(MetadataFlyweight, cls).__new__(cls)\n            instance.descriptor_id = descriptor_id\n            instance.attributes = attributes\n            cls._registry[descriptor_id] = instance\n        return cls._registry[descriptor_id]\n</code></pre> <p>Highlights:</p> <ul> <li>Ensures singleton per descriptor id</li> <li>Safe to use in multi-threaded environments</li> <li>Additional attributes are stored only once</li> <li>Returns existing object if descriptor already exists</li> </ul>"},{"location":"structural/flyweight/#representation","title":"Representation","text":"<pre><code>def __repr__(self):\n    return f\"MetadataFlyweight(id={self.descriptor_id}, attributes={self.attributes})\"\n</code></pre> <ul> <li>Useful for debugging and logging</li> <li>Shows descriptor id and attributes of shared object</li> </ul>"},{"location":"structural/flyweight/#usage-example-spark-pipeline","title":"Usage Example: Spark Pipeline","text":"<pre><code># Imagine multiple tasks processing rows with the same descriptor\ntasks = [\n    {\"descriptor_id\": \"user\", \"fields\": [\"id\", \"name\"]},\n    {\"descriptor_id\": \"order\", \"fields\": [\"id\", \"amount\"]},\n    {\"descriptor_id\": \"user\", \"fields\": [\"id\", \"name\"]},\n]\n\nflyweights = [MetadataFlyweight(**t) for t in tasks]\n\nfor fw in flyweights:\n    print(fw)\n</code></pre> <p>Output:</p> <pre><code>MetadataFlyweight(id=user, attributes={'fields': ['id', 'name']})\nMetadataFlyweight(id=order, attributes={'fields': ['id', 'amount']})\nMetadataFlyweight(id=user, attributes={'fields': ['id', 'name']})\n</code></pre> <p>Notice that the <code>user</code> descriptor is shared across tasks.</p>"},{"location":"structural/flyweight/#benefits","title":"Benefits","text":"Pros Cons Reduces memory usage by sharing objects Requires careful immutability management Faster serialization/deserialization Central registry can become a bottleneck Thread-safe creation Slight overhead on first creation Simplifies metadata consistency Not suitable for mutable objects"},{"location":"structural/flyweight/#advanced-considerations","title":"Advanced Considerations","text":"<ul> <li>Pickling in Spark: Ensure descriptors are pickle-friendly; avoid complex references that cannot be serialized</li> <li>Distributed caching: Could extend the registry to use a distributed cache (Redis, Broadcast variables) for cross-node sharing</li> <li>Lazy attribute computation: Flyweight can defer heavy initialization until first access</li> <li>Immutable objects: Flyweights must remain immutable to safely share across threads and tasks</li> </ul>"},{"location":"structural/flyweight/#when-to-use-flyweight","title":"When to Use Flyweight","text":"<p>\u2705 Use when:</p> <ul> <li>Many objects share identical read-only metadata</li> <li>Memory usage or serialization overhead is significant</li> <li>You want centralized management for consistent references</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Objects are mutable or change per task</li> <li>Metadata is unique per record or rarely reused</li> <li>Overhead of registry outweighs memory savings</li> </ul>"},{"location":"structural/proxy/","title":"Proxy Pattern: HTTP API Client with Caching, Rate-Limiting, and Circuit Breaker","text":""},{"location":"structural/proxy/#problem","title":"Problem","text":"<p>Your HTTP API client (<code>ParagoNClient</code>) is used in edge services and frontends to fetch and update user data:</p> <ul> <li>Repeated API calls for the same user lead to redundant network requests</li> <li>High request rates can exceed third-party API limits</li> <li>Transient API failures may propagate to clients, causing downtime</li> <li>Managing caching, rate-limits, and circuit-breaking separately makes code complex and scattered</li> </ul>"},{"location":"structural/proxy/#without-proxy","title":"Without Proxy","text":"<pre><code># \u274c Direct client calls without policies\nuser = client.get_user(\"user123\")\nclient.update_user(\"user123\", {\"plan\": \"pro\"})\n</code></pre> <p>Problems:</p> <ul> <li>No caching \u2192 repeated fetches always hit the API</li> <li>No rate-limiting \u2192 risk of hitting API quotas</li> <li>No circuit breaker \u2192 failures propagate to callers</li> <li>Hard to add new policies without modifying client</li> </ul>"},{"location":"structural/proxy/#solution","title":"Solution","text":"<p>Use the Proxy Pattern to wrap the original client and provide additional behaviors transparently:</p> <ul> <li>Caching \u2192 store recent API responses to reduce repeated network calls</li> <li>Rate-limiting \u2192 prevent excessive API requests</li> <li>Circuit breaker \u2192 stop requests when failures exceed a threshold and allow recovery after cooldown</li> <li>Transparent interface \u2192 same method signatures as <code>ParagoNClient</code>, so callers can swap easily</li> </ul> <p>This allows:</p> <ul> <li>Centralized policy enforcement</li> <li>Reduced API calls and latency</li> <li>Safe handling of transient failures</li> <li>Simplified client usage for frontends</li> </ul>"},{"location":"structural/proxy/#core-design","title":"Core Design","text":"<pre><code>import time\n</code></pre>"},{"location":"structural/proxy/#proxy-class","title":"Proxy Class","text":"<pre><code>class ParagoNClientProxy:\n    def __init__(self, client: ParagoNClient, cache_ttl: int = 60,\n                 rate_limit: int = 10, breaker_threshold: int = 5):\n        self.client = client\n        self.cache = {}\n        self.cache_ttl = cache_ttl\n        self.rate_limit = rate_limit\n        self.breaker_threshold = breaker_threshold\n        self.failure_count = 0\n        self.last_failure_time = None\n</code></pre> <ul> <li><code>client</code> \u2192 the real API client being wrapped</li> <li><code>cache</code> \u2192 stores cached responses keyed by user ID</li> <li><code>cache_ttl</code> \u2192 cache expiration time in seconds</li> <li><code>rate_limit</code> \u2192 max number of concurrent requests in the cache</li> <li><code>breaker_threshold</code> \u2192 number of failures before opening the circuit</li> </ul>"},{"location":"structural/proxy/#caching-logic","title":"Caching Logic","text":"<pre><code>if user_id in self.cache:\n    cached_entry = self.cache[user_id]\n    if current_time - cached_entry['timestamp'] &lt; self.cache_ttl:\n        print(f\"Returning cached data for user {user_id}\")\n        return cached_entry['data']\n</code></pre> <ul> <li>Reduces repeated API calls for the same user</li> <li>TTL ensures cache freshness</li> </ul>"},{"location":"structural/proxy/#rate-limiting-logic","title":"Rate-Limiting Logic","text":"<pre><code>if len(self.cache) &gt;= self.rate_limit:\n    raise Exception(\"Rate limit exceeded. Try again later.\")\n</code></pre> <ul> <li>Simple policy based on number of cached entries</li> <li>Prevents excessive load on the API</li> </ul>"},{"location":"structural/proxy/#circuit-breaker-logic","title":"Circuit Breaker Logic","text":"<pre><code>if self.failure_count &gt;= self.breaker_threshold:\n    if current_time - self.last_failure_time &lt; 60:  # cooldown\n        raise Exception(\"Circuit breaker is open. Request blocked.\")\n    else:\n        self.failure_count = 0  # reset after cooldown\n</code></pre> <ul> <li>Stops requests when failures exceed threshold</li> <li>Allows recovery after a cooldown period</li> </ul>"},{"location":"structural/proxy/#unified-proxy-api","title":"Unified Proxy API","text":"<pre><code>def get_user(self, user_id: str) -&gt; dict:\n    # Implements caching, rate-limiting, and circuit breaker\n    ...\n\ndef update_user(self, user_id: str, data: dict) -&gt; bool:\n    # Implements circuit breaker and invalidates cache on update\n    ...\n</code></pre> <ul> <li>Methods mirror the real client API</li> <li>Transparent for callers</li> </ul>"},{"location":"structural/proxy/#usage-example","title":"Usage Example","text":"<pre><code>client = ParagoNClient()\nproxy = ParagoNClientProxy(client, cache_ttl=60, rate_limit=10, breaker_threshold=5)\n\nuser = proxy.get_user(\"user123\")  # Fetches from API\nuser_cached = proxy.get_user(\"user123\")  # Returns cached\nproxy.update_user(\"user123\", {\"plan\": \"pro\"})  # Invalidates cache\n</code></pre>"},{"location":"structural/proxy/#testing-example-fast-circuit-breaker","title":"Testing Example (Fast Circuit Breaker)","text":"<pre><code># Test caching, rate-limiting, and circuit breaker without real waits\ntest_proxy_fast_circuit_breaker()\n</code></pre> <ul> <li>Uses cache TTL of 2s and breaker threshold of 2</li> <li>Simulates circuit breaker cooldown by fast-forwarding last_failure_time</li> <li>Tests all proxy behaviors deterministically</li> </ul>"},{"location":"structural/proxy/#benefits","title":"Benefits","text":"Pros Cons Transparent API proxy Adds some overhead per call Reduces redundant network requests Simple rate-limiting logic only Protects against cascading failures Circuit breaker cooldown is fixed Centralizes policy enforcement Not suitable for complex policies"},{"location":"structural/proxy/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Configurable cache backends (Redis, memcached)</li> <li>Pluggable rate-limiting policies (token bucket, leaky bucket)</li> <li>Metrics collection (success/failure counts, cache hits/misses)</li> <li>Async-friendly implementation using <code>asyncio</code></li> <li>Dynamic circuit breaker policies per endpoint</li> </ul>"},{"location":"structural/proxy/#when-to-use-proxy","title":"When to Use Proxy","text":"<p>\u2705 Use when:</p> <ul> <li>You want to add cross-cutting concerns like caching, throttling, or fault-tolerance</li> <li>You want transparent client swapping without changing callers</li> <li>You need centralized handling of third-party API calls</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>No additional behavior is required beyond the client</li> <li>Overhead of proxy outweighs benefits for low-traffic services</li> </ul>"}]}