{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Gang of Four Design Patterns","text":"<p>Welcome to an interactive guide to design patterns tailored for your domain: microservices, React frontends, Spark data pipelines, and custom Python clients.</p>"},{"location":"#about-this-guide","title":"About This Guide","text":"<p>This repository implements the 23 Gang of Four design patterns with real-world examples from your development experience:</p> <ul> <li>Microservices: Building resilient distributed systems</li> <li>Frontend Integration: React components communicating with backend services</li> <li>Data Pipelines: Spark ETL jobs and orchestration</li> <li>API Clients: Custom Python clients for third-party systems (like <code>ParagoNClient</code>)</li> </ul> <p>Each pattern includes: - \ud83d\udcd6 Detailed explanation and motivation - \ud83d\udcbb Complete, runnable code examples - \ud83c\udfaf Real scenarios from your domain - \u26a1 Practical considerations and trade-offs - \ud83e\uddea Testing strategies</p>"},{"location":"#pattern-categories","title":"Pattern Categories","text":""},{"location":"#creational-patterns","title":"Creational Patterns","text":"<p>Deal with object creation mechanisms. Useful for managing dependencies, abstracting instantiation, and building complex configurations.</p> <ul> <li>Singleton: Single shared instance per process (e.g., <code>ParagoNClient</code> manager)</li> <li>Factory Method: Choose implementation based on configuration</li> <li>Abstract Factory: Create families of related objects</li> <li>Builder: Assemble complex objects step-by-step (e.g., <code>SparkJobBuilder</code>)</li> <li>Prototype: Clone and customize objects rapidly</li> </ul>"},{"location":"#structural-patterns","title":"Structural Patterns","text":"<p>Deal with object composition and how classes/objects relate. Useful for adapting interfaces, adding behavior, and managing hierarchies.</p> <ul> <li>Adapter: Map incompatible interfaces (e.g., ParagoN API \u2192 internal DTOs)</li> <li>Bridge: Decouple abstraction from implementation (e.g., ingestion + storage backends)</li> <li>Composite: Build tree structures of operations</li> <li>Decorator: Add behavior dynamically (e.g., retries, tracing, metrics)</li> <li>Facade: Simplify complex subsystems (e.g., user onboarding)</li> <li>Flyweight: Share immutable objects efficiently (e.g., Spark metadata)</li> <li>Proxy: Control access with policies (e.g., caching, rate-limiting)</li> </ul>"},{"location":"#behavioral-patterns","title":"Behavioral Patterns","text":"<p>Deal with object collaboration and responsibility distribution.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Explore a pattern: Pick one that matches your current challenge</li> <li>Read the problem: Understand the real-world scenario</li> <li>Study the solution: See the complete implementation</li> <li>Try it: Copy, adapt, and use in your code</li> <li>Extend it: Customize for your specific needs</li> </ol>"},{"location":"#example-builder-pattern","title":"Example: Builder Pattern","text":"<p>Looking to simplify Spark job configuration?</p> <pre><code>job_spec = (SparkJobBuilder()\n    .from_s3(\"prod-bucket\", \"data/customers/\")\n    .with_filter(\"status = 'active'\")\n    .with_windowing(\"tumbling\", duration=3600)\n    .with_resources(executors=10, memory_gb=4)\n    .build())\n</code></pre> <p>Learn more about the Builder pattern \u2192</p> <p>Last Updated: December 2025 | Version: 1.0</p>"},{"location":"behavioral/","title":"Behavioral Patterns","text":"<p>Behavioral patterns deal with object collaboration and responsibility distribution. They focus on how objects interact, communicate, and distribute work.</p>"},{"location":"behavioral/#overview","title":"Overview","text":"<p>In microservices, event-driven systems, and orchestration, behavioral patterns help: - Handle requests through chains: Pass requests along a chain of handlers (Chain of Responsibility) - Encapsulate operations: Queue, retry, and undo operations as objects (Command) - Define domain-specific languages: Parse and execute DSL scripts (Interpreter) - Iterate over collections: Traverse elements without exposing structure (Iterator) - Coordinate interactions: Centralize communication between components (Mediator) - Capture state: Save and restore state for undo/replay (Memento) - Notify dependents: React to state changes (Observer) - Alter behavior by state: Change behavior based on internal state (State) - Select algorithms: Switch algorithms at runtime (Strategy) - Define algorithm skeletons: Let subclasses override steps (Template Method) - Apply operations to elements: Perform operations without modifying objects (Visitor)</p>"},{"location":"behavioral/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Chain of Responsibility Pipeline of handlers Event validation \u2192 enrichment \u2192 routing Command Encapsulate operations Queue complex actions for async execution Interpreter Parse domain languages DSL for pipeline definitions Iterator Traverse collections Page through API results seamlessly Mediator Centralize coordination Orchestrate multi-service workflows Memento Capture/restore state Pipeline checkpoints and rollback Observer Notify dependents UI updates on backend state changes State Alter behavior by state Order lifecycle state machine Strategy Select algorithms Pluggable retry/backoff strategies Template Method Define algorithm skeleton ETL job template with customizable steps Visitor Apply operations Transform/validate config trees"},{"location":"behavioral/#when-to-use","title":"When to Use","text":"<p>Choose behavioral patterns when you need to: - Decouple senders from receivers - Encapsulate requests as objects - Define families of algorithms - Change object behavior at runtime - Coordinate complex interactions between components - Provide extensibility without modifying existing code</p>"},{"location":"creational/","title":"Creational Patterns","text":"<p>Creational patterns deal with object creation mechanisms, trying to create objects in a way that is suitable to the situation. They abstract the instantiation process.</p>"},{"location":"creational/#overview","title":"Overview","text":"<p>In microservices and data pipelines, creational patterns help: - Manage dependencies: Ensure single instances of clients (Singleton) - Abstract creation logic: Different client types for different environments (Factory Method) - Build complex configurations: Step-by-step assembly of job specs (Builder) - Clone and customize: Rapidly create variations of pipeline configs (Prototype)</p>"},{"location":"creational/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Singleton One global instance per process ParagoNClient manager Factory Method Choose implementation based on config Create prod/test/async clients Abstract Factory Create families of related objects Ecosystem-specific client sets Builder Assemble complex objects step-by-step SparkJobBuilder Prototype Clone and customize objects rapidly Pipeline spec cloning"},{"location":"creational/#when-to-use","title":"When to Use","text":"<p>Choose creational patterns when you need to: - Decouple object creation from your business logic - Support multiple ways to construct similar objects - Enforce constraints on how objects are instantiated - Improve testability through dependency injection</p>"},{"location":"creational/abstract_factory/","title":"Abstract Factory Pattern","text":""},{"location":"creational/abstract_factory/#problem","title":"Problem","text":"<p>Your platform integrates with multiple third-party ecosystems (ParagoN, another API provider). Each provider requires a set of cooperating client objects:</p> <ul> <li>ParagoN ecosystem: <code>AuthClient</code>, <code>DataClient</code>, <code>WebhookClient</code></li> <li>Another API: Different implementations of the same interfaces but different protocols/auth</li> </ul> <p>Without Abstract Factory, you'd end up with messy conditional logic everywhere:</p> <pre><code># \u274c Messy without pattern\nif provider == \"paragon\":\n    auth = ParagoNAuthClient()\n    data = ParagoNDataClient()\n    webhook = ParagoNWebhookClient()\nelif provider == \"another\":\n    auth = AnotherAuthClient()\n    data = AnotherDataClient()\n    webhook = AnotherWebhookClient()\n</code></pre>"},{"location":"creational/abstract_factory/#solution","title":"Solution","text":"<p>Define abstract interfaces and provider-specific factories:</p> <pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\n\nclass Provider(Enum):\n    PARAGON = \"paragon\"\n    ANOTHER_API = \"another_api\"\n    MOCK = \"mock\"\n\nclass BaseAuthClient(ABC):\n    @abstractmethod\n    def authenticate(self, credentials: dict):\n        raise NotImplementedError\n\nclass BaseDataClient(ABC):\n    @abstractmethod\n    def fetch_data(self, query: str) -&gt; dict:\n        raise NotImplementedError\n\nclass BaseWebhookClient(ABC):\n    @abstractmethod\n    def send_webhook(self, payload: dict) -&gt; None:\n        raise NotImplementedError\n\nclass BaseClientFactory(ABC):\n    @abstractmethod\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_data_client(self) -&gt; BaseDataClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        raise NotImplementedError\n\n# ParagoN implementations\nclass ParagoNAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass ParagoNDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass ParagoNWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass ParagoNClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return ParagoNAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return ParagoNDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return ParagoNWebhookClient()\n\n# Mock implementations for testing\nclass MockAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass MockDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass MockWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass MockClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return MockAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return MockDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return MockWebhookClient()\n\ndef load_factory(provider_name: Provider) -&gt; BaseClientFactory:\n    if provider_name == Provider.PARAGON:\n        return ParagoNClientFactory()\n    if provider_name == Provider.MOCK:\n        return MockClientFactory()\n    raise ValueError(f\"Unknown provider: {provider_name.value}\")\n</code></pre>"},{"location":"creational/abstract_factory/#usage","title":"Usage","text":"<pre><code># Configuration-driven ecosystem setup\nprovider = Provider.PARAGON\nfactory = load_factory(provider)\n\n# Create a coordinated set of clients\nauth_client = factory.create_auth_client()\ndata_client = factory.create_data_client()\nwebhook_client = factory.create_webhook_client()\n\n# All three work together seamlessly\nauth_client.authenticate({\"api_key\": \"...\"})\nwebhook_client.send_webhook({\"event\": \"user_created\"})\ndata = data_client.fetch_data(\"SELECT * FROM users\")\n\n# Easy to switch to testing\ntest_factory = load_factory(Provider.MOCK)\ntest_auth = test_factory.create_auth_client()\ntest_data = test_factory.create_data_client()\n</code></pre>"},{"location":"creational/abstract_factory/#benefits","title":"Benefits","text":"<ul> <li>Consistency: All clients from one factory are compatible</li> <li>Easy provider switching: Change config, not code</li> <li>Testability: Create a mock factory for integration tests</li> <li>Extensibility: Adding a new provider requires only one new factory class</li> </ul>"},{"location":"creational/abstract_factory/#testing-example","title":"Testing Example","text":"<pre><code>def test_ecosystem_integration():\n    # Use mock factory for testing\n    factory = load_factory(Provider.MOCK)\n    auth_client = factory.create_auth_client()\n    data_client = factory.create_data_client()\n    webhook_client = factory.create_webhook_client()\n\n    # All clients are now mocked\n    auth_client.authenticate({})\n    webhook_client.send_webhook({})\n    data = data_client.fetch_data(\"\")\n</code></pre>"},{"location":"creational/builder/","title":"Builder Pattern","text":""},{"location":"creational/builder/#problem","title":"Problem","text":"<p>Constructing a complex Spark job configuration requires many optional parameters: - Input sources (S3, Kafka, database) - Transforms (filters, maps, joins) - Windowing and triggers - Resource settings (executors, memory) - Monitoring hooks</p> <p>Without a builder, you'd either: 1. Use a giant constructor with 20 parameters 2. Pass a loose dictionary around 3. Use <code>**kwargs</code> and lose type safety</p> <pre><code># \u274c Messy constructor\njob = SparkJob(\n    source, format, schema_path,\n    filter_fn, map_fn, join_key,\n    window_type, window_duration, trigger_interval,\n    executor_count, executor_memory, num_cores,\n    shuffle_partitions, log_level, metrics_enabled,\n    # ... and 10 more parameters\n)\n</code></pre>"},{"location":"creational/builder/#solution","title":"Solution","text":"<p>Use a builder with fluent chaining to assemble the spec step-by-step:</p> <pre><code>from copy import deepcopy\n\nclass SparkJobSpec:\n    __slots__ = (\n        \"input_source\",\n        \"transforms\",\n        \"windowing\",\n        \"triggers\",\n        \"resources\",\n        \"monitoring_hooks\",\n        \"spec_version\",\n    )\n\n    def __init__(self, *, input_source, transforms, windowing, triggers, resources, monitoring_hooks, spec_version=\"1.0\"):\n        object.__setattr__(self, \"input_source\", input_source)\n        object.__setattr__(self, \"transforms\", tuple(transforms))\n        object.__setattr__(self, \"windowing\", windowing)\n        object.__setattr__(self, \"triggers\", triggers)\n        object.__setattr__(self, \"resources\", deepcopy(resources))\n        object.__setattr__(self, \"monitoring_hooks\", tuple(monitoring_hooks))\n        object.__setattr__(self, \"spec_version\", spec_version)\n\n    def __setattr__(self, key, value):\n        if hasattr(self, key):\n            raise AttributeError(\"Cannot modify immutable SparkJobSpec\")\n        super().__setattr__(key, value)\n\n    def __delattr__(self, key):\n        raise AttributeError(\"SparkJobSpec is immutable\")\n\n    def serialize(self):\n        return {\n            \"input_source\": self.input_source,\n            \"transforms\": self.transforms,\n            \"windowing\": self.windowing,\n            \"triggers\": self.triggers,\n            \"resources\": self.resources,\n            \"monitoring_hooks\": self.monitoring_hooks,\n            \"spec_version\": self.spec_version,\n        }\n\nclass SparkJobBuilder:\n    def __init__(self):\n        self._input_source = None\n        self._transforms = []\n        self._windowing = None\n        self._triggers = None\n        self._resources = {}\n        self._monitoring_hooks = []\n        self._spec_version = \"1.0\"\n\n    def input_source(self, source):\n        self._input_source = source\n        return self\n\n    def set_transforms(self, transforms):\n        self._transforms = transforms\n        return self\n\n    def windowing(self, windowing):\n        self._windowing = windowing\n        return self\n\n    def triggers(self, triggers):\n        self._triggers = triggers\n        return self\n\n    def resources(self, resources):\n        self._resources = resources\n        return self\n\n    def monitoring_hooks(self, hooks):\n        self._monitoring_hooks = hooks\n        return self\n\n    def spec_version(self, version):\n        self._spec_version = version\n        return self\n\n    def build(self):\n        return SparkJobSpec(\n            input_source=self._input_source,\n            transforms=self._transforms,\n            windowing=self._windowing,\n            triggers=self._triggers,\n            resources=self._resources,\n            monitoring_hooks=self._monitoring_hooks,\n            spec_version=self._spec_version\n        )\n</code></pre>"},{"location":"creational/builder/#usage-examples","title":"Usage Examples","text":"<pre><code># Build a job spec fluently\nbuilder = SparkJobBuilder()\njob_spec = (builder\n    .input_source({\"type\": \"s3\", \"bucket\": \"prod-bucket\", \"path\": \"data/\"})\n    .set_transforms([\n        {\"type\": \"filter\", \"condition\": \"status = 'active'\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"}\n    ])\n    .windowing({\"type\": \"tumbling\", \"duration\": 3600})\n    .triggers({\"type\": \"micro-batch\", \"interval\": 60})\n    .resources({\"executor_count\": 10, \"executor_memory_gb\": 4})\n    .monitoring_hooks([{\"type\": \"prometheus\"}])\n    .build())\n\n# Serialize for storage\nconfig = job_spec.serialize()\n\n# Immutability guaranteed\ntry:\n    job_spec.input_source = \"new_value\"  # Raises AttributeError\nexcept AttributeError as e:\n    print(f\"Cannot modify: {e}\")\n</code></pre>"},{"location":"creational/builder/#advantages","title":"Advantages","text":"Pros Cons Readable fluent interface Extra classes/code Type-safe method signatures Slightly more overhead Immutable final spec Not suitable for tiny objects Easy to extend with new methods Requires careful implementation Versioning built-in"},{"location":"creational/builder/#advanced-builder-cloning","title":"Advanced: Builder Cloning","text":"<pre><code># Clone an existing job and modify\nmodified_job = (SparkJobBuilder()\n    # Copy settings from existing spec\n    # then override specific fields\n    .from_s3(\"test-bucket\", \"data/customers/\")\n    .with_resources(executors=2)  # Fewer resources for testing\n    .build())\n</code></pre>"},{"location":"creational/factory_method/","title":"Factory Method Pattern","text":""},{"location":"creational/factory_method/#problem","title":"Problem","text":"<p>Your system needs to create API clients for different runtime contexts: - Production: Real <code>ParagoNClient</code> connecting to live API - Testing: Mock client returning fixtures - Async workers: Async-aware client variant</p> <p>Without a factory, you'd hardcode client selection scattered throughout your code. The Factory Method centralizes this logic.</p>"},{"location":"creational/factory_method/#solution","title":"Solution","text":"<p>Define a factory method that returns the appropriate client based on configuration:</p> <pre><code>from enum import Enum\nfrom typing import Union\n\nclass ClientType(Enum):\n    PRODUCTION = \"production\"\n    TESTING = \"testing\"\n    ASYNC = \"async\"\n\nclass HttpMethod(Enum):\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n\nclass ParagonClientConfig:\n    def __init__(self, client_type: ClientType, client_id: str = None, client_secret: str = None):\n        self.client_type = client_type\n        self.client_id = client_id or \"default_id\"\n        self.client_secret = client_secret or \"default_secret\"\n\nclass BaseClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    def request(self, method, url, headers, data, query_params):\n        raise NotImplementedError\n\nclass BaseAsyncClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        raise NotImplementedError\n\nclass ParagonSyncClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"SyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonAsyncClient(BaseAsyncClient):\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"AsyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonMockClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"MockClient: {method} {url} with {data} and {query_params}\"\n\ndef create_client(config: ParagonClientConfig) -&gt; Union[BaseClient, BaseAsyncClient]:\n    if config.client_type == ClientType.PRODUCTION:\n        return ParagonSyncClient(config)\n    elif config.client_type == ClientType.TESTING:\n        return ParagonMockClient(config)\n    elif config.client_type == ClientType.ASYNC:\n        return ParagonAsyncClient(config)\n    else:\n        raise ValueError(f\"Unknown client type: {config.client_type}\")\n</code></pre>"},{"location":"creational/factory_method/#usage","title":"Usage","text":"<pre><code># Configuration-driven client creation\nconfig_prod = ParagonClientConfig(ClientType.PRODUCTION, client_id=\"prod_123\")\nprod_client = create_client(config_prod)\n\nconfig_test = ParagonClientConfig(ClientType.TESTING)\ntest_client = create_client(config_test)\n\nconfig_async = ParagonClientConfig(ClientType.ASYNC, client_id=\"async_123\")\nasync_client = create_client(config_async)\n\n# Use the appropriate client\nresult_prod = prod_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\nresult_test = test_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\n</code></pre>"},{"location":"creational/factory_method/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Decouples client selection from usage Extra indirection for simple cases Easy to add new client types Factory method becomes complex with many types Testability: swap with mock effortlessly String-based selection is fragile Configuration-driven behavior"},{"location":"creational/factory_method/#advanced-factory-with-config-objects","title":"Advanced: Factory with Config Objects","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ClientConfig:\n    env: str\n    api_key: str = None\n    timeout: int = 30\n    retries: int = 3\n\ndef create_client_from_config(config: ClientConfig) -&gt; ClientBase:\n    if config.env == \"prod\":\n        return ParagoNClientProd(api_key=config.api_key, \n                                 timeout=config.timeout, \n                                 retries=config.retries)\n    elif config.env == \"test\":\n        return ParagoNClientMock()\n    # ... etc\n</code></pre>"},{"location":"creational/prototype/","title":"Prototype Pattern","text":""},{"location":"creational/prototype/#problem","title":"Problem","text":"<p>In your ETL orchestration, you often reuse pipeline configurations with minor variations:</p> <pre><code># Base template: daily customer data pipeline\nbase_config = {\n    \"name\": \"customer_daily\",\n    \"source\": {\"s3\": \"s3://prod-data/customers/\"},\n    \"transforms\": [\n        {\"type\": \"filter\", \"condition\": \"is_active=true\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"},\n        {\"type\": \"aggregate\", \"window\": \"1d\"},\n    ],\n    \"resources\": {\"executors\": 10, \"memory_gb\": 4},\n}\n</code></pre> <p>Now you need variations for: - Testing: Same config but with test data and fewer resources - Weekly rollup: Different transforms and schedule - A/B test variant: Modified enrichment logic</p> <p>Without Prototype, you rebuild each from scratch, leading to duplication and inconsistency.</p>"},{"location":"creational/prototype/#solution","title":"Solution","text":"<p>Implement a cloning mechanism with provenance tracking:</p> <pre><code>from copy import deepcopy, copy\n\nclass PipelineSpec:\n    \"\"\"Pipeline specification with cloning and provenance.\"\"\"\n\n    def __init__(self, *, name: str, input_source: str, \n                 transforms: list, resources: dict, \n                 metadata: dict = None):\n        self.name = name\n        self.input_source = input_source\n        self.transforms = transforms\n        self.resources = resources\n        self.metadata = metadata or {}\n\n    def clone(self, deep: bool = True, **overrides):\n        \"\"\"Clone the spec, optionally overriding fields.\n\n        Args:\n            deep: If True, recursively copy nested structures\n            **overrides: Fields to override in the cloned spec\n\n        Returns:\n            PipelineSpec: New independent instance\n        \"\"\"\n        if deep:\n            # Deep copy: all nested structures are independent\n            cloned_transforms = deepcopy(self.transforms)\n            cloned_resources = deepcopy(self.resources)\n            cloned_metadata = deepcopy(self.metadata)\n        else:\n            # Shallow copy: nested objects are shared\n            cloned_transforms = copy(self.transforms)\n            cloned_resources = copy(self.resources)\n            cloned_metadata = copy(self.metadata)\n\n        # Apply overrides\n        for key, value in overrides.items():\n            if key == \"transforms\":\n                cloned_transforms = value\n            elif key == \"resources\":\n                cloned_resources = value\n            elif key == \"metadata\":\n                cloned_metadata = value\n\n        # Create new spec\n        new_spec = PipelineSpec(\n            name=overrides.get(\"name\", self.name),\n            input_source=overrides.get(\"input_source\", self.input_source),\n            transforms=cloned_transforms,\n            resources=cloned_resources,\n            metadata=cloned_metadata,\n        )\n\n        # Update provenance\n        new_spec.metadata['cloned_from'] = self.name\n\n        return new_spec\n</code></pre>"},{"location":"creational/prototype/#usage-examples","title":"Usage Examples","text":""},{"location":"creational/prototype/#basic-cloning-with-deep-copy","title":"Basic Cloning with Deep Copy","text":"<pre><code># Create a base pipeline spec\nbase_spec = PipelineSpec(\n    name=\"base_etl\",\n    input_source=\"s3://input/data\",\n    transforms=[\"parse\", \"clean\", \"enrich\"],\n    resources={\"executor_memory\": \"4G\", \"partitions\": 100},\n)\n\n# Clone it for a new use case (deep copy by default)\n# All nested structures are independent\ntest_spec = base_spec.clone(\n    name=\"test_etl\",\n    input_source=\"s3://test/data\",\n)\n\n# Modify test_spec's transforms - does NOT affect base_spec\ntest_spec.transforms.append(\"validate\")\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Still [\"parse\", \"clean\", \"enrich\"]\nprint(f\"test_spec transforms: {test_spec.transforms}\")  # [\"parse\", \"clean\", \"enrich\", \"validate\"]\nprint(f\"test_spec provenance: {test_spec.metadata.get('cloned_from')}\")  # \"base_etl\"\n</code></pre>"},{"location":"creational/prototype/#shallow-copy-for-shared-resources","title":"Shallow Copy for Shared Resources","text":"<pre><code># When you want nested structures to share references\n# (e.g., shared resource pool configuration)\nshared_spec = base_spec.clone(deep=False)\nshared_spec.resources[\"executor_memory\"] = \"8G\"\n\n# With shallow copy, the list reference is shared\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Changes are reflected\n</code></pre>"},{"location":"creational/prototype/#templating-with-overrides","title":"Templating with Overrides","text":"<pre><code># Template for different environments\nprod_spec = base_spec.clone(\n    name=\"prod_etl\",\n    input_source=\"s3://prod/data\",\n    resources={\"executor_memory\": \"16G\", \"partitions\": 500},\n)\n\n# Create a debug variant\ndebug_spec = prod_spec.clone(\n    name=\"prod_etl_debug\",\n    transforms=[\"parse\", \"clean\"],  # Skip expensive enrich\n    resources={\"executor_memory\": \"2G\", \"partitions\": 10},\n)\n</code></pre>"},{"location":"creational/prototype/#shallow-vs-deep-clone","title":"Shallow vs Deep Clone","text":"Shallow Copy Deep Copy <code>cloned_transforms = copy(self.transforms)</code> <code>cloned_transforms = deepcopy(self.transforms)</code> New list, but items still shared Completely independent copy Faster, less memory Safer, prevents accidental mutations Use when: only changing top-level fields Use when: modifying nested structures <pre><code># Shallow clone example\nbase_transforms = [{\"type\": \"filter\"}]\nclone1 = copy(base_transforms)\nclone1[0][\"condition\"] = \"active=true\"\n# base_transforms ALSO changed! \u26a0\ufe0f\n\n# Deep clone example\nclone2 = deepcopy(base_transforms)\nclone2[0][\"condition\"] = \"active=true\"\n# base_transforms unchanged \u2713\n</code></pre>"},{"location":"creational/prototype/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Avoid duplication of complex configs Deep copying can be slow/memory-intensive Track config evolution via provenance Extra book-keeping required Support templates and variants Shallow copy gotchas if not careful Faster than manual rebuilding Complex graphs may need custom cloning"},{"location":"creational/singleton/","title":"Singleton Pattern","text":""},{"location":"creational/singleton/#problem","title":"Problem","text":"<p>When you run microservices with multiple threads or async workers, you need a single, shared instance of expensive resources like database connections or API clients. The <code>ParagoNClientManager</code> singleton ensures all threads in a process use the same cached client instance, avoiding redundant credentials and connection overhead.</p>"},{"location":"creational/singleton/#solution","title":"Solution","text":"<p>Create a class that enforces single instantiation:</p> <pre><code>from threading import Lock\n\nclass ParagonNSingleton:\n    _instance = None\n    _lock = Lock()\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.token = 0\n\n    def _fetch_token(self) -&gt; int:\n        # Simulate fetching a token using the API key\n        with self._lock:\n            self.token += 1\n            return self.token\n\n    def refresh_token(self):\n        self.token = self._fetch_token()\n        return self.token\n\n\nclass ParagonNSingletonManager:\n    _instance = None \n    _lock = Lock()\n\n    def __new__(cls, *args, **kwargs):\n        raise NotImplementedError(\"Use get_instance() method to get the singleton instance.\")\n\n    @classmethod \n    def get_instance(cls, api_key: str) -&gt; \"ParagonNSingletonManager\":\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(ParagonNSingletonManager)\n                    cls._instance.client = ParagonNSingleton(api_key)\n        return cls._instance\n\n    @classmethod\n    def get_client(cls, api_key: str) -&gt; ParagonNSingleton:\n        instance = cls.get_instance(api_key)\n        return instance.client\n</code></pre>"},{"location":"creational/singleton/#key-features","title":"Key Features","text":"<ul> <li>Thread-safe: Uses double-checked locking to prevent race conditions</li> <li>Lazy initialization: Client created only on first use</li> <li>Global access: <code>ParagoNClientManager().get_client()</code> from anywhere</li> <li>Credentials managed: API keys loaded once per process</li> </ul>"},{"location":"creational/singleton/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Get singleton client instance\na = ParagonNSingletonManager.get_client(\"my_api_key\")\nb = ParagonNSingletonManager.get_client(\"my_api_key\")\n\nprint(a is b)  # True - both are the same instance\n\n# Thread-safe access\ndef access_client():\n    client = ParagonNSingletonManager.get_client(\"my_api_key\")\n    client.refresh_token()\n    print(f\"Token: {client.token}\")\n\nimport threading\nthreads = [threading.Thread(target=access_client) for _ in range(5)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"creational/singleton/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Single shared instance reduces overhead Hard to test (global state) Thread-safe token refresh Difficult to mock in unit tests Credentials loaded once Tight coupling to singleton Easy global access Hidden dependencies"},{"location":"creational/singleton/#testing-tip","title":"Testing Tip","text":"<p>For unit tests, consider a test-friendly alternative: use dependency injection or a registry pattern instead of a pure singleton.</p> <pre><code># Testable version\nclient_manager = ParagoNClientManager()\n# Pass to handlers as dependency\ndef handle_user_request(user_id, client_manager=client_manager):\n    client = client_manager.get_client()\n    # ...\n</code></pre>"},{"location":"structural/","title":"Structural Patterns","text":"<p>Structural patterns deal with object composition, creating larger structures from classes and objects through inheritance and composition.</p>"},{"location":"structural/#overview","title":"Overview","text":"<p>In microservices, React integrations, and data pipelines, structural patterns help: - Adapt incompatible interfaces: Hide third-party API quirks (Adapter) - Decouple abstraction from implementation: Support multiple backends (Bridge) - Compose hierarchies: Build tree structures of operations (Composite) - Add behavior dynamically: Wrap objects with cross-cutting concerns (Decorator) - Simplify complex subsystems: Provide unified high-level interfaces (Facade) - Share expensive objects: Reduce memory footprint of large datasets (Flyweight) - Control access: Add layers like caching and rate-limiting (Proxy)</p>"},{"location":"structural/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Adapter Make incompatible interfaces work together Map ParagoN API to internal User DTO Bridge Decouple abstraction from implementation Ingestion logic + multiple storage backends Composite Treat individual and composite objects uniformly Tree of bulk operations across services Decorator Add responsibilities dynamically Wrap clients with retry/tracing/metrics Facade Provide simplified interface to complex subsystem Onboarding orchestration Flyweight Share fine-grained objects efficiently Shared metadata across Spark tasks Proxy Control access to another object Client wrapper with caching + rate-limiting"},{"location":"structural/#when-to-use","title":"When to Use","text":"<p>Choose structural patterns when you need to: - Work with external systems that have incompatible interfaces - Support multiple implementations transparently - Build flexible hierarchies of objects - Reduce memory usage for large numbers of similar objects - Add or modify behavior without changing class definitions</p>"},{"location":"structural/adapter/","title":"Adapter Pattern","text":""},{"location":"structural/adapter/#problem","title":"Problem","text":"<p>Your microservices and React frontend rely on consistent internal data models (DTOs). However, the ParagoN API (which you interact with via <code>ParagoNClient</code>) returns deeply nested JSON with inconsistent field names, optional fields, and different data types:</p>"},{"location":"structural/adapter/#paragon-api-response","title":"ParagoN API Response","text":"<pre><code>{\n  \"user_id\": \"12345\",\n  \"personal_info\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"contact\": {\n      \"email_addr\": \"john.doe@example.com\",\n      \"phone_num\": \"+1234567890\"\n    }\n  },\n  \"account_status\": \"ACTIVE\",\n  \"created_at\": \"2023-10-01T12:00:00Z\",\n  \"metadata\": {\n    \"tags\": [\"premium\", \"verified\"],\n    \"preferences\": {\"notifications\": true}\n  }\n}\n</code></pre>"},{"location":"structural/adapter/#your-internal-user-model","title":"Your Internal User Model","text":"<pre><code>{\n  \"id\": \"12345\",\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"phone\": \"+1234567890\",\n  \"status\": \"active\",\n  \"createdAt\": \"2023-10-01T12:00:00Z\",\n  \"tags\": [\"premium\", \"verified\"],\n  \"preferences\": {\"notifications\": true}\n}\n</code></pre> <p>Discrepancies: - Field name variations (<code>user_id</code> \u2192 <code>id</code>, <code>email_addr</code> \u2192 <code>email</code>, <code>account_status</code> \u2192 <code>status</code>) - Nested structure flattening - Type conversions (string status to lowercase) - Optional fields may be missing</p>"},{"location":"structural/adapter/#solution","title":"Solution","text":"<p>Implement bidirectional adapters that hide the complexity:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseAdapterModel(ABC):\n    \"\"\"Abstract adapter interface.\"\"\"\n\n    def __init__(self, external_data: dict):\n        self.external_data = external_data\n\n    @abstractmethod\n    def to_internal(self) -&gt; dict:\n        \"\"\"Convert external API response to internal model.\"\"\"\n        raise NotImplementedError(\"to_internal method not implemented\")\n\n    @abstractmethod\n    def to_external(self) -&gt; dict:\n        \"\"\"Convert internal model back to external API format.\"\"\"\n        raise NotImplementedError(\"to_external method not implemented\")\n\n\nclass ParagoNUserAdapter(BaseAdapterModel):\n    \"\"\"Maps ParagoN API user responses to internal User DTOs.\"\"\"\n\n    def to_internal(self) -&gt; dict:\n        \"\"\"Flatten ParagoN's nested response to internal User model.\"\"\"\n        data = self.external_data\n        internal_data = {\n            \"id\": data.get(\"user_id\"),\n            \"firstName\": data.get(\"personal_info\", {}).get(\"firstName\"),\n            \"lastName\": data.get(\"personal_info\", {}).get(\"lastName\"),\n            \"email\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"email_addr\"),\n            \"phone\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"phone_num\"),\n            \"status\": data.get(\"account_status\", \"\").lower(),\n            \"createdAt\": data.get(\"created_at\"),\n            \"tags\": data.get(\"metadata\", {}).get(\"tags\", []),\n            \"preferences\": data.get(\"metadata\", {}).get(\"preferences\", {}),\n        }\n        return internal_data\n\n    def to_external(self) -&gt; dict:\n        \"\"\"Map internal User model back to ParagoN format.\"\"\"\n        internal_data = self.external_data\n        external_data = {\n            \"user_id\": internal_data.get(\"id\"),\n            \"personal_info\": {\n                \"firstName\": internal_data.get(\"firstName\"),\n                \"lastName\": internal_data.get(\"lastName\"),\n                \"contact\": {\n                    \"email_addr\": internal_data.get(\"email\"),\n                    \"phone_num\": internal_data.get(\"phone\"),\n                },\n            },\n            \"account_status\": internal_data.get(\"status\", \"\").upper(),\n            \"created_at\": internal_data.get(\"createdAt\"),\n            \"metadata\": {\n                \"tags\": internal_data.get(\"tags\", []),\n                \"preferences\": internal_data.get(\"preferences\", {}),\n            },\n        }\n        return external_data\n</code></pre>"},{"location":"structural/adapter/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/adapter/#basic-adaptation-paragon-response-to-internal-model","title":"Basic Adaptation: ParagoN Response to Internal Model","text":"<pre><code># Receive data from ParagoN API\nparagon_response = {\n    \"user_id\": \"12345\",\n    \"personal_info\": {\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"contact\": {\n            \"email_addr\": \"john@example.com\",\n            \"phone_num\": \"+1234567890\"\n        }\n    },\n    \"account_status\": \"ACTIVE\",\n    \"created_at\": \"2023-10-01T12:00:00Z\",\n    \"metadata\": {\n        \"tags\": [\"premium\", \"verified\"],\n        \"preferences\": {\"notifications\": True}\n    }\n}\n\n# Adapt to internal model\nadapter = ParagoNUserAdapter(paragon_response)\ninternal_user = adapter.to_internal()\n\nprint(internal_user)\n# {\n#     \"id\": \"12345\",\n#     \"firstName\": \"John\",\n#     \"lastName\": \"Doe\",\n#     \"email\": \"john@example.com\",\n#     \"phone\": \"+1234567890\",\n#     \"status\": \"active\",  # lowercased\n#     \"createdAt\": \"2023-10-01T12:00:00Z\",\n#     \"tags\": [\"premium\", \"verified\"],\n#     \"preferences\": {\"notifications\": True}\n# }\n</code></pre>"},{"location":"structural/adapter/#reverse-adaptation-internal-model-to-paragon-format","title":"Reverse Adaptation: Internal Model to ParagoN Format","text":"<pre><code># Your internal user model (from React or database)\ninternal_user = {\n    \"id\": \"12345\",\n    \"firstName\": \"Jane\",\n    \"lastName\": \"Smith\",\n    \"email\": \"jane@example.com\",\n    \"phone\": \"+9876543210\",\n    \"status\": \"active\",\n    \"createdAt\": \"2023-10-01T12:00:00Z\",\n    \"tags\": [\"vip\"],\n    \"preferences\": {\"notifications\": False}\n}\n\n# Adapt back to ParagoN format for API request\nadapter = ParagoNUserAdapter(internal_user)\nparagon_format = adapter.to_external()\n\nprint(paragon_format)\n# {\n#     \"user_id\": \"12345\",\n#     \"personal_info\": {\n#         \"firstName\": \"Jane\",\n#         \"lastName\": \"Smith\",\n#         \"contact\": {\n#             \"email_addr\": \"jane@example.com\",\n#             \"phone_num\": \"+9876543210\"\n#         }\n#     },\n#     \"account_status\": \"ACTIVE\",  # uppercased\n#     \"created_at\": \"2023-10-01T12:00:00Z\",\n#     \"metadata\": {\n#         \"tags\": [\"vip\"],\n#         \"preferences\": {\"notifications\": False}\n#     }\n# }\n</code></pre>"},{"location":"structural/adapter/#handling-missing-fields","title":"Handling Missing Fields","text":"<pre><code># Incomplete ParagoN response (missing optional fields)\nincomplete_response = {\n    \"user_id\": \"99999\",\n    \"personal_info\": {\n        \"firstName\": \"Bob\"\n        # No lastName, contact info, etc.\n    },\n    \"account_status\": \"PENDING\"\n    # No created_at, metadata\n}\n\n# Adapter safely handles missing nested fields\nadapter = ParagoNUserAdapter(incomplete_response)\ninternal = adapter.to_internal()\n\nprint(internal)\n# {\n#     \"id\": \"99999\",\n#     \"firstName\": \"Bob\",\n#     \"lastName\": None,  # Missing field\n#     \"email\": None,\n#     \"phone\": None,\n#     \"status\": \"pending\",\n#     \"createdAt\": None,\n#     \"tags\": [],  # Default empty list\n#     \"preferences\": {}  # Default empty dict\n# }\n</code></pre>"},{"location":"structural/adapter/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Isolates your code from API changes Extra layer of indirection Type-safe internal models Maintenance overhead for each new API Easy to test and mock May add performance overhead Composable for complex transformations Debugging can be harder Consistent data contracts"},{"location":"structural/adapter/#performance-considerations","title":"Performance Considerations","text":"<p>For high-throughput scenarios: - Cache adapter instances if stateless - Minimize deep copying - Consider using <code>__slots__</code> for adapter classes - Profile serialization/deserialization overhead</p>"},{"location":"structural/bridge/","title":"Bridge Pattern","text":""},{"location":"structural/bridge/#problem","title":"Problem","text":"<p>Your ingestion logic must support multiple storage backends: - Production: S3 for large-scale data storage - Testing/Dev: Local filesystem for quick iteration - Multi-cloud: GCS for Google Cloud deployments</p> <p>Without Bridge, you'd have conditional logic sprinkled everywhere:</p> <pre><code># \u274c Tight coupling to specific backends\ndef ingest_data(backend_type, data):\n    if backend_type == \"s3\":\n        s3 = boto3.client(\"s3\")\n        s3.put_object(Bucket=\"my-bucket\", Key=\"data\", Body=data)\n    elif backend_type == \"local\":\n        with open(\"/data/local\", \"wb\") as f:\n            f.write(data)\n    elif backend_type == \"gcs\":\n        gcs = storage.Client()\n        bucket = gcs.bucket(\"my-bucket\")\n        blob = bucket.blob(\"data\")\n        blob.upload_from_string(data)\n</code></pre> <p>Problem: Ingestion logic is tightly coupled to storage implementation.</p>"},{"location":"structural/bridge/#solution","title":"Solution","text":"<p>Implement the bridge by separating the abstraction (ingestion logic) from implementations (storage backends):</p> <pre><code>from abc import ABC, abstractmethod\nfrom threading import Lock\n\n# Implementation interface - storage backends\nclass StorageBackend(ABC):\n    \"\"\"Abstract interface for storage operations.\"\"\"\n\n    @abstractmethod\n    def write(self, key: str, data: bytes):\n        \"\"\"Write data (idempotent - skips if exists).\"\"\"\n        raise NotImplementedError(\"write method not implemented\")\n\n    @abstractmethod\n    def update(self, key: str, data: bytes):\n        \"\"\"Update existing data (raises KeyError if not exists).\"\"\"\n        raise NotImplementedError(\"update method not implemented\")\n\n    @abstractmethod\n    def read(self, key: str) -&gt; bytes:\n        \"\"\"Read data from the backend.\"\"\"\n        raise NotImplementedError(\"read method not implemented\")\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if data exists in the backend.\"\"\"\n        raise NotImplementedError(\"exists method not implemented\")\n\n    @abstractmethod\n    def delete(self, key: str):\n        \"\"\"Delete data from the backend.\"\"\"\n        raise NotImplementedError(\"delete method not implemented\")\n\n\n# Concrete Implementation: S3\nclass S3Storage(StorageBackend):\n    \"\"\"AWS S3 storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        import boto3\n        self.s3_client = boto3.client(\"s3\")\n        self.bucket_name = bucket_name\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                print(f\"Data with key {key} already exists in S3. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in S3. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n        return response['Body'].read()\n\n    def exists(self, key: str) -&gt; bool:\n        try:\n            self.s3_client.head_object(Bucket=self.bucket_name, Key=key)\n            return True\n        except self.s3_client.exceptions.NoSuchKey:\n            return False\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.delete_object(Bucket=self.bucket_name, Key=key)\n\n\n# Concrete Implementation: Local Filesystem\nclass LocalStorage(StorageBackend):\n    \"\"\"Local filesystem storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, base_path: str):\n        import os\n        self.base_path = base_path\n        os.makedirs(base_path, exist_ok=True)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                print(f\"Data with key {key} already exists in Local Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in Local Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        with open(f\"{self.base_path}/{key}\", \"rb\") as f:\n            return f.read()\n\n    def exists(self, key: str) -&gt; bool:\n        import os\n        return os.path.exists(f\"{self.base_path}/{key}\")\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                import os\n                os.remove(f\"{self.base_path}/{key}\")\n\n\n# Concrete Implementation: Google Cloud Storage\nclass GCSStorage(StorageBackend):\n    \"\"\"Google Cloud Storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        from google.cloud import storage\n        self.client = storage.Client()\n        self.bucket = self.client.bucket(bucket_name)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                print(f\"Data with key {key} already exists in GCS Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in GCS Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        blob = self.bucket.blob(key)\n        return blob.download_as_bytes()\n\n    def exists(self, key: str) -&gt; bool:\n        blob = self.bucket.blob(key)\n        return blob.exists()\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.delete()\n\n\n# Concrete Implementation: In-memory Mock (for testing)\nclass MockStorage(StorageBackend):\n    \"\"\"In-memory mock storage for testing.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self):\n        self.storage = {}\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.storage[key] = data\n            else:\n                print(f\"Data with key {key} already exists in Mock Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.storage[key] = data\n            else:\n                raise KeyError(f\"Key {key} does not exist in Mock Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        return self.storage[key]\n\n    def exists(self, key: str) -&gt; bool:\n        return key in self.storage\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                del self.storage[key]\n\n\n# Abstraction: Ingestion Logic (independent of storage backend)\nclass IngestJob:\n    \"\"\"Ingestion job that works with any storage backend.\"\"\"\n\n    def __init__(self, backend: StorageBackend):\n        self.backend = backend\n\n    def execute(self, data_key: str, data: bytes):\n        \"\"\"Execute ingestion with idempotency check.\"\"\"\n        if not self.backend.exists(data_key):\n            self.backend.write(data_key, data)\n        else:\n            print(f\"Data with key {data_key} already exists. Skipping write.\")\n</code></pre>"},{"location":"structural/bridge/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/bridge/#basic-usage-production-testing-and-development","title":"Basic Usage: Production, Testing, and Development","text":"<pre><code># Production: use S3\ns3_backend = S3Storage(bucket_name=\"prod-data-lake\")\ningest_job = IngestJob(s3_backend)\ningest_job.execute(\"2025-12-16/customer_data.parquet\", b\"production data\")\n\n# Local Development: use local filesystem\nlocal_backend = LocalStorage(\"/tmp/data\")\ningest_job_local = IngestJob(local_backend)\ningest_job_local.execute(\"example_key\", b\"sample data\")\n\n# Testing: use in-memory mock storage\nmock_backend = MockStorage()\ningest_job_test = IngestJob(mock_backend)\ningest_job_test.execute(\"example_key\", b\"sample data in mock\")\n\n# Multi-cloud: use Google Cloud Storage\ngcs_backend = GCSStorage(bucket_name=\"multi-cloud-data\")\ningest_job_gcs = IngestJob(gcs_backend)\ningest_job_gcs.execute(\"2025-12-16/customer_data.parquet\", b\"gcs data\")\n</code></pre>"},{"location":"structural/bridge/#demonstrating-the-bridge-same-logic-different-backends","title":"Demonstrating the Bridge: Same Logic, Different Backends","text":"<pre><code># The key benefit: IngestJob logic is identical regardless of backend\ndef run_ingestion_pipeline(backend: StorageBackend, data_items: list):\n    \"\"\"Generic pipeline - works with ANY storage backend.\"\"\"\n    job = IngestJob(backend)\n    for item in data_items:\n        job.execute(item[\"key\"], item[\"data\"])\n    print(\"Pipeline complete!\")\n\n# Use the same function with different backends\ndata = [\n    {\"key\": \"file1.txt\", \"data\": b\"content1\"},\n    {\"key\": \"file2.txt\", \"data\": b\"content2\"},\n]\n\n# Try with mock first\nmock_backend = MockStorage()\nrun_ingestion_pipeline(mock_backend, data)\n\n# Deploy with S3\ns3_backend = S3Storage(bucket_name=\"my-bucket\")\nrun_ingestion_pipeline(s3_backend, data)\n\n# No code changes! Just swap the backend.\n</code></pre>"},{"location":"structural/bridge/#thread-safe-updates-with-write-and-update-methods","title":"Thread-Safe Updates with Write and Update Methods","text":"<pre><code># Write: idempotent, skips if exists (used for initial writes)\nbackend = MockStorage()\nbackend.write(\"user:123\", b'{\"name\": \"Alice\"}')  # Succeeds\nbackend.write(\"user:123\", b'{\"name\": \"Bob\"}')    # Skips (already exists)\n\n# Update: requires existing key (used for updates/patches)\ntry:\n    backend.update(\"user:123\", b'{\"name\": \"Bob\"}')  # Succeeds\n    backend.update(\"user:999\", b'{\"name\": \"Charlie\"}')  # Raises KeyError\nexcept KeyError as e:\n    print(f\"Update failed: {e}\")\n</code></pre>"},{"location":"structural/bridge/#concurrent-access-with-lock-protection","title":"Concurrent Access with Lock Protection","text":"<pre><code>import threading\n\ndef ingest_in_thread(backend: StorageBackend, key: str, data: bytes):\n    job = IngestJob(backend)\n    job.execute(key, data)\n\n# Test concurrent access with mock backend\nmock_backend = MockStorage()\nthreads = []\n\nfor i in range(5):\n    t = threading.Thread(\n        target=ingest_in_thread, \n        args=(mock_backend, \"concurrent_key\", f\"data_{i}\".encode())\n    )\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"Final data in mock: {mock_backend.read('concurrent_key')}\")\n# All threads are synchronized via Lock - no race conditions\n</code></pre>"},{"location":"structural/bridge/#testing-with-mock-backend","title":"Testing with Mock Backend","text":"<pre><code># Unit test example - no need for real AWS/GCS credentials\ndef test_ingest_job_with_mock_storage():\n    backend = MockStorage()\n    ingest_job = IngestJob(backend)\n\n    key = \"ingest_key\"\n    data = b\"ingest data\"\n\n    # Execute ingestion\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Verify data integrity\n    read_data = backend.read(key)\n    assert read_data == data\n\n    # Test idempotency: second execute should skip\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Cleanup\n    backend.delete(key)\n    assert backend.exists(key) == False\n</code></pre>"},{"location":"structural/bridge/#benefits","title":"Benefits","text":"Pros Cons Decouple job logic from storage Extra abstraction layers Add new backends without changing job code More code upfront Easy to test with mock storage Potential performance overhead Swap backends at runtime Interface mismatches between backends Support multi-cloud deployments"},{"location":"structural/bridge/#advanced-storage-factory","title":"Advanced: Storage Factory","text":"<pre><code>def create_storage(backend_type: str, **config) -&gt; StorageImplementation:\n    \"\"\"Factory for creating storage backends.\"\"\"\n    backends = {\n        \"s3\": lambda: S3Storage(**config),\n        \"local\": lambda: LocalStorage(**config),\n        \"gcs\": lambda: GCSStorage(**config),\n    }\n\n    if backend_type not in backends:\n        raise ValueError(f\"Unknown storage backend: {backend_type}\")\n\n    return backends[backend_type]()\n\n# Configuration-driven backend selection\nbackend_type = os.getenv(\"STORAGE_BACKEND\", \"s3\")\nstorage = create_storage(backend_type, bucket_name=\"my-bucket\")\njob = IngestJob(storage, {\"name\": \"my_job\"})\n</code></pre>"},{"location":"structural/composite/","title":"Composite Pattern","text":""},{"location":"structural/composite/#problem","title":"Problem","text":"<p>You need to model hierarchical operations performed across services. For example, a bulk-update composed of multiple sub-requests to different microservices:</p> <pre><code>Bulk Customer Update\n\u251c\u2500\u2500 Update Identity Service\n\u2502   \u251c\u2500\u2500 Update name\n\u2502   \u2514\u2500\u2500 Update email\n\u251c\u2500\u2500 Update Billing Service\n\u2502   \u2514\u2500\u2500 Update subscription status\n\u2514\u2500\u2500 Update Notification Service\n    \u2514\u2500\u2500 Re-subscribe to mailing list\n</code></pre> <p>Without Composite, you'd have deeply nested conditionals and ad-hoc aggregation logic.</p>"},{"location":"structural/composite/#solution","title":"Solution","text":"<p>Implement a tree structure where both leaf and composite nodes share a common interface:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass Operation(ABC):\n    \"\"\"Abstract operation that can be leaf or composite.\"\"\"\n\n    @abstractmethod\n    def execute(self) -&gt; Dict[str, Any]:\n        \"\"\"Execute and return result.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_status(self) -&gt; str:\n        \"\"\"Get operation status.\"\"\"\n        raise NotImplementedError\n\n\nclass LeafOperation(Operation):\n    \"\"\"Leaf: actual RPC to a microservice.\"\"\"\n\n    def __init__(self, service_name: str, operation: str, payload: Dict):\n        self.service_name = service_name\n        self.operation = operation\n        self.payload = payload\n        self.result = None\n        self.status = \"pending\"\n\n    def execute(self) -&gt; Dict[str, Any]:\n        \"\"\"Execute RPC to microservice.\"\"\"\n        print(f\"Executing {self.operation} on {self.service_name}...\")\n        # Call actual microservice\n        self.result = {\"service\": self.service_name, \"success\": True}\n        self.status = \"completed\"\n        return self.result\n\n    def get_status(self) -&gt; str:\n        return self.status\n\n\nclass CompositeOperation(Operation):\n    \"\"\"Composite: aggregates multiple operations.\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.children: List[Operation] = []\n        self.status = \"pending\"\n        self.results = {}\n\n    def add_operation(self, operation: Operation) -&gt; None:\n        \"\"\"Add child operation.\"\"\"\n        self.children.append(operation)\n\n    def execute(self) -&gt; Dict[str, Any]:\n        \"\"\"Execute all children.\"\"\"\n        print(f\"Executing composite operation: {self.name}\")\n        self.status = \"running\"\n\n        for child in self.children:\n            result = child.execute()\n            self.results[child.service_name if hasattr(child, 'service_name') else self.name] = result\n\n        self.status = \"completed\"\n        return self.results\n\n    def get_status(self) -&gt; str:\n        statuses = [child.get_status() for child in self.children]\n        if all(s == \"completed\" for s in statuses):\n            return \"completed\"\n        elif any(s == \"failed\" for s in statuses):\n            return \"failed\"\n        return \"running\"\n\n\n# Example: bulk customer update\ndef create_bulk_update_operation():\n    root = CompositeOperation(\"BulkCustomerUpdate\")\n\n    # Identity service updates\n    identity_ops = CompositeOperation(\"IdentityServiceBatch\")\n    identity_ops.add_operation(LeafOperation(\"identity\", \"update_name\", {\"user_id\": \"123\", \"name\": \"John Doe\"}))\n    identity_ops.add_operation(LeafOperation(\"identity\", \"update_email\", {\"user_id\": \"123\", \"email\": \"john@example.com\"}))\n    root.add_operation(identity_ops)\n\n    # Billing service update\n    root.add_operation(LeafOperation(\"billing\", \"update_subscription\", {\"user_id\": \"123\", \"status\": \"premium\"}))\n\n    # Notification service update\n    root.add_operation(LeafOperation(\"notifications\", \"subscribe\", {\"user_id\": \"123\", \"type\": \"marketing\"}))\n\n    return root\n\n# Execute\nbulk_op = create_bulk_update_operation()\nresults = bulk_op.execute()\nprint(f\"Final status: {bulk_op.get_status()}\")\n</code></pre>"},{"location":"structural/composite/#advantages","title":"Advantages","text":"<ul> <li>Treat leaf and composite nodes uniformly</li> <li>Easy to add new operation types</li> <li>Natural hierarchical structure</li> <li>Partial execution and error handling</li> </ul>"},{"location":"structural/decorator/","title":"Decorator Pattern","text":""},{"location":"structural/decorator/#problem","title":"Problem","text":"<p>You want to add cross-cutting features to API clients (retries, logging, metrics, tracing) without modifying their code:</p> <pre><code># \u274c Without decorators: bloated client class\nclass ParagoNClient:\n    def fetch_user(self, user_id):\n        # retry logic\n        for attempt in range(3):\n            try:\n                # logging\n                logger.info(f\"Fetching user {user_id}\")\n                # actual call\n                response = requests.get(f\"https://api.paragon.io/users/{user_id}\")\n                # tracing\n                span.log_event(\"fetch_user_success\")\n                # metrics\n                metrics.increment(\"paragon_api_calls\")\n                return response.json()\n            except Exception as e:\n                # error handling\n                logger.error(f\"Error: {e}\")\n                span.log_event(\"fetch_user_error\")\n</code></pre> <p>This is messy, hard to maintain, and makes the client class do too much.</p>"},{"location":"structural/decorator/#solution","title":"Solution","text":"<p>Use Decorators to wrap clients and add behavior dynamically:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Any, Callable\nfrom functools import wraps\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Abstract client interface\nclass ClientBase(ABC):\n    @abstractmethod\n    def fetch_user(self, user_id: str) -&gt; dict:\n        raise NotImplementedError\n\n\n# Real client\nclass ParagoNClient(ClientBase):\n    def fetch_user(self, user_id: str) -&gt; dict:\n        \"\"\"Bare-bones client.\"\"\"\n        import requests\n        response = requests.get(f\"https://api.paragon.io/users/{user_id}\")\n        return response.json()\n\n\n# Decorator: add retries\nclass RetryingClient(ClientBase):\n    def __init__(self, client: ClientBase, max_retries: int = 3):\n        self.client = client\n        self.max_retries = max_retries\n\n    def fetch_user(self, user_id: str) -&gt; dict:\n        for attempt in range(self.max_retries):\n            try:\n                return self.client.fetch_user(user_id)\n            except Exception as e:\n                if attempt &lt; self.max_retries - 1:\n                    logger.warning(f\"Retry {attempt + 1}/{self.max_retries} for user {user_id}\")\n                    time.sleep(2 ** attempt)  # Exponential backoff\n                else:\n                    raise\n\n\n# Decorator: add logging\nclass LoggingClient(ClientBase):\n    def __init__(self, client: ClientBase):\n        self.client = client\n\n    def fetch_user(self, user_id: str) -&gt; dict:\n        logger.info(f\"Fetching user {user_id}\")\n        try:\n            result = self.client.fetch_user(user_id)\n            logger.info(f\"Successfully fetched user {user_id}\")\n            return result\n        except Exception as e:\n            logger.error(f\"Failed to fetch user {user_id}: {e}\")\n            raise\n\n\n# Decorator: add metrics\nclass MetricsClient(ClientBase):\n    def __init__(self, client: ClientBase, metrics_registry):\n        self.client = client\n        self.metrics = metrics_registry\n\n    def fetch_user(self, user_id: str) -&gt; dict:\n        start = time.time()\n        try:\n            result = self.client.fetch_user(user_id)\n            duration = time.time() - start\n            self.metrics.histogram(\"paragon_api_duration_seconds\", duration)\n            self.metrics.counter(\"paragon_api_calls_total\", 1, tags={\"status\": \"success\"})\n            return result\n        except Exception as e:\n            self.metrics.counter(\"paragon_api_calls_total\", 1, tags={\"status\": \"error\"})\n            raise\n\n\n# Decorator: add tracing\nclass TracingClient(ClientBase):\n    def __init__(self, client: ClientBase, tracer):\n        self.client = client\n        self.tracer = tracer\n\n    def fetch_user(self, user_id: str) -&gt; dict:\n        with self.tracer.trace(\"fetch_user\") as span:\n            span.set_tag(\"user_id\", user_id)\n            try:\n                result = self.client.fetch_user(user_id)\n                span.set_tag(\"status\", \"success\")\n                return result\n            except Exception as e:\n                span.set_tag(\"status\", \"error\")\n                span.set_tag(\"error\", str(e))\n                raise\n</code></pre>"},{"location":"structural/decorator/#chainable-composition","title":"Chainable Composition","text":"<pre><code># Base client\nclient = ParagoNClient()\n\n# Add decorators in order\nclient = LoggingClient(client)\nclient = RetryingClient(client, max_retries=3)\nclient = MetricsClient(client, metrics_registry)\nclient = TracingClient(client, tracer)\n\n# Use the decorated client\nuser = client.fetch_user(\"user_123\")\n# \u2192 Tracing + Metrics + Retry + Logging automatically applied!\n</code></pre>"},{"location":"structural/decorator/#configuration-based-decorator-selection","title":"Configuration-Based Decorator Selection","text":"<pre><code>def create_decorated_client(config: Dict[str, Any]) -&gt; ClientBase:\n    \"\"\"Create client with decorators based on configuration.\"\"\"\n    client = ParagoNClient()\n\n    if config.get(\"enable_logging\"):\n        client = LoggingClient(client)\n\n    if config.get(\"enable_retries\"):\n        client = RetryingClient(client, max_retries=config.get(\"max_retries\", 3))\n\n    if config.get(\"enable_metrics\"):\n        client = MetricsClient(client, metrics_registry)\n\n    if config.get(\"enable_tracing\"):\n        client = TracingClient(client, tracer)\n\n    return client\n\n# Production\nprod_config = {\n    \"enable_logging\": True,\n    \"enable_retries\": True,\n    \"max_retries\": 5,\n    \"enable_metrics\": True,\n    \"enable_tracing\": True,\n}\nprod_client = create_decorated_client(prod_config)\n\n# Testing\ntest_config = {\n    \"enable_logging\": True,\n    \"enable_retries\": False,\n    \"enable_metrics\": False,\n    \"enable_tracing\": False,\n}\ntest_client = create_decorated_client(test_config)\n</code></pre>"},{"location":"structural/decorator/#advantages","title":"Advantages","text":"Pros Cons Add behavior without modifying client Order of decorators matters Highly composable Performance overhead for each layer Testable independently Can become hard to debug Configuration-driven Single Responsibility Principle"},{"location":"structural/facade/","title":"Facade Pattern","text":""},{"location":"structural/facade/#problem","title":"Problem","text":"<p>Your onboarding subsystem involves multiple complex interactions: - Identity service: create user account - Billing service: provision subscription - ParagoN service: register with external provider - Notification service: send welcome email</p> <p>Without Facade, controllers are cluttered with orchestration logic:</p> <pre><code># \u274c Complex, fragmented logic\n@app.post(\"/onboard\")\ndef onboard_user(user_data):\n    # Call identity service\n    identity_response = identity_client.create_user(user_data[\"email\"])\n    if not identity_response.success:\n        return {\"error\": \"Failed to create identity\"}\n\n    # Call billing service\n    billing_response = billing_client.provision_subscription(identity_response.user_id)\n    if not billing_response.success:\n        # Rollback identity?\n        return {\"error\": \"Failed to provision billing\"}\n\n    # Call ParagoN\n    paragon_response = paragon_client.register_user(user_data)\n    if not paragon_response.success:\n        # Rollback billing and identity?\n        return {\"error\": \"Failed to register with ParagoN\"}\n\n    # Call notifications\n    notif_response = notification_client.send_welcome_email(user_data[\"email\"])\n\n    # Handle partial failures, rollbacks, etc...\n</code></pre> <p>This is messy, error-prone, and hard to reuse.</p>"},{"location":"structural/facade/#solution","title":"Solution","text":"<p>Create a Facade that hides the complexity behind a simple high-level API:</p> <pre><code>from typing import Dict, Any, Tuple\nfrom enum import Enum\n\nclass OnboardingStatus(Enum):\n    SUCCESS = \"success\"\n    PARTIAL_FAILURE = \"partial_failure\"\n    FAILURE = \"failure\"\n\nclass OnboardingResult:\n    def __init__(self, status: OnboardingStatus, user_id: str = None, error: str = None):\n        self.status = status\n        self.user_id = user_id\n        self.error = error\n        self.details = {}\n\nclass OnboardingFacade:\n    \"\"\"High-level onboarding orchestration.\"\"\"\n\n    def __init__(self, identity_client, billing_client, paragon_client, notification_client):\n        self.identity = identity_client\n        self.billing = billing_client\n        self.paragon = paragon_client\n        self.notifications = notification_client\n\n    def onboard_user(self, user_data: Dict[str, Any]) -&gt; OnboardingResult:\n        \"\"\"Orchestrate complete onboarding process.\"\"\"\n        result = OnboardingResult(OnboardingStatus.SUCCESS)\n\n        try:\n            # Step 1: Create identity\n            identity_response = self.identity.create_user(user_data[\"email\"])\n            if not identity_response.success:\n                return OnboardingResult(\n                    OnboardingStatus.FAILURE,\n                    error=f\"Identity creation failed: {identity_response.error}\"\n                )\n            user_id = identity_response.user_id\n            result.user_id = user_id\n            result.details[\"identity\"] = {\"status\": \"success\", \"user_id\": user_id}\n\n            # Step 2: Provision billing\n            billing_response = self.billing.provision_subscription(\n                user_id,\n                plan_id=user_data.get(\"plan_id\", \"standard\")\n            )\n            if not billing_response.success:\n                # Partial failure: rollback identity\n                self._rollback_identity(user_id)\n                return OnboardingResult(\n                    OnboardingStatus.FAILURE,\n                    error=f\"Billing provisioning failed: {billing_response.error}\"\n                )\n            result.details[\"billing\"] = {\"status\": \"success\", \"subscription_id\": billing_response.subscription_id}\n\n            # Step 3: Register with ParagoN\n            try:\n                paragon_response = self.paragon.register_user(user_data)\n                if paragon_response.success:\n                    result.details[\"paragon\"] = {\"status\": \"success\"}\n                else:\n                    # Non-critical failure: mark as partial\n                    result.status = OnboardingStatus.PARTIAL_FAILURE\n                    result.details[\"paragon\"] = {\"status\": \"failed\", \"error\": paragon_response.error}\n            except Exception as e:\n                # Log but continue\n                result.status = OnboardingStatus.PARTIAL_FAILURE\n                result.details[\"paragon\"] = {\"status\": \"error\", \"message\": str(e)}\n\n            # Step 4: Send welcome email (fire-and-forget)\n            try:\n                self.notifications.send_welcome_email(\n                    email=user_data[\"email\"],\n                    user_id=user_id\n                )\n                result.details[\"notifications\"] = {\"status\": \"sent\"}\n            except Exception as e:\n                # Log but don't fail onboarding\n                result.details[\"notifications\"] = {\"status\": \"failed\", \"message\": str(e)}\n\n            return result\n\n        except Exception as e:\n            return OnboardingResult(\n                OnboardingStatus.FAILURE,\n                error=f\"Unexpected error: {str(e)}\"\n            )\n\n    def _rollback_identity(self, user_id: str):\n        \"\"\"Rollback identity creation on failure.\"\"\"\n        try:\n            self.identity.delete_user(user_id)\n        except Exception as e:\n            # Log rollback failure\n            print(f\"Rollback failed for user {user_id}: {e}\")\n\n\n# Usage\nfacade = OnboardingFacade(\n    identity_client=IdentityClient(),\n    billing_client=BillingClient(),\n    paragon_client=ParagoNClient(),\n    notification_client=NotificationClient(),\n)\n\n# Controllers are now clean\n@app.post(\"/onboard\")\ndef onboard_user_handler(user_data: Dict):\n    result = facade.onboard_user(user_data)\n\n    if result.status == OnboardingStatus.SUCCESS:\n        return {\"success\": True, \"user_id\": result.user_id}\n    elif result.status == OnboardingStatus.PARTIAL_FAILURE:\n        return {\"success\": True, \"user_id\": result.user_id, \"warnings\": result.details}\n    else:\n        return {\"success\": False, \"error\": result.error}, 400\n</code></pre>"},{"location":"structural/facade/#advantages","title":"Advantages","text":"Pros Cons Hides complex orchestration Can become complex itself over time Reusable across controllers Might do too much (God object) Clean error handling and rollback Hard to test all failure paths Idempotent operations Performance overhead of abstraction Easy to extend"},{"location":"structural/facade/#testing","title":"Testing","text":"<pre><code>class MockOnboardingFacade(OnboardingFacade):\n    \"\"\"Mock facade for testing.\"\"\"\n\n    def onboard_user(self, user_data):\n        # Return success immediately\n        return OnboardingResult(\n            OnboardingStatus.SUCCESS,\n            user_id=\"mock_user_123\"\n        )\n\n@pytest.fixture\ndef onboarding_facade():\n    return MockOnboardingFacade(\n        None, None, None, None  # All mocked\n    )\n\ndef test_onboarding_success(onboarding_facade):\n    result = onboarding_facade.onboard_user({\"email\": \"test@example.com\"})\n    assert result.status == OnboardingStatus.SUCCESS\n</code></pre>"},{"location":"structural/flyweight/","title":"Flyweight Pattern","text":""},{"location":"structural/flyweight/#problem","title":"Problem","text":"<p>Your Spark cluster processes millions of records that reference a small set of schema descriptors and lookup metadata. Storing each record's full metadata copy wastes memory and increases serialization overhead across worker processes.</p> <p>Example: Processing 1 million events, each with a schema descriptor (1KB each): - Without Flyweight: 1 million \u00d7 1KB = 1GB memory waste - With Flyweight: 1 shared descriptor \u00d7 1KB + 1 million references = ~8MB + references</p>"},{"location":"structural/flyweight/#solution","title":"Solution","text":"<p>Create a shared registry of immutable metadata objects that tasks reference by ID:</p> <pre><code>from typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nimport threading\n\n@dataclass(frozen=True)  # Immutable\nclass SchemaDescriptor:\n    \"\"\"Shared schema metadata.\"\"\"\n    schema_id: str\n    name: str\n    version: str\n    fields: tuple  # Immutable tuple of field definitions\n    created_at: str\n\n    def __hash__(self):\n        return hash(self.schema_id)\n\n\n@dataclass(frozen=True)\nclass LookupMetadata:\n    \"\"\"Shared lookup table metadata.\"\"\"\n    lookup_id: str\n    name: str\n    data: tuple  # Immutable, e.g., valid enum values\n    last_updated: str\n\n\nclass FlyweightRegistry:\n    \"\"\"Central registry for shared immutable objects.\"\"\"\n\n    def __init__(self):\n        self._schemas: Dict[str, SchemaDescriptor] = {}\n        self._lookups: Dict[str, LookupMetadata] = {}\n        self._lock = threading.RLock()\n\n    def register_schema(self, descriptor: SchemaDescriptor) -&gt; None:\n        \"\"\"Register a schema descriptor.\"\"\"\n        with self._lock:\n            self._schemas[descriptor.schema_id] = descriptor\n\n    def get_schema(self, schema_id: str) -&gt; Optional[SchemaDescriptor]:\n        \"\"\"Get schema by ID (thread-safe).\"\"\"\n        with self._lock:\n            return self._schemas.get(schema_id)\n\n    def register_lookup(self, metadata: LookupMetadata) -&gt; None:\n        \"\"\"Register lookup metadata.\"\"\"\n        with self._lock:\n            self._lookups[metadata.lookup_id] = metadata\n\n    def get_lookup(self, lookup_id: str) -&gt; Optional[LookupMetadata]:\n        \"\"\"Get lookup metadata by ID.\"\"\"\n        with self._lock:\n            return self._lookups.get(lookup_id)\n\n    def preload_common_schemas(self):\n        \"\"\"Preload commonly-used schemas.\"\"\"\n        # Schemas shared across many tasks\n        self.register_schema(SchemaDescriptor(\n            schema_id=\"event_v1\",\n            name=\"Event\",\n            version=\"1.0\",\n            fields=(\"timestamp\", \"user_id\", \"event_type\", \"properties\"),\n            created_at=\"2025-01-01\",\n        ))\n\n        self.register_schema(SchemaDescriptor(\n            schema_id=\"user_v1\",\n            name=\"User\",\n            version=\"1.0\",\n            fields=(\"user_id\", \"email\", \"status\", \"created_at\"),\n            created_at=\"2025-01-01\",\n        ))\n\n\n# Global registry\n_registry = FlyweightRegistry()\n_registry.preload_common_schemas()\n\n\nclass EventRecord:\n    \"\"\"Record that references shared schema via ID.\"\"\"\n\n    def __init__(self, schema_id: str, data: Dict[str, Any]):\n        self.schema_id = schema_id  # Just store ID\n        self.data = data\n\n    def get_schema(self) -&gt; SchemaDescriptor:\n        \"\"\"Retrieve shared schema from registry.\"\"\"\n        return _registry.get_schema(self.schema_id)\n\n    def validate(self) -&gt; bool:\n        \"\"\"Validate data against shared schema.\"\"\"\n        schema = self.get_schema()\n        if not schema:\n            raise ValueError(f\"Schema {self.schema_id} not found\")\n\n        # Check if all required fields present\n        for field in schema.fields:\n            if field not in self.data:\n                return False\n        return True\n\n\n# Usage in Spark pipeline\ndef process_events(event_iterator):\n    \"\"\"Spark task that processes events without storing schema copies.\"\"\"\n\n    for event_dict in event_iterator:\n        # Create event with reference to shared schema\n        record = EventRecord(\n            schema_id=\"event_v1\",\n            data=event_dict\n        )\n\n        # Validate using shared schema\n        if record.validate():\n            # Process validated event\n            process_valid_event(record)\n        else:\n            handle_invalid_event(record)\n\n\n# Broadcasting registry to workers\ndef spark_job(spark):\n    \"\"\"Spark job setup.\"\"\"\n\n    # Broadcast registry to all workers (one-time cost)\n    registry_broadcast = spark.sparkContext.broadcast(_registry)\n\n    # RDD/DataFrame processing\n    events_df = spark.read.parquet(\"s3://data/events/\")\n\n    # Each partition processes events using shared registry\n    events_df.foreach(lambda row: process_events_with_registry(row, registry_broadcast.value))\n</code></pre>"},{"location":"structural/flyweight/#benefits","title":"Benefits","text":"Pros Cons Massive memory savings for large datasets All flyweights must be immutable Reduced serialization overhead Adds indirection (registry lookups) Easier to update metadata globally Complexity of registry management Scales to millions of objects Thread safety needed"},{"location":"structural/flyweight/#advanced-lazy-loading-with-lru-cache","title":"Advanced: Lazy Loading with LRU Cache","text":"<pre><code>from functools import lru_cache\n\nclass CachedFlyweightRegistry(FlyweightRegistry):\n    \"\"\"Registry with LRU cache for frequently-accessed schemas.\"\"\"\n\n    @lru_cache(maxsize=1000)\n    def get_schema_cached(self, schema_id: str) -&gt; Optional[SchemaDescriptor]:\n        \"\"\"Cached lookup for hot schemas.\"\"\"\n        return self.get_schema(schema_id)\n</code></pre>"},{"location":"structural/flyweight/#when-to-use","title":"When to Use","text":"<ul> <li>Processing millions of records with shared metadata</li> <li>Memory constraints in distributed systems</li> <li>Metadata is immutable and rarely changes</li> <li>High serialization costs between worker processes</li> </ul>"},{"location":"structural/proxy/","title":"Proxy Pattern","text":""},{"location":"structural/proxy/#problem","title":"Problem","text":"<p>Your edge services aggregate calls to ParagoNClient on behalf of React frontends. You want to add: - Caching: Avoid repeated API calls for the same user - Rate-limiting: Respect ParagoN's rate limits - Circuit breaker: Gracefully handle ParagoN API outages</p> <p>Without Proxy, you'd scatter this logic throughout your code:</p> <pre><code># \u274c Messy: caching, rate-limiting, and circuit-breaking mixed with business logic\ncache = {}\n\ndef fetch_user_handler(user_id):\n    # Check cache\n    if user_id in cache:\n        return cache[user_id]\n\n    # Rate limit check\n    if should_rate_limit():\n        return {\"error\": \"Rate limited\"}\n\n    # Circuit breaker check\n    if circuit_breaker.is_open():\n        return {\"error\": \"Service unavailable\"}\n\n    try:\n        user = paragon_client.fetch_user(user_id)\n        cache[user_id] = user  # Cache result\n        return user\n    except Exception as e:\n        circuit_breaker.record_failure()\n        return {\"error\": str(e)}\n</code></pre> <p>This is mixed with business logic and hard to test.</p>"},{"location":"structural/proxy/#solution","title":"Solution","text":"<p>Create a Proxy that wraps the real client and adds cross-cutting concerns transparently:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nclass ClientInterface(ABC):\n    @abstractmethod\n    def fetch_user(self, user_id: str) -&gt; Dict[str, Any]:\n        raise NotImplementedError\n\n\nclass ParagoNClient(ClientInterface):\n    \"\"\"Real client - only handles API calls.\"\"\"\n\n    def fetch_user(self, user_id: str) -&gt; Dict[str, Any]:\n        # Actual HTTP call to ParagoN\n        import requests\n        response = requests.get(f\"https://api.paragon.io/users/{user_id}\")\n        return response.json()\n\n\nclass CachingProxy(ClientInterface):\n    \"\"\"Proxy: add caching layer.\"\"\"\n\n    def __init__(self, client: ClientInterface, ttl_seconds: int = 300):\n        self.client = client\n        self.cache: Dict[str, tuple] = {}  # (value, expires_at)\n        self.ttl = ttl_seconds\n\n    def fetch_user(self, user_id: str) -&gt; Dict[str, Any]:\n        # Check cache\n        if user_id in self.cache:\n            value, expires_at = self.cache[user_id]\n            if datetime.now() &lt; expires_at:\n                return value  # Return cached value\n            else:\n                del self.cache[user_id]  # Expired\n\n        # Fetch from real client\n        user = self.client.fetch_user(user_id)\n\n        # Cache result\n        self.cache[user_id] = (user, datetime.now() + timedelta(seconds=self.ttl))\n\n        return user\n\n\nclass RateLimitingProxy(ClientInterface):\n    \"\"\"Proxy: add rate limiting.\"\"\"\n\n    def __init__(self, client: ClientInterface, max_requests: int = 100, window_seconds: int = 60):\n        self.client = client\n        self.max_requests = max_requests\n        self.window = window_seconds\n        self.requests = []  # Timestamps of requests\n\n    def fetch_user(self, user_id: str) -&gt; Dict[str, Any]:\n        now = time.time()\n\n        # Remove old requests outside window\n        self.requests = [ts for ts in self.requests if now - ts &lt; self.window]\n\n        if len(self.requests) &gt;= self.max_requests:\n            raise Exception(f\"Rate limit exceeded: {self.max_requests} requests per {self.window}s\")\n\n        # Record this request\n        self.requests.append(now)\n\n        # Call real client\n        return self.client.fetch_user(user_id)\n\n\nclass CircuitBreaker:\n    \"\"\"Simple circuit breaker.\"\"\"\n\n    def __init__(self, failure_threshold: int = 5, timeout_seconds: int = 60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout_seconds\n        self.failures = 0\n        self.last_failure_time = None\n        self.state = \"closed\"  # closed, open, half-open\n\n    def record_success(self):\n        self.failures = 0\n        self.state = \"closed\"\n\n    def record_failure(self):\n        self.failures += 1\n        self.last_failure_time = time.time()\n        if self.failures &gt;= self.failure_threshold:\n            self.state = \"open\"\n\n    def is_open(self) -&gt; bool:\n        if self.state == \"closed\":\n            return False\n\n        # Check if timeout expired\n        if time.time() - self.last_failure_time &gt; self.timeout:\n            self.state = \"half-open\"\n            self.failures = 0\n            return False\n\n        return self.state == \"open\"\n\n\nclass CircuitBreakerProxy(ClientInterface):\n    \"\"\"Proxy: add circuit breaker.\"\"\"\n\n    def __init__(self, client: ClientInterface, breaker: CircuitBreaker):\n        self.client = client\n        self.breaker = breaker\n\n    def fetch_user(self, user_id: str) -&gt; Dict[str, Any]:\n        if self.breaker.is_open():\n            raise Exception(\"Circuit breaker open: ParagoN service unavailable\")\n\n        try:\n            user = self.client.fetch_user(user_id)\n            self.breaker.record_success()\n            return user\n        except Exception as e:\n            self.breaker.record_failure()\n            raise\n</code></pre>"},{"location":"structural/proxy/#composing-proxies","title":"Composing Proxies","text":"<pre><code># Build layered proxy stack\nreal_client = ParagoNClient()\n\n# Add rate limiting first\nrate_limited = RateLimitingProxy(real_client, max_requests=100)\n\n# Add circuit breaker\nbreaker = CircuitBreaker(failure_threshold=5)\nprotected = CircuitBreakerProxy(rate_limited, breaker)\n\n# Add caching last (cache failures too? Or only successes?)\ncached = CachingProxy(protected, ttl_seconds=300)\n\n# Use the fully proxied client\nuser = cached.fetch_user(\"user_123\")\n# \u2192 Caching + Circuit Breaker + Rate Limiting applied!\n</code></pre>"},{"location":"structural/proxy/#configuration-based-proxy-selection","title":"Configuration-Based Proxy Selection","text":"<pre><code>def create_proxied_client(config: Dict[str, Any]) -&gt; ClientInterface:\n    \"\"\"Factory for creating proxied clients.\"\"\"\n    client = ParagoNClient()\n\n    if config.get(\"enable_rate_limiting\"):\n        client = RateLimitingProxy(\n            client,\n            max_requests=config.get(\"max_requests\", 100),\n            window_seconds=config.get(\"rate_limit_window\", 60)\n        )\n\n    if config.get(\"enable_circuit_breaker\"):\n        breaker = CircuitBreaker(\n            failure_threshold=config.get(\"failure_threshold\", 5),\n            timeout_seconds=config.get(\"breaker_timeout\", 60)\n        )\n        client = CircuitBreakerProxy(client, breaker)\n\n    if config.get(\"enable_caching\"):\n        client = CachingProxy(\n            client,\n            ttl_seconds=config.get(\"cache_ttl\", 300)\n        )\n\n    return client\n\n# Production: full stack\nprod_client = create_proxied_client({\n    \"enable_rate_limiting\": True,\n    \"max_requests\": 1000,\n    \"enable_circuit_breaker\": True,\n    \"enable_caching\": True,\n    \"cache_ttl\": 600,\n})\n\n# Testing: only caching\ntest_client = create_proxied_client({\n    \"enable_caching\": True,\n    \"cache_ttl\": 10,\n})\n</code></pre>"},{"location":"structural/proxy/#advantages","title":"Advantages","text":"Pros Cons Transparent to callers Order of proxies matters Clean separation of concerns Performance overhead per layer Easy to enable/disable features Can become hard to debug Highly composable Complex stack can be confusing Policies configured externally"},{"location":"structural/proxy/#testing","title":"Testing","text":"<pre><code>def test_caching_proxy():\n    mock_client = MockParagoNClient()\n    proxy = CachingProxy(mock_client, ttl_seconds=10)\n\n    # First call\n    user1 = proxy.fetch_user(\"user_123\")\n    assert mock_client.call_count == 1\n\n    # Second call (cached)\n    user2 = proxy.fetch_user(\"user_123\")\n    assert mock_client.call_count == 1  # Still 1!\n    assert user1 == user2\n</code></pre>"}]}