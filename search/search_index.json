{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Gang of Four Design Patterns","text":"<p>Welcome to an interactive guide to design patterns tailored for your domain: microservices, React frontends, Spark data pipelines, and custom Python clients.</p>"},{"location":"#about-this-guide","title":"About This Guide","text":"<p>This repository implements the 23 Gang of Four design patterns with real-world examples from your development experience:</p> <ul> <li>Microservices: Building resilient distributed systems</li> <li>Frontend Integration: React components communicating with backend services</li> <li>Data Pipelines: Spark ETL jobs and orchestration</li> <li>API Clients: Custom Python clients for third-party systems (like <code>ParagoNClient</code>)</li> </ul> <p>Each pattern includes: - \ud83d\udcd6 Detailed explanation and motivation - \ud83d\udcbb Complete, runnable code examples - \ud83c\udfaf Real scenarios from your domain - \u26a1 Practical considerations and trade-offs - \ud83e\uddea Testing strategies</p>"},{"location":"#pattern-categories","title":"Pattern Categories","text":""},{"location":"#creational-patterns","title":"Creational Patterns","text":"<p>Deal with object creation mechanisms. Useful for managing dependencies, abstracting instantiation, and building complex configurations.</p> <ul> <li>Singleton: Single shared instance per process (e.g., <code>ParagoNClient</code> manager)</li> <li>Factory Method: Choose implementation based on configuration</li> <li>Abstract Factory: Create families of related objects</li> <li>Builder: Assemble complex objects step-by-step (e.g., <code>SparkJobBuilder</code>)</li> <li>Prototype: Clone and customize objects rapidly</li> </ul>"},{"location":"#structural-patterns","title":"Structural Patterns","text":"<p>Deal with object composition and how classes/objects relate. Useful for adapting interfaces, adding behavior, and managing hierarchies.</p> <ul> <li>Adapter: Map incompatible interfaces (e.g., ParagoN API \u2192 internal DTOs)</li> <li>Bridge: Decouple abstraction from implementation (e.g., ingestion + storage backends)</li> <li>Composite: Build tree structures of operations</li> <li>Decorator: Add behavior dynamically (e.g., retries, tracing, metrics)</li> <li>Facade: Simplify complex subsystems (e.g., user onboarding)</li> <li>Flyweight: Share immutable objects efficiently (e.g., Spark metadata)</li> <li>Proxy: Control access with policies (e.g., caching, rate-limiting)</li> </ul>"},{"location":"#behavioral-patterns","title":"Behavioral Patterns","text":"<p>Deal with object collaboration and responsibility distribution.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Explore a pattern: Pick one that matches your current challenge</li> <li>Read the problem: Understand the real-world scenario</li> <li>Study the solution: See the complete implementation</li> <li>Try it: Copy, adapt, and use in your code</li> <li>Extend it: Customize for your specific needs</li> </ol>"},{"location":"#example-builder-pattern","title":"Example: Builder Pattern","text":"<p>Looking to simplify Spark job configuration?</p> <pre><code>job_spec = (SparkJobBuilder()\n    .from_s3(\"prod-bucket\", \"data/customers/\")\n    .with_filter(\"status = 'active'\")\n    .with_windowing(\"tumbling\", duration=3600)\n    .with_resources(executors=10, memory_gb=4)\n    .build())\n</code></pre> <p>Learn more about the Builder pattern \u2192</p> <p>Last Updated: December 2025 | Version: 1.0</p>"},{"location":"behavioral/","title":"Behavioral Patterns","text":"<p>Behavioral patterns deal with object collaboration and responsibility distribution. They focus on how objects interact, communicate, and distribute work.</p>"},{"location":"behavioral/#overview","title":"Overview","text":"<p>In microservices, event-driven systems, and orchestration, behavioral patterns help: - Handle requests through chains: Pass requests along a chain of handlers (Chain of Responsibility) - Encapsulate operations: Queue, retry, and undo operations as objects (Command) - Define domain-specific languages: Parse and execute DSL scripts (Interpreter) - Iterate over collections: Traverse elements without exposing structure (Iterator) - Coordinate interactions: Centralize communication between components (Mediator) - Capture state: Save and restore state for undo/replay (Memento) - Notify dependents: React to state changes (Observer) - Alter behavior by state: Change behavior based on internal state (State) - Select algorithms: Switch algorithms at runtime (Strategy) - Define algorithm skeletons: Let subclasses override steps (Template Method) - Apply operations to elements: Perform operations without modifying objects (Visitor)</p>"},{"location":"behavioral/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Chain of Responsibility Pipeline of handlers Event validation \u2192 enrichment \u2192 routing Command Encapsulate operations Queue complex actions for async execution Interpreter Parse domain languages DSL for pipeline definitions Iterator Traverse collections Page through API results seamlessly Mediator Centralize coordination Orchestrate multi-service workflows Memento Capture/restore state Pipeline checkpoints and rollback Observer Notify dependents UI updates on backend state changes State Alter behavior by state Order lifecycle state machine Strategy Select algorithms Pluggable retry/backoff strategies Template Method Define algorithm skeleton ETL job template with customizable steps Visitor Apply operations Transform/validate config trees"},{"location":"behavioral/#when-to-use","title":"When to Use","text":"<p>Choose behavioral patterns when you need to: - Decouple senders from receivers - Encapsulate requests as objects - Define families of algorithms - Change object behavior at runtime - Coordinate complex interactions between components - Provide extensibility without modifying existing code</p>"},{"location":"behavioral/chain_of_responsibility/","title":"Chain of Responsibility Pattern","text":""},{"location":"behavioral/chain_of_responsibility/#problem","title":"Problem","text":"<p>Build a pipeline of handlers to process incoming events/messages in your data platform. Handlers include validation, enrichment (calling external APIs), authorization, and routing. Each handler may handle the message, modify it, or pass it to the next handler.</p> <p>Constraints &amp; hints: - Handlers should be easily composed and reordered. - Support short-circuiting, retries, and prioritized handling. - Useful for modularizing pre-processing logic before persisting or forwarding.</p> <p>Deliverable: describe handler interfaces and an example chain for event ingestion.</p>"},{"location":"behavioral/chain_of_responsibility/#solution","title":"Solution","text":"<p>Define an abstract <code>Handler</code> base class with a <code>handle</code> method that accepts an event and optionally passes it to the next handler. Create concrete handlers for specific processing steps (validation, enrichment, authorization, routing) and chain them together in priority order.</p> <pre><code>from abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Optional\n\n\n@dataclass\nclass Event:\n    \"\"\"Represents an event flowing through the handler chain.\"\"\"\n    event_id: str\n    payload: Dict[str, Any]\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    authorized: bool = False\n    route: Optional[str] = None\n\n\nclass Handler(ABC):\n    \"\"\"Abstract base class for handlers in the chain of responsibility pattern.\"\"\"\n\n    def __init__(\n        self, next_handler: Optional[\"Handler\"] = None, priority: int = 0\n    ) -&gt; None:\n        \"\"\"Initialize a handler.\"\"\"\n        self.next_handler = next_handler\n        self.priority = priority\n\n    @abstractmethod\n    def handle(self, event: Event) -&gt; Event:\n        \"\"\"Process the event and pass to next handler if present.\"\"\"\n        pass\n\n\nclass ValidationHandler(Handler):\n    \"\"\"Handler that validates event structure and content.\"\"\"\n\n    def handle(self, event: Event) -&gt; Event:\n        \"\"\"Validate the event.\"\"\"\n        print(\"Validating event\")\n        if not event.event_id:\n            raise ValueError(\"Invalid event: missing event_id\")\n        if self.next_handler:\n            return self.next_handler.handle(event)\n        return event\n\n\nclass EnrichmentHandler(Handler):\n    \"\"\"Handler that enriches event data with additional information.\"\"\"\n\n    def handle(self, event: Event) -&gt; Event:\n        \"\"\"Enrich the event with additional data.\"\"\"\n        print(\"Enriching event\")\n        event.payload[\"enriched\"] = True\n        if self.next_handler:\n            return self.next_handler.handle(event)\n        return event\n\n\nclass AuthorizationHandler(Handler):\n    \"\"\"Handler that authorizes the event.\"\"\"\n\n    def handle(self, event: Event) -&gt; Event:\n        \"\"\"Authorize the event.\"\"\"\n        print(\"Authorizing event\")\n        event.authorized = True\n        if self.next_handler:\n            return self.next_handler.handle(event)\n        return event\n\n\nclass RoutingHandler(Handler):\n    \"\"\"Handler that determines the routing destination for the event.\"\"\"\n\n    def handle(self, event: Event) -&gt; Event:\n        \"\"\"Route the event to its destination.\"\"\"\n        print(\"Routing event\")\n        event.route = \"default_route\"\n        if self.next_handler:\n            return self.next_handler.handle(event)\n        return event\n\n\ndef build_handler_chain() -&gt; Handler:\n    \"\"\"Build and return the handler chain in priority order.\"\"\"\n    routing_handler = RoutingHandler(priority=4)\n    authorization_handler = AuthorizationHandler(\n        next_handler=routing_handler, priority=3\n    )\n    enrichment_handler = EnrichmentHandler(\n        next_handler=authorization_handler, priority=2\n    )\n    validation_handler = ValidationHandler(next_handler=enrichment_handler, priority=1)\n\n    handlers = [\n        validation_handler,\n        enrichment_handler,\n        authorization_handler,\n        routing_handler,\n    ]\n    handlers.sort(key=lambda h: h.priority)\n    for i in range(len(handlers) - 1):\n        handlers[i].next_handler = handlers[i + 1]\n\n    return handlers[0]\n</code></pre>"},{"location":"behavioral/chain_of_responsibility/#key-features","title":"Key Features","text":"<ul> <li>Modular Processing: Each handler focuses on a single responsibility.</li> <li>Composable: Handlers can be easily reordered or added/removed without modifying others.</li> <li>Prioritized Execution: Handlers can be ordered by priority for flexible sequencing.</li> <li>Short-Circuiting: A handler can stop the chain if validation fails (via exceptions).</li> <li>Transparent Modification: Handlers can modify events before passing to the next handler.</li> <li>Decoupled Chain: Each handler only knows about the next handler, not the entire chain.</li> </ul>"},{"location":"behavioral/chain_of_responsibility/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Example usage\ndef main() -&gt; None:\n    \"\"\"Demonstrate chain of responsibility pattern with event processing.\"\"\"\n    event = Event(event_id=\"evt_123\", payload={\"data\": \"sample\"})\n    handler_chain = build_handler_chain()\n    processed_event = handler_chain.handle(event)\n\n    print(\"Event processed successfully:\")\n    print(f\"  Enriched: {processed_event.payload.get('enriched')}\")\n    print(f\"  Authorized: {processed_event.authorized}\")\n    print(f\"  Route: {processed_event.route}\")\n\n    # Check invalid event\n    try:\n        invalid_event = Event(event_id=None, payload={\"data\": \"sample\"})\n        handler_chain.handle(invalid_event)\n    except ValueError as e:\n        print(f\"Error processing event: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"behavioral/chain_of_responsibility/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Decouples sender from processors Request may not be handled Easy to add/remove handlers dynamically Hard to debug long chains Single Responsibility Principle Order dependency can be fragile Flexible handler ordering Potential performance overhead Clear separation of concerns Chain can become complex Supports short-circuiting Hard to trace execution flow"},{"location":"behavioral/chain_of_responsibility/#testing-tip","title":"Testing Tip","text":"<p>Mock handlers or use a test chain to verify event transformations:</p> <pre><code>def test_event_processing() -&gt; None:\n    \"\"\"Test successful event processing through the chain.\"\"\"\n    event = Event(event_id=\"evt_123\", payload={\"data\": \"sample\"})\n    handler_chain = build_handler_chain()\n    processed_event = handler_chain.handle(event)\n\n    assert processed_event.payload.get(\"enriched\") is True\n    assert processed_event.authorized is True\n    assert processed_event.route == \"default_route\"\n\n\ndef test_event_validation_failure() -&gt; None:\n    \"\"\"Test that validation fails for invalid events.\"\"\"\n    event = Event(event_id=None, payload={\"data\": \"sample\"})\n    handler_chain = build_handler_chain()\n    try:\n        handler_chain.handle(event)\n        assert False, \"Expected ValueError for invalid event\"\n    except ValueError as e:\n        assert str(e) == \"Invalid event: missing event_id\"\n</code></pre>"},{"location":"behavioral/chain_of_responsibility/#real-world-applications","title":"Real-World Applications","text":"<ul> <li>HTTP Request Processing: Middleware chains in web frameworks (authentication, logging, compression).</li> <li>Event Pipeline: Data ingestion with validation, enrichment, and routing stages.</li> <li>Log Processing: Handlers for filtering, formatting, and routing log messages.</li> <li>Payment Processing: Multi-stage approval chain with different authorization levels.</li> <li>Document Workflow: Sequential processing stages (validation, signing, archiving).</li> <li>API Gateway: Request/response processing with authentication, rate limiting, transformation.</li> </ul>"},{"location":"behavioral/command/","title":"Command Pattern","text":""},{"location":"behavioral/command/#problem","title":"Problem","text":"<p>You need to encapsulate complex operations (e.g., \"create customer across services\", \"provision resources\") as commands that can be queued, retried, logged, and undone when possible. Design a <code>Command</code> object model that supports asynchronous execution and persistent queuing.</p> <p>Constraints &amp; hints: - Commands should be serializable to store in a durable queue. - Support undo/compensating commands for failure recovery. - Useful for implementing background workers and sagas across microservices.</p> <p>Deliverable: specify the command interface and how commands are scheduled and retried in your system.</p>"},{"location":"behavioral/command/#solution","title":"Solution","text":"<p>Define an abstract <code>Command</code> class with <code>execute</code>, <code>undo</code>, <code>serialize</code>, and <code>deserialize</code> methods. Implement concrete commands for specific operations. Create a <code>CommandFactory</code> for deserialization and a <code>CommandScheduler</code> to manage command queuing and execution with automatic compensation on failure.</p> <pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import Any, ClassVar, Dict\nimport asyncio\n\n\nclass CommandTypes(str, Enum):\n    \"\"\"Enumeration of supported command types.\"\"\"\n    CREATE_CUSTOMER = \"create_customer\"\n    PROVISION_RESOURCES = \"provision_resources\"\n\n\nclass Command(ABC):\n    \"\"\"Abstract base class for commands that can be executed, undone, and serialized.\n\n    Commands encapsulate operations that can be queued, logged, retried, and rolled back.\n    They support serialization for durable storage and deserialization for recovery.\n    \"\"\"\n\n    @abstractmethod\n    async def execute(self) -&gt; None:\n        \"\"\"Execute the command.\"\"\"\n        pass\n\n    @abstractmethod\n    async def undo(self) -&gt; None:\n        \"\"\"Undo the command if possible.\"\"\"\n        pass\n\n    @abstractmethod\n    async def serialize(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize the command to a dictionary for storage.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    async def deserialize(cls, data: Dict[str, Any]) -&gt; \"Command\":\n        \"\"\"Deserialize a command from a dictionary.\"\"\"\n        pass\n\n\nclass CreateCustomerCommand(Command):\n    \"\"\"Command to create a new customer.\"\"\"\n\n    def __init__(self, customer_id: str, customer_data: Dict[str, Any]) -&gt; None:\n        \"\"\"Initialize the create customer command.\"\"\"\n        self.customer_id = customer_id\n        self.customer_data = customer_data\n\n    async def execute(self) -&gt; None:\n        \"\"\"Create the customer.\"\"\"\n        print(f\"Creating customer {self.customer_id} with data {self.customer_data}\")\n\n    async def undo(self) -&gt; None:\n        \"\"\"Delete the created customer (compensation).\"\"\"\n        print(f\"Deleting customer {self.customer_id}\")\n\n    async def serialize(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize the command for storage.\"\"\"\n        return {\n            \"type\": CommandTypes.CREATE_CUSTOMER,\n            \"customer_id\": self.customer_id,\n            \"customer_data\": self.customer_data,\n        }\n\n    @classmethod\n    async def deserialize(cls, data: Dict[str, Any]) -&gt; \"CreateCustomerCommand\":\n        \"\"\"Deserialize from dictionary.\"\"\"\n        return cls(customer_id=data[\"customer_id\"], customer_data=data[\"customer_data\"])\n\n\nclass ProvisionResourcesCommand(Command):\n    \"\"\"Command to provision cloud resources.\"\"\"\n\n    def __init__(self, resource_id: str, resource_config: Dict[str, Any]) -&gt; None:\n        \"\"\"Initialize the provision resources command.\"\"\"\n        self.resource_id = resource_id\n        self.resource_config = resource_config\n\n    async def execute(self) -&gt; None:\n        \"\"\"Provision the resources.\"\"\"\n        print(f\"Provisioning resources {self.resource_id} with config {self.resource_config}\")\n\n    async def undo(self) -&gt; None:\n        \"\"\"Deprovision the resources (compensation).\"\"\"\n        print(f\"Deprovisioning resources {self.resource_id}\")\n\n    async def serialize(self) -&gt; Dict[str, Any]:\n        \"\"\"Serialize the command for storage.\"\"\"\n        return {\n            \"type\": CommandTypes.PROVISION_RESOURCES,\n            \"resource_id\": self.resource_id,\n            \"resource_config\": self.resource_config,\n        }\n\n    @classmethod\n    async def deserialize(cls, data: Dict[str, Any]) -&gt; \"ProvisionResourcesCommand\":\n        \"\"\"Deserialize from dictionary.\"\"\"\n        return cls(\n            resource_id=data[\"resource_id\"], resource_config=data[\"resource_config\"]\n        )\n\n\nclass CommandFactory:\n    \"\"\"Factory to create command instances from serialized data.\"\"\"\n\n    command_map: ClassVar[Dict[CommandTypes, type[Command]]] = {\n        CommandTypes.CREATE_CUSTOMER: CreateCustomerCommand,\n        CommandTypes.PROVISION_RESOURCES: ProvisionResourcesCommand,\n    }\n\n    @classmethod\n    async def create_command(cls, data: Dict[str, Any]) -&gt; Command:\n        \"\"\"Create a command instance from serialized data.\"\"\"\n        command_type = data.get(\"type\")\n        command_class = cls.command_map.get(command_type)\n        if not command_class:\n            raise ValueError(f\"Unknown command type: {command_type}\")\n        return await command_class.deserialize(data)\n\n\nclass CommandScheduler:\n    \"\"\"Schedules and executes commands from a queue.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the command scheduler with an empty queue.\"\"\"\n        self.queue: list[Dict[str, Any]] = []\n\n    async def schedule(self, command: Command) -&gt; None:\n        \"\"\"Schedule a command for execution.\"\"\"\n        serialized_command = await command.serialize()\n        self.queue.append(serialized_command)\n        print(f\"Scheduled command: {serialized_command}\")\n\n    async def execute_next(self) -&gt; None:\n        \"\"\"Execute the next command in the queue.\"\"\"\n        if not self.queue:\n            print(\"No commands to execute.\")\n            return\n\n        serialized_command = self.queue.pop(0)\n        command = await CommandFactory.create_command(serialized_command)\n        try:\n            await command.execute()\n            print(f\"Executed command: {serialized_command}\")\n        except Exception as e:\n            print(f\"Command execution failed: {e}. Attempting to undo.\")\n            await command.undo()\n            print(f\"Undid command: {serialized_command}\")\n</code></pre>"},{"location":"behavioral/command/#key-features","title":"Key Features","text":"<ul> <li>Encapsulation: Operations are wrapped as command objects, making them first-class values.</li> <li>Serialization: Commands can be persisted to a durable queue for recovery.</li> <li>Asynchronous Execution: Built on async/await for non-blocking execution.</li> <li>Automatic Compensation: Failed commands trigger undo operations in reverse order.</li> <li>Factory Pattern: Dynamic command creation from serialized data.</li> <li>Extensibility: New commands can be added by implementing the Command interface.</li> </ul>"},{"location":"behavioral/command/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code>async def main() -&gt; None:\n    \"\"\"Demonstrate command execution using the CommandScheduler.\"\"\"\n    scheduler = CommandScheduler()\n\n    # Schedule customer creation\n    command = CreateCustomerCommand(\n        customer_id=\"123\", customer_data={\"name\": \"John Doe\"}\n    )\n    await scheduler.schedule(command)\n    await scheduler.execute_next()\n\n    # Schedule resource provisioning\n    command2 = ProvisionResourcesCommand(\n        resource_id=\"res-456\", resource_config={\"type\": \"vm\", \"size\": \"large\"}\n    )\n    await scheduler.schedule(command2)\n    await scheduler.execute_next()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"behavioral/command/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Encapsulates operations as objects Adds complexity for simple operations Supports undo/compensation patterns Requires careful design of undo logic Enables queuing and retries Async operations can be hard to debug Serializable for durability Potential memory overhead if many commands queued Decouples requestor from executor Need to manage command ordering Supports logging and auditing Factory can become large with many commands"},{"location":"behavioral/command/#testing-tip","title":"Testing Tip","text":"<p>Mock the underlying services to test command behavior without side effects:</p> <pre><code>from unittest.mock import AsyncMock, Mock\n\nasync def test_command_execution():\n    \"\"\"Test command execution without side effects.\"\"\"\n    # Test successful execution\n    command = CreateCustomerCommand(\n        customer_id=\"test123\", customer_data={\"name\": \"Test User\"}\n    )\n    await command.execute()\n    await command.undo()\n\n    # Test serialization round-trip\n    serialized = await command.serialize()\n    deserialized = await CreateCustomerCommand.deserialize(serialized)\n    assert command.customer_id == deserialized.customer_id\n    assert command.customer_data == deserialized.customer_data\n</code></pre>"},{"location":"behavioral/command/#real-world-applications","title":"Real-World Applications","text":"<ul> <li>Microservices Sagas: Orchestrate multi-service transactions with compensation.</li> <li>Background Jobs: Queue long-running operations for asynchronous processing.</li> <li>Audit Logging: Track all operations in a durable, auditable queue.</li> <li>Distributed Transactions: Implement two-phase commit patterns.</li> <li>Event Sourcing: Store commands as events for complete history.</li> </ul>"},{"location":"behavioral/iterator/","title":"Iterator Pattern","text":""},{"location":"behavioral/iterator/#problem","title":"Problem","text":"<p>Third-party APIs return paginated results. Build an <code>Iterator</code> abstraction that yields domain objects across pages transparently and integrates with your async and sync callers (both backend and ingestion workers).</p> <p>Constraints &amp; hints: - Support backpressure for streaming consumers. - Provide adapters for both sync and async iteration. - Handle rate limits and transient errors while paginating.</p> <p>Deliverable: define an iterator interface and examples for iterating over a paginated ParagoN endpoint.</p>"},{"location":"behavioral/iterator/#solution","title":"Solution","text":"<p>Define an iterator that handles pagination transparently, providing both synchronous and asynchronous implementations. The iterator buffers results from pages and yields them one by one, fetching new pages as needed.</p> <pre><code>\"\"\"\nProblem: Third-party APIs return paginated results. Build an `Iterator` abstraction that yields domain objects across\npages transparently and integrates with your async and sync callers (both backend and ingestion workers).\n\nConstraints &amp; hints:\n- Support backpressure for streaming consumers.\n- Provide adapters for both sync and async iteration.\n- Handle rate limits and transient errors while paginating.\n\nDeliverable: define an iterator interface and examples for iterating over a paginated ParagoN endpoint.\n\"\"\"\n\n\nclass SyncCaller:\n    def fetch_page(self, page_token=None):\n        # Simulate fetching a page of results from a paginated API\n        if page_token is None:\n            return {\"data\": [1, 2, 3], \"next_page_token\": \"token1\"}\n        elif page_token == \"token1\":\n            return {\"data\": [4, 5, 6], \"next_page_token\": \"token2\"}\n        elif page_token == \"token2\":\n            return {\"data\": [7, 8, 9], \"next_page_token\": None}\n        else:\n            return {\"data\": [], \"next_page_token\": None}\n\n\nclass AsyncCaller:\n    async def fetch_page(self, page_token=None):\n        # Simulate fetching a page of results from a paginated API\n        if page_token is None:\n            return {\"data\": [1, 2, 3], \"next_page_token\": \"token1\"}\n        elif page_token == \"token1\":\n            return {\"data\": [4, 5, 6], \"next_page_token\": \"token2\"}\n        elif page_token == \"token2\":\n            return {\"data\": [7, 8, 9], \"next_page_token\": None}\n        else:\n            return {\"data\": [], \"next_page_token\": None}\n\n\nclass PaginatedIterator:\n    def __init__(self, caller):\n        self.caller = caller\n        self.page_token = None\n        self.buffer = []\n        self.finished = False\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if not self.buffer and not self.finished:\n            page = self.caller.fetch_page(self.page_token)\n            self.buffer.extend(page[\"data\"])\n            self.page_token = page[\"next_page_token\"]\n            if self.page_token is None:\n                self.finished = True\n        if not self.buffer:\n            raise StopIteration\n        return self.buffer.pop(0)\n\n\nclass AsyncPaginatedIterator:\n    def __init__(self, caller):\n        self.caller = caller\n        self.page_token = None\n        self.buffer = []\n        self.finished = False\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        if not self.buffer and not self.finished:\n            page = await self.caller.fetch_page(self.page_token)\n            self.buffer.extend(page[\"data\"])\n            self.page_token = page[\"next_page_token\"]\n            if self.page_token is None:\n                self.finished = True\n        if not self.buffer:\n            raise StopAsyncIteration\n        return self.buffer.pop(0)\n\n\n# Example usage for sync iterator\nsync_caller = SyncCaller()\nsync_iterator = PaginatedIterator(sync_caller)\nfor item in sync_iterator:\n    print(f\"Sync item: {item}\")\n\n# Example usage for async iterator\nimport asyncio\n\n\nasync def async_main():\n    async_caller = AsyncCaller()\n    async_iterator = AsyncPaginatedIterator(async_caller)\n    async for item in async_iterator:\n        print(f\"Async item: {item}\")\n\n\nasyncio.run(async_main())\n\n# Unit tests with pytest\nimport pytest\n\n\ndef test_sync_iterator():\n    sync_caller = SyncCaller()\n    sync_iterator = PaginatedIterator(sync_caller)\n    results = list(sync_iterator)\n    assert results == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n@pytest.mark.asyncio\nasync def test_async_iterator():\n    async_caller = AsyncCaller()\n    async_iterator = AsyncPaginatedIterator(async_caller)\n    results = []\n    async for item in async_iterator:\n        results.append(item)\n    assert results == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"behavioral/iterator/#when-to-use","title":"When to Use","text":"<p>Use the Iterator pattern when you need to traverse collections or data sources that are paginated or have complex access patterns. It's particularly useful for: - Paginated API results - Large datasets that can't fit in memory - Streaming data sources - Providing a uniform interface for different data access methods (sync/async) - Implementing custom collection classes that need to support iteration /workspaces/design-patterns-gog/docs/behavioral/iterator.md"},{"location":"behavioral/mediator/","title":"Mediator Pattern","text":""},{"location":"behavioral/mediator/#problem","title":"Problem","text":"<p>Several modules (auth, billing, notifications, UI updates) need coordinated interactions during user lifecycle operations. Design a <code>Mediator</code> that centralizes interaction logic so modules don't reference each other directly.</p> <p>Constraints &amp; hints: - Mediator should manage sequencing, retries, and error propagation between participants. - Useful for reducing coupling between microservices or in-process modules. - Consider pluggable participants and observability hooks.</p> <p>Deliverable: define the mediator contract and an example scenario for user profile synchronization.</p>"},{"location":"behavioral/mediator/#solution","title":"Solution","text":"<p>Define an abstract <code>MediatorStep</code> class with <code>execute</code> and <code>compensate</code> methods. Implement concrete steps that wrap individual services. Create a <code>UserLifecycleMediator</code> that orchestrates the steps with built-in retry logic and transactional compensation (rollback) on failure.</p> <pre><code>from abc import ABC, abstractmethod\nimport random\n\n\nclass BillingService:\n    \"\"\"Service responsible for charging users.\"\"\"\n\n    def charge_user(self, user_id: int, amount: float) -&gt; None:\n        \"\"\"Charge a user the specified amount.\"\"\"\n        if random.choice([True, False]):\n            raise Exception(\"Billing service failed.\")\n        print(f\"Charging user {user_id} an amount of {amount}.\")\n\n\nclass AuthService:\n    \"\"\"Service responsible for user authentication.\"\"\"\n\n    def authenticate_user(self, user_id: int) -&gt; None:\n        \"\"\"Authenticate a user.\"\"\"\n        if random.choice([True, False]):\n            raise Exception(\"Authentication service failed.\")\n        print(f\"Authenticating user {user_id}.\")\n\n\nclass NotificationService:\n    \"\"\"Service responsible for sending notifications to users.\"\"\"\n\n    def send_notification(self, user_id: int, message: str) -&gt; None:\n        \"\"\"Send a notification to a user.\"\"\"\n        if random.choice([True, False]):\n            raise Exception(\"Notification service failed.\")\n        print(f\"Sending notification to user {user_id}: {message}\")\n\n\nclass MediatorPayload:\n    \"\"\"Payload containing data for mediator step execution.\"\"\"\n\n    def __init__(self, user_id: int, amount: float, message: str) -&gt; None:\n        \"\"\"Initialize the mediator payload.\"\"\"\n        self.user_id = user_id\n        self.amount = amount\n        self.message = message\n\n\nclass MediatorStep(ABC):\n    \"\"\"Abstract base class for mediator steps.\"\"\"\n\n    @abstractmethod\n    def execute(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute the step.\"\"\"\n        pass\n\n    def compensate(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Compensate/rollback this step if a later step fails.\"\"\"\n        pass\n\n\nclass AuthStep(MediatorStep):\n    \"\"\"Mediator step for user authentication.\"\"\"\n\n    def __init__(self, auth_service: AuthService) -&gt; None:\n        \"\"\"Initialize the authentication step.\"\"\"\n        self.auth_service = auth_service\n\n    def execute(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute authentication.\"\"\"\n        self.auth_service.authenticate_user(payload.user_id)\n\n    def compensate(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Rollback authentication.\"\"\"\n        print(f\"Compensating authentication for user {payload.user_id}.\")\n\n\nclass BillingStep(MediatorStep):\n    \"\"\"Mediator step for charging users.\"\"\"\n\n    def __init__(self, billing_service: BillingService) -&gt; None:\n        \"\"\"Initialize the billing step.\"\"\"\n        self.billing_service = billing_service\n\n    def execute(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute billing charge.\"\"\"\n        self.billing_service.charge_user(payload.user_id, payload.amount)\n\n    def compensate(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Rollback billing charge.\"\"\"\n        print(f\"Compensating billing for user {payload.user_id}.\")\n\n\nclass NotificationStep(MediatorStep):\n    \"\"\"Mediator step for sending notifications.\"\"\"\n\n    def __init__(self, notification_service: NotificationService) -&gt; None:\n        \"\"\"Initialize the notification step.\"\"\"\n        self.notification_service = notification_service\n\n    def execute(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute notification send.\"\"\"\n        self.notification_service.send_notification(payload.user_id, payload.message)\n\n    def compensate(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Rollback notification send.\"\"\"\n        print(f\"Compensating notification for user {payload.user_id}.\")\n\n\nclass UserLifecycleMediator:\n    \"\"\"Mediator that orchestrates user lifecycle operations with retry and compensation logic.\"\"\"\n\n    def __init__(self, steps: list[MediatorStep], num_of_retry: int = 3) -&gt; None:\n        \"\"\"Initialize the mediator with a sequence of steps.\"\"\"\n        self.steps = steps\n        self.num_of_retry = num_of_retry\n\n    def execute(self, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute all steps in sequence with compensation on failure.\"\"\"\n        executed_steps: list[MediatorStep] = []\n        try:\n            for step in self.steps:\n                self.execute_with_retry(step, payload)\n                executed_steps.append(step)\n        except Exception as e:\n            print(f\"Error occurred: {e}. Initiating compensation.\")\n            for step in reversed(executed_steps):\n                step.compensate(payload)\n\n    def execute_with_retry(self, step: MediatorStep, payload: MediatorPayload) -&gt; None:\n        \"\"\"Execute a step with retry logic.\"\"\"\n        for attempt in range(self.num_of_retry):\n            try:\n                step.execute(payload)\n                return\n            except Exception as e:\n                print(f\"Retry {attempt + 1} failed: {e}\")\n                if attempt == self.num_of_retry - 1:\n                    print(\"Max retries reached. Raising exception.\")\n                    raise e\n</code></pre>"},{"location":"behavioral/mediator/#key-features","title":"Key Features","text":"<ul> <li>Centralized Orchestration: All module interactions flow through the mediator, reducing coupling.</li> <li>Retry Logic: Automatic retry mechanism built-in for fault tolerance.</li> <li>Compensation/Rollback: Failed operations trigger automatic compensation in reverse order.</li> <li>Pluggable Steps: New steps can be added without modifying the mediator logic.</li> <li>Observability: Clear logging of execution, retries, and compensation.</li> </ul>"},{"location":"behavioral/mediator/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Example usage\nif __name__ == \"__main__\":\n    auth_service = AuthService()\n    billing_service = BillingService()\n    notification_service = NotificationService()\n\n    steps = [\n        AuthStep(auth_service),\n        BillingStep(billing_service),\n        NotificationStep(notification_service),\n    ]\n\n    mediator = UserLifecycleMediator(steps)\n\n    payload = MediatorPayload(user_id=1, amount=100, message=\"Welcome to our service!\")\n    mediator.execute(payload)\n</code></pre>"},{"location":"behavioral/mediator/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Reduces coupling between modules Mediator can become a bottleneck or \"God Object\" Centralizes complex orchestration logic Harder to trace execution flow across steps Built-in retry and compensation patterns Requires careful design of step ordering Easy to add new steps Testing mediator logic can be complex Improves maintainability of interactions May add latency for sequential operations"},{"location":"behavioral/mediator/#testing-tip","title":"Testing Tip","text":"<p>Mock the services and steps in unit tests to verify mediator behavior without real side effects.</p> <pre><code>from unittest.mock import Mock\n\n# Mock services\nmock_auth = Mock()\nmock_billing = Mock()\nmock_notification = Mock()\n\nauth_step = AuthStep(mock_auth)\nbilling_step = BillingStep(mock_billing)\nnotification_step = NotificationStep(mock_notification)\n\nmediator = UserLifecycleMediator([auth_step, billing_step, notification_step])\n\n# Test payload\npayload = MediatorPayload(user_id=1, amount=100, message=\"Test\")\n\n# Execute and verify calls\nmediator.execute(payload)\nmock_auth.authenticate_user.assert_called_once_with(1)\nmock_billing.charge_user.assert_called_once_with(1, 100)\nmock_notification.send_notification.assert_called_once_with(1, \"Test\")\n</code></pre>"},{"location":"behavioral/memento/","title":"Memento Pattern","text":""},{"location":"behavioral/memento/#problem","title":"Problem","text":"<p>Long-running data pipelines occasionally need checkpoints and the ability to rollback to a prior state when a failed transformation corrupts downstream data. Design a <code>Memento</code> mechanism that captures checkpointed pipeline state (including offsets, schema versions, and short-lived credentials) and allows safe restoration.</p> <p>Constraints &amp; hints: - Mementos must be stored durably and be small enough to transfer between workers. - Respect sensitive data handling\u2014avoid storing secrets in plaintext. - Useful for reproducible retries and disaster recovery.</p> <p>Deliverable: describe what a <code>PipelineMemento</code> contains and how the orchestration layer applies it to resume or rollback.</p>"},{"location":"behavioral/memento/#solution","title":"Solution","text":"<p>The Memento pattern is used to capture and externalize an object's internal state so that the object can be restored to this state later. In this implementation, we have a <code>PipelineMemento</code> class that stores the pipeline's state, including offsets, schema versions, and credentials. The <code>PipelineOrchestrator</code> class manages the creation of mementos and restoration from them.</p> <p>The <code>PipelineMemento</code> contains: - Offsets: A dictionary mapping data source names to their current offsets. - Schema versions: A dictionary mapping data source names to their schema versions. - Credentials: A dictionary of short-lived credentials, handled securely.</p> <p>The orchestration layer applies the memento by restoring the state from the memento, allowing the pipeline to resume or rollback to a previous checkpoint.</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Any\nimport copy\n\n\nclass PipelineMemento:\n    \"\"\"A memento that captures the state of a data pipeline at a specific checkpoint.\n\n    This class encapsulates the pipeline's state including offsets, schema versions,\n    and credentials, providing a way to save and restore the state securely.\n    \"\"\"\n\n    def __init__(self, offsets: Dict[str, Any], schema_versions: Dict[str, Any], credentials: Dict[str, str]) -&gt; None:\n        \"\"\"Initialize the PipelineMemento.\n\n        Args:\n            offsets: A dictionary mapping data source names to their current offsets.\n            schema_versions: A dictionary mapping data source names to their schema versions.\n            credentials: A dictionary of short-lived credentials (should be handled securely).\n        \"\"\"\n        self._offsets = copy.deepcopy(offsets)\n        self._schema_versions = copy.deepcopy(schema_versions)\n        self._credentials = self._secure_credentials(credentials)\n\n    def _secure_credentials(self, credentials: Dict[str, str]) -&gt; Dict[str, str]:\n        \"\"\"Securely handle credentials (e.g., encrypt or mask sensitive data).\n\n        Args:\n            credentials: A dictionary of credentials.\n\n        Returns:\n            A secured version of the credentials.\n        \"\"\"\n        # Placeholder for actual secure handling logic\n        secured = {k: f\"secured_{v}\" for k, v in credentials.items()}\n        return secured\n\n    @property\n    def offsets(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the stored offsets.\n\n        Returns:\n            A deep copy of the offsets dictionary.\n        \"\"\"\n        return copy.deepcopy(self._offsets)\n\n    @property\n    def schema_versions(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the stored schema versions.\n\n        Returns:\n            A deep copy of the schema versions dictionary.\n        \"\"\"\n        return copy.deepcopy(self._schema_versions)\n\n    @property\n    def credentials(self) -&gt; Dict[str, str]:\n        \"\"\"Get the stored credentials.\n\n        Returns:\n            A deep copy of the credentials dictionary.\n        \"\"\"\n        return copy.deepcopy(self._credentials)\n\n\nclass PipelineOrchestrator:\n    \"\"\"Orchestration layer that manages pipeline execution and state restoration.\n\n    This class handles the creation of mementos for checkpointing and restoration\n    of pipeline state in case of failures.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the PipelineOrchestrator with empty state.\"\"\"\n        self.current_offsets: Dict[str, Any] = {}\n        self.current_schema_versions: Dict[str, Any] = {}\n        self.current_credentials: Dict[str, str] = {}\n\n    def create_memento(self) -&gt; PipelineMemento:\n        \"\"\"Create a memento capturing the current pipeline state.\n\n        Returns:\n            A PipelineMemento object containing the current state.\n        \"\"\"\n        return PipelineMemento(\n            offsets=self.current_offsets,\n            schema_versions=self.current_schema_versions,\n            credentials=self.current_credentials\n        )\n\n    def restore_from_memento(self, memento: PipelineMemento) -&gt; None:\n        \"\"\"Restore the pipeline state from a given memento.\n\n        Args:\n            memento: The PipelineMemento to restore from.\n        \"\"\"\n        self.current_offsets = memento.offsets\n        self.current_schema_versions = memento.schema_versions\n        self.current_credentials = memento.credentials\n        print(\"Pipeline state restored from memento.\")\n\n\n# Example usage:\norchestrator = PipelineOrchestrator()\norchestrator.current_offsets = {'source1': 100, 'source2': 200}\norchestrator.current_schema_versions = {'source1': 'v1.2', 'source2': 'v3.4'}\norchestrator.current_credentials = {'db_user': 'user123', 'db_pass': 'pass123'}\nmemento = orchestrator.create_memento()\n\n# Simulate a failure and restore from memento\norchestrator.current_offsets = {}\norchestrator.current_schema_versions = {}\norchestrator.current_credentials = {}\norchestrator.restore_from_memento(memento)\nprint(f\"Current Offsets: {orchestrator.current_offsets}\")\n</code></pre>"},{"location":"behavioral/observer/","title":"Observer Pattern","text":""},{"location":"behavioral/observer/#problem","title":"Problem","text":"<p>Build an <code>Observer</code>-style event system so backend services can notify interested frontends (React) and other services about state changes (e.g., customer status updates, pipeline job progress). Observers should be able to subscribe/unsubscribe dynamically and receive typed notifications.</p> <p>Constraints &amp; hints: - Support multiple transports (websocket for React, message bus for services). - Ensure subscribers receive delivered events at-least-once or exactly-once based on configuration. - Consider backpressure and slow consumer handling.</p> <p>Deliverable: design the observer registration API and an example notification flow for UI updates.</p>"},{"location":"behavioral/observer/#solution","title":"Solution","text":"<p>Define an abstract <code>Observer</code> class with an <code>update</code> method, and an abstract <code>Subject</code> class with methods for registering, unregistering, and notifying observers. Implement concrete subjects and observers for different transports like WebSocket and Message Bus.</p> <pre><code>from abc import abstractmethod, ABC\n\nWebSocketType = str  # Placeholder for actual WebSocket type\nMessageBusType = str  # Placeholder for actual Message Bus type\n\n\nclass Observer(ABC):\n    @abstractmethod\n    def update(self, event: str, data: dict):\n        \"\"\"Receive update from subject.\"\"\"\n        pass\n\n\nclass Subject(ABC):\n    @abstractmethod\n    def register_observer(self, observer: Observer):\n        \"\"\"Register an observer to receive updates.\"\"\"\n        pass\n\n    @abstractmethod\n    def unregister_observer(self, observer: Observer):\n        \"\"\"Unregister an observer from receiving updates.\"\"\"\n        pass\n\n    @abstractmethod\n    def notify_observers(self, event: str, data: dict):\n        \"\"\"Notify all registered observers about an event.\"\"\"\n        pass\n\n\nclass EventSubject(Subject):\n    def __init__(self):\n        self._observers = set()\n\n    def register_observer(self, observer: Observer):\n        self._observers.add(observer)\n\n    def unregister_observer(self, observer: Observer):\n        self._observers.discard(observer)\n\n    def notify_observers(self, event: str, data: dict):\n        for observer in self._observers:\n            observer.update(event, data)\n\n\nclass ReactObserver(Observer):\n    def __init__(self, websocket: WebSocketType):\n        self.websocket = websocket\n\n    def update(self, event: str, data: dict):\n        print(\n            f\"ReactObserver received event '{event}' with data: {data} with websocket {self.websocket}\"\n        )\n\n\nclass MessageBusObserver(Observer):\n    def __init__(self, message_bus: MessageBusType):\n        self.message_bus = message_bus\n\n    def update(self, event: str, data: dict):\n        print(\n            f\"MessageBusObserver received event '{event}' with data: {data} with message bus {self.message_bus}\"\n        )\n\n\n# Example usage\nif __name__ == \"__main__\":\n    subject = EventSubject()\n\n    react_observer = ReactObserver(websocket=\"WebSocketConnection1\")\n    message_bus_observer = MessageBusObserver(message_bus=\"MessageBusConnection1\")\n\n    subject.register_observer(react_observer)\n    subject.register_observer(message_bus_observer)\n\n    # Notify observers about a customer status update\n    subject.notify_observers(\n        event=\"customer_status_update\", data={\"customer_id\": 123, \"status\": \"active\"}\n    )\n\n    # Unregister the React observer and notify again\n    subject.unregister_observer(react_observer)\n    subject.notify_observers(\n        event=\"pipeline_job_progress\", data={\"job_id\": 456, \"progress\": \"50%\"}\n    )\n</code></pre>"},{"location":"behavioral/observer/#when-to-use","title":"When to Use","text":"<p>Use the Observer pattern when you need to notify multiple objects about changes in the state of another object, especially when the number of observers is dynamic and can change at runtime. It's particularly useful in event-driven systems, GUI frameworks, and distributed systems where components need to react to state changes without tight coupling. /workspaces/design-patterns-gog/docs/behavioral/observer.md"},{"location":"behavioral/strategy/","title":"Strategy Pattern","text":""},{"location":"behavioral/strategy/#problem","title":"Problem","text":"<p>Your clients must support different retry and backoff strategies depending on the endpoint (idempotent vs non-idempotent) and environment (dev vs prod). Implement a <code>Strategy</code> abstraction for pluggable retry/backoff and serialization strategies used across clients and pipeline connectors.</p>"},{"location":"behavioral/strategy/#solution","title":"Solution","text":"<p>Define an abstract <code>RetryStrategy</code> class with a <code>get_delay</code> method, then implement concrete strategies like <code>FixedIntervalStrategy</code> and <code>ExponentialBackoffWithJitterStrategy</code>. Integrate into a client class like <code>HttpClient</code> for automatic retries.</p> <pre><code>import time\nimport random\nfrom abc import ABC, abstractmethod\n\nclass RetryStrategy(ABC):\n    @abstractmethod\n    def get_delay(self, attempt: int) -&gt; float:\n        \"\"\"Calculate the delay before the next retry based on the attempt number.\"\"\"\n        pass\n\nclass FixedIntervalStrategy(RetryStrategy):\n    def __init__(self, interval: float):\n        self.interval = interval\n\n    def get_delay(self, attempt: int) -&gt; float:\n        return self.interval\n\nclass ExponentialBackoffWithJitterStrategy(RetryStrategy):\n    def __init__(self, base_delay: float, max_delay: float):\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n\n    def get_delay(self, attempt: int) -&gt; float:\n        exp_delay = min(self.base_delay * (2 ** attempt), self.max_delay)\n        jitter = random.uniform(0, exp_delay * 0.1)  # 10% jitter\n        return exp_delay + jitter\n\n\nclass HttpClient:\n    def __init__(self, strategy: RetryStrategy, max_attempts: int):\n        self.strategy = strategy\n        self.max_attempts = max_attempts\n\n    def get(self, url: str):\n        attempt = 0\n        while attempt &lt; self.max_attempts:\n            try:\n                # Simulate an HTTP GET request (replace with actual request logic)\n                print(f\"Attempt {attempt + 1} to GET {url}\")\n                if random.random() &lt; 0.7:  # Simulate a failure 70% of the time\n                    raise Exception(\"Simulated request failure\")\n                print(\"Request succeeded\")\n                return \"Response data\"\n            except Exception as e:\n                print(f\"Request failed: {e}\")\n                attempt += 1\n                if attempt &lt; self.max_attempts:\n                    delay = self.strategy.get_delay(attempt)\n                    print(f\"Retrying in {delay:.2f} seconds...\")\n                    time.sleep(delay)\n                else:\n                    print(\"Max attempts reached. Giving up.\")\n                    raise\n</code></pre>"},{"location":"behavioral/strategy/#key-features","title":"Key Features","text":"<ul> <li>Pluggable: Strategies can be swapped via configuration or runtime.</li> <li>Composable: Strategies can be extended or combined (e.g., add jitter to exponential backoff).</li> <li>Decoupled: Core client logic unchanged; reliability tuned via strategies.</li> </ul>"},{"location":"behavioral/strategy/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Example usage\nif __name__ == \"__main__\":\n    strategy = ExponentialBackoffWithJitterStrategy(base_delay=1.0, max_delay=10.0)\n    client = HttpClient(strategy=strategy, max_attempts=5)\n    try:\n        response = client.get('https://api.example.com/data')\n        print(f\"Received response: {response}\")\n    except Exception as e:\n        print(f\"Final failure: {e}\")\n</code></pre>"},{"location":"behavioral/strategy/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Easy to swap strategies Requires careful design to avoid tight coupling Composable implementations Potential for strategy explosion if not managed Improves reliability without changing core logic Adds abstraction overhead"},{"location":"behavioral/strategy/#testing-tip","title":"Testing Tip","text":"<p>Mock the strategy in unit tests to isolate client behavior.</p> <pre><code>from unittest.mock import Mock\n\n# Mock strategy for testing\nmock_strategy = Mock()\nmock_strategy.get_delay.return_value = 0.1\nclient = HttpClient(strategy=mock_strategy, max_attempts=3)\n# Test client logic without real delays\n</code></pre>"},{"location":"behavioral/template_method/","title":"Template Method Design Pattern","text":""},{"location":"behavioral/template_method/#problem","title":"Problem","text":"<p>In many applications, especially those involving data processing like ETL (Extract, Transform, Load) jobs, there's a common high-level algorithm that remains consistent, but the specific implementation details of individual steps vary depending on the context or data source. For example, an ETL process might always involve extracting data, transforming it, and then loading it. However, the extraction method for a CSV file will differ from that of a database, and similarly for transformation and loading.</p> <p>The challenge is to define a skeleton of an algorithm in an operation, deferring some steps to subclasses. This allows subclasses to redefine certain steps of an algorithm without changing the algorithm's structure.</p>"},{"location":"behavioral/template_method/#solution","title":"Solution","text":"<p>The Template Method design pattern addresses this by defining the skeleton of an algorithm in a base class, but allows subclasses to override specific steps without changing the overall structure. This promotes code reuse and ensures consistency in the high-level algorithm.</p> <p>In the provided Python example, <code>ETLTemplate</code> serves as the abstract base class that defines the template method <code>run()</code>. This method orchestrates the execution of the ETL process: <code>extract()</code>, <code>transform()</code>, <code>load()</code>, <code>validate()</code>, and <code>monitor()</code>. The <code>extract()</code>, <code>transform()</code>, and <code>load()</code> methods are declared as abstract, forcing subclasses to provide their concrete implementations. The <code>validate()</code> and <code>monitor()</code> methods provide default implementations that can also be overridden by subclasses if needed.</p> <p>This pattern ensures that: - The overall flow of the ETL job (extract -&gt; transform -&gt; load -&gt; validate -&gt; monitor) is fixed. - Subclasses can provide their specific logic for extraction, transformation, and loading. - Common pre/post steps like validation and monitoring are enforced.</p>"},{"location":"behavioral/template_method/#structure","title":"Structure","text":"<pre><code>+----------------+       +-------------------+\n| ETLTemplate    |       | ConcreteETL       |\n+----------------+       +-------------------+\n| + run()        |\u2500\u2500\u2500\u2500\u2500\u2500&gt;| + run()           |\n| + extract()    |&lt;\u2500\u2500\u2500\u2500\u2500 | + extract()       |\n| + transform()  |&lt;\u2500\u2500\u2500\u2500\u2500 | + transform()     |\n| + load()       |&lt;\u2500\u2500\u2500\u2500\u2500 | + load()          |\n| + validate()   |       | + validate() (opt)|\n| + monitor()    |       | + monitor() (opt) |\n+----------------+       +-------------------+\n</code></pre>"},{"location":"behavioral/template_method/#example-code","title":"Example Code","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass ETLTemplate(ABC):\n    def run(self):\n        self.extract()\n        self.transform()\n        self.load()\n        self.validate()\n        self.monitor()\n\n    @abstractmethod\n    def extract(self):\n        pass\n\n    @abstractmethod\n    def transform(self):\n        pass\n\n    @abstractmethod\n    def load(self):\n        pass\n\n    def validate(self):\n        print(\"Validating data...\")\n\n    def monitor(self):\n        print(\"Monitoring ETL job...\")\n\nclass CSVETL(ETLTemplate):\n    def extract(self):\n        print(\"Extracting data from CSV file...\")\n\n    def transform(self):\n        print(\"Transforming CSV data...\")\n\n    def load(self):\n        print(\"Loading data into the database...\")\n\n# Example usage\nif __name__ == \"__main__\":\n    etl_job = CSVETL()\n    etl_job.run()\n</code></pre>"},{"location":"behavioral/template_method/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\n\ndef test_csv_etl(capsys):\n    etl_job = CSVETL()\n    etl_job.run()\n    captured = capsys.readouterr()\n    assert \"Extracting data from CSV file...\" in captured.out\n    assert \"Transforming CSV data...\" in captured.out\n    assert \"Loading data into the database...\" in captured.out\n    assert \"Validating data...\" in captured.out\n    assert \"Monitoring ETL job...\" in captured.out\n</code></pre>"},{"location":"behavioral/template_method/#explanation","title":"Explanation","text":"<ul> <li><code>ETLTemplate</code> (Abstract Base Class): This class defines the template method <code>run()</code> which outlines the fixed sequence of operations. It declares abstract methods (<code>extract</code>, <code>transform</code>, <code>load</code>) that must be implemented by concrete subclasses, and provides concrete methods (<code>validate</code>, <code>monitor</code>) with default behaviors that can be optionally overridden.</li> <li><code>CSVETL</code> (Concrete Class): This class is a concrete implementation of <code>ETLTemplate</code>. It provides specific logic for <code>extracting</code> data from a CSV file, <code>transforming</code> that data, and <code>loading</code> it into a database. It reuses the default <code>validate</code> and <code>monitor</code> steps defined in the base class.</li> </ul> <p>This pattern is highly effective for building frameworks where invariant parts of an algorithm are encapsulated, and variant parts are left for implementers to define.</p>"},{"location":"behavioral/visitor/","title":"Visitor Pattern","text":""},{"location":"behavioral/visitor/#problem","title":"Problem","text":"<p>You need to perform different operations over a tree-like structure of configuration nodes (e.g., validation, metrics collection, migration). Implement a <code>Visitor</code> that can traverse config trees and run operations without modifying the tree classes.</p> <p>Constraints &amp; hints: - Visitors should be easy to add for new operations. - Traversal should support pre/post-order hooks and short-circuiting. - Useful for decoupling operations from the data model.</p> <p>Deliverable: define the visitor interface and an example visitor for validating config versions.</p>"},{"location":"behavioral/visitor/#solution","title":"Solution","text":"<p>Define an abstract <code>ConfigVisitor</code> class with <code>pre_hook</code>, <code>post_hook</code>, and <code>visit</code> methods, and a <code>traverse</code> method for tree traversal. Define an abstract <code>ConfigNode</code> class with an <code>accept</code> method. Implement concrete nodes and visitors.</p> <pre><code>\"\"\"\nProblem: You need to perform different operations over a tree-like structure of configuration nodes (e.g., validation,\nmetrics collection, migration). Implement a `Visitor` that can traverse config trees and run operations without\nmodifying the tree classes.\n\nConstraints &amp; hints:\n- Visitors should be easy to add for new operations.\n- Traversal should support pre/post-order hooks and short-circuiting.\n- Useful for decoupling operations from the data model.\n\nDeliverable: define the visitor interface and an example visitor for validating config versions.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\n\nclass ConfigNode(ABC):\n    \"\"\"Abstract base class for configuration nodes in a tree structure.\"\"\"\n\n    def __init__(self, children: Optional[List[\"ConfigNode\"]] = None):\n        \"\"\"Initialize a ConfigNode with optional children.\"\"\"\n        self.children = children\n\n    @abstractmethod\n    def accept(self, visitor: \"ConfigVisitor\") -&gt; None:\n        \"\"\"Accept a visitor to perform operations on this node.\"\"\"\n        pass\n\n\nclass ConcreteConfigNode(ConfigNode):\n    \"\"\"A concrete configuration node with a name and version.\"\"\"\n\n    def __init__(\n        self, name: str, version: int, children: Optional[List[\"ConfigNode\"]] = None\n    ):\n        self.name = name\n        self.version = version\n        super().__init__(children)\n\n    def accept(self, visitor: \"ConfigVisitor\") -&gt; None:\n        visitor.traverse(self)\n\n\nclass ConfigVisitor(ABC):\n    \"\"\"Abstract base class for visitors that can operate on ConfigNode objects.\"\"\"\n\n    @abstractmethod\n    def pre_hook(self) -&gt; bool:\n        \"\"\"Pre-hook method called before visiting a node. Return True to continue traversal.\"\"\"\n        pass\n\n    @abstractmethod\n    def post_hook(self) -&gt; None:\n        \"\"\"Post-hook method called after visiting a node and its children.\"\"\"\n        pass\n\n    @abstractmethod\n    def visit(self, node: ConfigNode) -&gt; None:\n        \"\"\"Visit a specific node and perform operations on it.\"\"\"\n        pass\n\n    def traverse(self, node: ConfigNode) -&gt; None:\n        \"\"\"Traverse the tree starting from the given node, calling hooks and visit.\"\"\"\n        # Pre hook for short circuiting the traversal\n        if self.pre_hook():\n            self.visit(node)\n\n            if node.children:\n                for c in node.children:\n                    self.traverse(c)\n\n        self.post_hook()\n\n\nclass VersionValidationVisitor(ConfigVisitor):\n    \"\"\"Visitor that validates the version of all ConfigNode objects in a tree.\"\"\"\n\n    def __init__(self, version_id: int) -&gt; None:\n        \"\"\"Initialize the visitor with the expected version ID.\"\"\"\n        self.is_valid = True\n        self.version_id = version_id\n\n    def pre_hook(self) -&gt; bool:\n        \"\"\"Pre-hook to check if traversal should continue. Returns True if valid so far.\"\"\"\n        return self.is_valid\n\n    def post_hook(self) -&gt; None:\n        \"\"\"Post-hook called after traversal. No operation needed.\"\"\"\n        pass\n\n    def visit(self, node: ConfigNode) -&gt; None:\n        \"\"\"Visit a node and validate its version if it's a ConcreteConfigNode.\"\"\"\n        if isinstance(node, ConcreteConfigNode):\n            if node.version != self.version_id:\n                print(\"Version mismatch found\")\n                self.is_valid = False\n        else:\n            print(\"Unknown node type encountered during validation.\")\n            self.is_valid = False\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample config tree\n    root = ConcreteConfigNode(\n        \"root\",\n        1,\n        [\n            ConcreteConfigNode(\"child1\", 1),\n            ConcreteConfigNode(\"child2\", 2),  # This will cause a version mismatch\n        ],\n    )\n\n    # Create a visitor for version validation\n    validator = VersionValidationVisitor(version_id=1)\n\n    # Traverse the config tree with the visitor\n    root.accept(validator)\n\n    if validator.is_valid:\n        print(\"All config nodes have the correct version.\")\n    else:\n        print(\"Some config nodes have incorrect versions.\")\n\n    # Valid case\n    valid_root = ConcreteConfigNode(\n        \"root\", 1, [ConcreteConfigNode(\"child1\", 1), ConcreteConfigNode(\"child2\", 1)]\n    )\n    valid_validator = VersionValidationVisitor(version_id=1)\n    valid_root.accept(valid_validator)\n    if valid_validator.is_valid:\n        print(\"All config nodes have the correct version.\")\n    else:\n        print(\"Some config nodes have incorrect versions.\")\n\n\n# Unit tests with pytest\nimport pytest\n\n\ndef test_version_validation_visitor():\n    # Create a sample config tree with matching versions\n    root = ConcreteConfigNode(\n        \"root\", 1, [ConcreteConfigNode(\"child1\", 1), ConcreteConfigNode(\"child2\", 1)]\n    )\n\n    validator = VersionValidationVisitor(version_id=1)\n    root.accept(validator)\n    assert validator.is_valid == True\n\n    # Create a sample config tree with a version mismatch\n    root_mismatch = ConcreteConfigNode(\n        \"root\",\n        1,\n        [\n            ConcreteConfigNode(\"child1\", 1),\n            ConcreteConfigNode(\"child2\", 2),  # Mismatch here\n        ],\n    )\n\n    validator_mismatch = VersionValidationVisitor(version_id=1)\n    root_mismatch.accept(validator_mismatch)\n    assert validator_mismatch.is_valid == False\n</code></pre>"},{"location":"behavioral/visitor/#when-to-use","title":"When to Use","text":"<p>Use the Visitor pattern when you need to perform operations on elements of a complex object structure without modifying the classes of the elements. It's particularly useful for tree-like structures where you want to add new operations without changing the node classes, and when operations involve different types of nodes that require type-specific behavior. /workspaces/design-patterns-gog/docs/behavioral/visitor.md"},{"location":"creational/","title":"Creational Patterns","text":"<p>Creational patterns deal with object creation mechanisms, trying to create objects in a way that is suitable to the situation. They abstract the instantiation process.</p>"},{"location":"creational/#overview","title":"Overview","text":"<p>In microservices and data pipelines, creational patterns help: - Manage dependencies: Ensure single instances of clients (Singleton) - Abstract creation logic: Different client types for different environments (Factory Method) - Build complex configurations: Step-by-step assembly of job specs (Builder) - Clone and customize: Rapidly create variations of pipeline configs (Prototype)</p>"},{"location":"creational/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Singleton One global instance per process ParagoNClient manager Factory Method Choose implementation based on config Create prod/test/async clients Abstract Factory Create families of related objects Ecosystem-specific client sets Builder Assemble complex objects step-by-step SparkJobBuilder Prototype Clone and customize objects rapidly Pipeline spec cloning"},{"location":"creational/#when-to-use","title":"When to Use","text":"<p>Choose creational patterns when you need to: - Decouple object creation from your business logic - Support multiple ways to construct similar objects - Enforce constraints on how objects are instantiated - Improve testability through dependency injection</p>"},{"location":"creational/abstract_factory/","title":"Abstract Factory Pattern","text":""},{"location":"creational/abstract_factory/#problem","title":"Problem","text":"<p>Your platform integrates with multiple third-party ecosystems (ParagoN, another API provider). Each provider requires a set of cooperating client objects:</p> <ul> <li>ParagoN ecosystem: <code>AuthClient</code>, <code>DataClient</code>, <code>WebhookClient</code></li> <li>Another API: Different implementations of the same interfaces but different protocols/auth</li> </ul> <p>Without Abstract Factory, you'd end up with messy conditional logic everywhere:</p> <pre><code># \u274c Messy without pattern\nif provider == \"paragon\":\n    auth = ParagoNAuthClient()\n    data = ParagoNDataClient()\n    webhook = ParagoNWebhookClient()\nelif provider == \"another\":\n    auth = AnotherAuthClient()\n    data = AnotherDataClient()\n    webhook = AnotherWebhookClient()\n</code></pre>"},{"location":"creational/abstract_factory/#solution","title":"Solution","text":"<p>Define abstract interfaces and provider-specific factories:</p> <pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\n\nclass Provider(Enum):\n    PARAGON = \"paragon\"\n    ANOTHER_API = \"another_api\"\n    MOCK = \"mock\"\n\nclass BaseAuthClient(ABC):\n    @abstractmethod\n    def authenticate(self, credentials: dict):\n        raise NotImplementedError\n\nclass BaseDataClient(ABC):\n    @abstractmethod\n    def fetch_data(self, query: str) -&gt; dict:\n        raise NotImplementedError\n\nclass BaseWebhookClient(ABC):\n    @abstractmethod\n    def send_webhook(self, payload: dict) -&gt; None:\n        raise NotImplementedError\n\nclass BaseClientFactory(ABC):\n    @abstractmethod\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_data_client(self) -&gt; BaseDataClient:\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        raise NotImplementedError\n\n# ParagoN implementations\nclass ParagoNAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass ParagoNDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass ParagoNWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass ParagoNClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return ParagoNAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return ParagoNDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return ParagoNWebhookClient()\n\n# Mock implementations for testing\nclass MockAuthClient(BaseAuthClient):\n    def authenticate(self, credentials: dict):\n        pass\n\nclass MockDataClient(BaseDataClient):\n    def fetch_data(self, query: str) -&gt; dict:\n        return {}\n\nclass MockWebhookClient(BaseWebhookClient):\n    def send_webhook(self, payload: dict) -&gt; None:\n        pass\n\nclass MockClientFactory(BaseClientFactory):\n    def create_auth_client(self) -&gt; BaseAuthClient:\n        return MockAuthClient()\n\n    def create_data_client(self) -&gt; BaseDataClient:\n        return MockDataClient()\n\n    def create_webhook_client(self) -&gt; BaseWebhookClient:\n        return MockWebhookClient()\n\ndef load_factory(provider_name: Provider) -&gt; BaseClientFactory:\n    if provider_name == Provider.PARAGON:\n        return ParagoNClientFactory()\n    if provider_name == Provider.MOCK:\n        return MockClientFactory()\n    raise ValueError(f\"Unknown provider: {provider_name.value}\")\n</code></pre>"},{"location":"creational/abstract_factory/#usage","title":"Usage","text":"<pre><code># Configuration-driven ecosystem setup\nprovider = Provider.PARAGON\nfactory = load_factory(provider)\n\n# Create a coordinated set of clients\nauth_client = factory.create_auth_client()\ndata_client = factory.create_data_client()\nwebhook_client = factory.create_webhook_client()\n\n# All three work together seamlessly\nauth_client.authenticate({\"api_key\": \"...\"})\nwebhook_client.send_webhook({\"event\": \"user_created\"})\ndata = data_client.fetch_data(\"SELECT * FROM users\")\n\n# Easy to switch to testing\ntest_factory = load_factory(Provider.MOCK)\ntest_auth = test_factory.create_auth_client()\ntest_data = test_factory.create_data_client()\n</code></pre>"},{"location":"creational/abstract_factory/#benefits","title":"Benefits","text":"<ul> <li>Consistency: All clients from one factory are compatible</li> <li>Easy provider switching: Change config, not code</li> <li>Testability: Create a mock factory for integration tests</li> <li>Extensibility: Adding a new provider requires only one new factory class</li> </ul>"},{"location":"creational/abstract_factory/#testing-example","title":"Testing Example","text":"<pre><code>def test_ecosystem_integration():\n    # Use mock factory for testing\n    factory = load_factory(Provider.MOCK)\n    auth_client = factory.create_auth_client()\n    data_client = factory.create_data_client()\n    webhook_client = factory.create_webhook_client()\n\n    # All clients are now mocked\n    auth_client.authenticate({})\n    webhook_client.send_webhook({})\n    data = data_client.fetch_data(\"\")\n</code></pre>"},{"location":"creational/builder/","title":"Builder Pattern","text":""},{"location":"creational/builder/#problem","title":"Problem","text":"<p>Constructing a complex Spark job configuration requires many optional parameters: - Input sources (S3, Kafka, database) - Transforms (filters, maps, joins) - Windowing and triggers - Resource settings (executors, memory) - Monitoring hooks</p> <p>Without a builder, you'd either: 1. Use a giant constructor with 20 parameters 2. Pass a loose dictionary around 3. Use <code>**kwargs</code> and lose type safety</p> <pre><code># \u274c Messy constructor\njob = SparkJob(\n    source, format, schema_path,\n    filter_fn, map_fn, join_key,\n    window_type, window_duration, trigger_interval,\n    executor_count, executor_memory, num_cores,\n    shuffle_partitions, log_level, metrics_enabled,\n    # ... and 10 more parameters\n)\n</code></pre>"},{"location":"creational/builder/#solution","title":"Solution","text":"<p>Use a builder with fluent chaining to assemble the spec step-by-step:</p> <pre><code>from copy import deepcopy\n\nclass SparkJobSpec:\n    __slots__ = (\n        \"input_source\",\n        \"transforms\",\n        \"windowing\",\n        \"triggers\",\n        \"resources\",\n        \"monitoring_hooks\",\n        \"spec_version\",\n    )\n\n    def __init__(self, *, input_source, transforms, windowing, triggers, resources, monitoring_hooks, spec_version=\"1.0\"):\n        object.__setattr__(self, \"input_source\", input_source)\n        object.__setattr__(self, \"transforms\", tuple(transforms))\n        object.__setattr__(self, \"windowing\", windowing)\n        object.__setattr__(self, \"triggers\", triggers)\n        object.__setattr__(self, \"resources\", deepcopy(resources))\n        object.__setattr__(self, \"monitoring_hooks\", tuple(monitoring_hooks))\n        object.__setattr__(self, \"spec_version\", spec_version)\n\n    def __setattr__(self, key, value):\n        if hasattr(self, key):\n            raise AttributeError(\"Cannot modify immutable SparkJobSpec\")\n        super().__setattr__(key, value)\n\n    def __delattr__(self, key):\n        raise AttributeError(\"SparkJobSpec is immutable\")\n\n    def serialize(self):\n        return {\n            \"input_source\": self.input_source,\n            \"transforms\": self.transforms,\n            \"windowing\": self.windowing,\n            \"triggers\": self.triggers,\n            \"resources\": self.resources,\n            \"monitoring_hooks\": self.monitoring_hooks,\n            \"spec_version\": self.spec_version,\n        }\n\nclass SparkJobBuilder:\n    def __init__(self):\n        self._input_source = None\n        self._transforms = []\n        self._windowing = None\n        self._triggers = None\n        self._resources = {}\n        self._monitoring_hooks = []\n        self._spec_version = \"1.0\"\n\n    def input_source(self, source):\n        self._input_source = source\n        return self\n\n    def set_transforms(self, transforms):\n        self._transforms = transforms\n        return self\n\n    def windowing(self, windowing):\n        self._windowing = windowing\n        return self\n\n    def triggers(self, triggers):\n        self._triggers = triggers\n        return self\n\n    def resources(self, resources):\n        self._resources = resources\n        return self\n\n    def monitoring_hooks(self, hooks):\n        self._monitoring_hooks = hooks\n        return self\n\n    def spec_version(self, version):\n        self._spec_version = version\n        return self\n\n    def build(self):\n        return SparkJobSpec(\n            input_source=self._input_source,\n            transforms=self._transforms,\n            windowing=self._windowing,\n            triggers=self._triggers,\n            resources=self._resources,\n            monitoring_hooks=self._monitoring_hooks,\n            spec_version=self._spec_version\n        )\n</code></pre>"},{"location":"creational/builder/#usage-examples","title":"Usage Examples","text":"<pre><code># Build a job spec fluently\nbuilder = SparkJobBuilder()\njob_spec = (builder\n    .input_source({\"type\": \"s3\", \"bucket\": \"prod-bucket\", \"path\": \"data/\"})\n    .set_transforms([\n        {\"type\": \"filter\", \"condition\": \"status = 'active'\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"}\n    ])\n    .windowing({\"type\": \"tumbling\", \"duration\": 3600})\n    .triggers({\"type\": \"micro-batch\", \"interval\": 60})\n    .resources({\"executor_count\": 10, \"executor_memory_gb\": 4})\n    .monitoring_hooks([{\"type\": \"prometheus\"}])\n    .build())\n\n# Serialize for storage\nconfig = job_spec.serialize()\n\n# Immutability guaranteed\ntry:\n    job_spec.input_source = \"new_value\"  # Raises AttributeError\nexcept AttributeError as e:\n    print(f\"Cannot modify: {e}\")\n</code></pre>"},{"location":"creational/builder/#advantages","title":"Advantages","text":"Pros Cons Readable fluent interface Extra classes/code Type-safe method signatures Slightly more overhead Immutable final spec Not suitable for tiny objects Easy to extend with new methods Requires careful implementation Versioning built-in"},{"location":"creational/builder/#advanced-builder-cloning","title":"Advanced: Builder Cloning","text":"<pre><code># Clone an existing job and modify\nmodified_job = (SparkJobBuilder()\n    # Copy settings from existing spec\n    # then override specific fields\n    .from_s3(\"test-bucket\", \"data/customers/\")\n    .with_resources(executors=2)  # Fewer resources for testing\n    .build())\n</code></pre>"},{"location":"creational/factory_method/","title":"Factory Method Pattern","text":""},{"location":"creational/factory_method/#problem","title":"Problem","text":"<p>Your system needs to create API clients for different runtime contexts: - Production: Real <code>ParagoNClient</code> connecting to live API - Testing: Mock client returning fixtures - Async workers: Async-aware client variant</p> <p>Without a factory, you'd hardcode client selection scattered throughout your code. The Factory Method centralizes this logic.</p>"},{"location":"creational/factory_method/#solution","title":"Solution","text":"<p>Define a factory method that returns the appropriate client based on configuration:</p> <pre><code>from enum import Enum\nfrom typing import Union\n\nclass ClientType(Enum):\n    PRODUCTION = \"production\"\n    TESTING = \"testing\"\n    ASYNC = \"async\"\n\nclass HttpMethod(Enum):\n    GET = \"GET\"\n    POST = \"POST\"\n    PUT = \"PUT\"\n    DELETE = \"DELETE\"\n\nclass ParagonClientConfig:\n    def __init__(self, client_type: ClientType, client_id: str = None, client_secret: str = None):\n        self.client_type = client_type\n        self.client_id = client_id or \"default_id\"\n        self.client_secret = client_secret or \"default_secret\"\n\nclass BaseClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    def request(self, method, url, headers, data, query_params):\n        raise NotImplementedError\n\nclass BaseAsyncClient:\n    def __init__(self, client_config: ParagonClientConfig):\n        self.client_config = client_config\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        raise NotImplementedError\n\nclass ParagonSyncClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"SyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonAsyncClient(BaseAsyncClient):\n    async def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"AsyncClient: {method} {url} with {data} and {query_params}\"\n\nclass ParagonMockClient(BaseClient):\n    def request(self, method: HttpMethod, url: str, headers: dict, data: dict, query_params: dict):\n        return f\"MockClient: {method} {url} with {data} and {query_params}\"\n\ndef create_client(config: ParagonClientConfig) -&gt; Union[BaseClient, BaseAsyncClient]:\n    if config.client_type == ClientType.PRODUCTION:\n        return ParagonSyncClient(config)\n    elif config.client_type == ClientType.TESTING:\n        return ParagonMockClient(config)\n    elif config.client_type == ClientType.ASYNC:\n        return ParagonAsyncClient(config)\n    else:\n        raise ValueError(f\"Unknown client type: {config.client_type}\")\n</code></pre>"},{"location":"creational/factory_method/#usage","title":"Usage","text":"<pre><code># Configuration-driven client creation\nconfig_prod = ParagonClientConfig(ClientType.PRODUCTION, client_id=\"prod_123\")\nprod_client = create_client(config_prod)\n\nconfig_test = ParagonClientConfig(ClientType.TESTING)\ntest_client = create_client(config_test)\n\nconfig_async = ParagonClientConfig(ClientType.ASYNC, client_id=\"async_123\")\nasync_client = create_client(config_async)\n\n# Use the appropriate client\nresult_prod = prod_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\nresult_test = test_client.request(HttpMethod.GET, \"https://api.paragon.io/users/123\", {}, {}, {})\n</code></pre>"},{"location":"creational/factory_method/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Decouples client selection from usage Extra indirection for simple cases Easy to add new client types Factory method becomes complex with many types Testability: swap with mock effortlessly String-based selection is fragile Configuration-driven behavior"},{"location":"creational/factory_method/#advanced-factory-with-config-objects","title":"Advanced: Factory with Config Objects","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ClientConfig:\n    env: str\n    api_key: str = None\n    timeout: int = 30\n    retries: int = 3\n\ndef create_client_from_config(config: ClientConfig) -&gt; ClientBase:\n    if config.env == \"prod\":\n        return ParagoNClientProd(api_key=config.api_key, \n                                 timeout=config.timeout, \n                                 retries=config.retries)\n    elif config.env == \"test\":\n        return ParagoNClientMock()\n    # ... etc\n</code></pre>"},{"location":"creational/prototype/","title":"Prototype Pattern","text":""},{"location":"creational/prototype/#problem","title":"Problem","text":"<p>In your ETL orchestration, you often reuse pipeline configurations with minor variations:</p> <pre><code># Base template: daily customer data pipeline\nbase_config = {\n    \"name\": \"customer_daily\",\n    \"source\": {\"s3\": \"s3://prod-data/customers/\"},\n    \"transforms\": [\n        {\"type\": \"filter\", \"condition\": \"is_active=true\"},\n        {\"type\": \"enrich\", \"api\": \"ParagoNClient\"},\n        {\"type\": \"aggregate\", \"window\": \"1d\"},\n    ],\n    \"resources\": {\"executors\": 10, \"memory_gb\": 4},\n}\n</code></pre> <p>Now you need variations for: - Testing: Same config but with test data and fewer resources - Weekly rollup: Different transforms and schedule - A/B test variant: Modified enrichment logic</p> <p>Without Prototype, you rebuild each from scratch, leading to duplication and inconsistency.</p>"},{"location":"creational/prototype/#solution","title":"Solution","text":"<p>Implement a cloning mechanism with provenance tracking:</p> <pre><code>from copy import deepcopy, copy\n\nclass PipelineSpec:\n    \"\"\"Pipeline specification with cloning and provenance.\"\"\"\n\n    def __init__(self, *, name: str, input_source: str, \n                 transforms: list, resources: dict, \n                 metadata: dict = None):\n        self.name = name\n        self.input_source = input_source\n        self.transforms = transforms\n        self.resources = resources\n        self.metadata = metadata or {}\n\n    def clone(self, deep: bool = True, **overrides):\n        \"\"\"Clone the spec, optionally overriding fields.\n\n        Args:\n            deep: If True, recursively copy nested structures\n            **overrides: Fields to override in the cloned spec\n\n        Returns:\n            PipelineSpec: New independent instance\n        \"\"\"\n        if deep:\n            # Deep copy: all nested structures are independent\n            cloned_transforms = deepcopy(self.transforms)\n            cloned_resources = deepcopy(self.resources)\n            cloned_metadata = deepcopy(self.metadata)\n        else:\n            # Shallow copy: nested objects are shared\n            cloned_transforms = copy(self.transforms)\n            cloned_resources = copy(self.resources)\n            cloned_metadata = copy(self.metadata)\n\n        # Apply overrides\n        for key, value in overrides.items():\n            if key == \"transforms\":\n                cloned_transforms = value\n            elif key == \"resources\":\n                cloned_resources = value\n            elif key == \"metadata\":\n                cloned_metadata = value\n\n        # Create new spec\n        new_spec = PipelineSpec(\n            name=overrides.get(\"name\", self.name),\n            input_source=overrides.get(\"input_source\", self.input_source),\n            transforms=cloned_transforms,\n            resources=cloned_resources,\n            metadata=cloned_metadata,\n        )\n\n        # Update provenance\n        new_spec.metadata['cloned_from'] = self.name\n\n        return new_spec\n</code></pre>"},{"location":"creational/prototype/#usage-examples","title":"Usage Examples","text":""},{"location":"creational/prototype/#basic-cloning-with-deep-copy","title":"Basic Cloning with Deep Copy","text":"<pre><code># Create a base pipeline spec\nbase_spec = PipelineSpec(\n    name=\"base_etl\",\n    input_source=\"s3://input/data\",\n    transforms=[\"parse\", \"clean\", \"enrich\"],\n    resources={\"executor_memory\": \"4G\", \"partitions\": 100},\n)\n\n# Clone it for a new use case (deep copy by default)\n# All nested structures are independent\ntest_spec = base_spec.clone(\n    name=\"test_etl\",\n    input_source=\"s3://test/data\",\n)\n\n# Modify test_spec's transforms - does NOT affect base_spec\ntest_spec.transforms.append(\"validate\")\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Still [\"parse\", \"clean\", \"enrich\"]\nprint(f\"test_spec transforms: {test_spec.transforms}\")  # [\"parse\", \"clean\", \"enrich\", \"validate\"]\nprint(f\"test_spec provenance: {test_spec.metadata.get('cloned_from')}\")  # \"base_etl\"\n</code></pre>"},{"location":"creational/prototype/#shallow-copy-for-shared-resources","title":"Shallow Copy for Shared Resources","text":"<pre><code># When you want nested structures to share references\n# (e.g., shared resource pool configuration)\nshared_spec = base_spec.clone(deep=False)\nshared_spec.resources[\"executor_memory\"] = \"8G\"\n\n# With shallow copy, the list reference is shared\nprint(f\"base_spec transforms: {base_spec.transforms}\")  # Changes are reflected\n</code></pre>"},{"location":"creational/prototype/#templating-with-overrides","title":"Templating with Overrides","text":"<pre><code># Template for different environments\nprod_spec = base_spec.clone(\n    name=\"prod_etl\",\n    input_source=\"s3://prod/data\",\n    resources={\"executor_memory\": \"16G\", \"partitions\": 500},\n)\n\n# Create a debug variant\ndebug_spec = prod_spec.clone(\n    name=\"prod_etl_debug\",\n    transforms=[\"parse\", \"clean\"],  # Skip expensive enrich\n    resources={\"executor_memory\": \"2G\", \"partitions\": 10},\n)\n</code></pre>"},{"location":"creational/prototype/#shallow-vs-deep-clone","title":"Shallow vs Deep Clone","text":"Shallow Copy Deep Copy <code>cloned_transforms = copy(self.transforms)</code> <code>cloned_transforms = deepcopy(self.transforms)</code> New list, but items still shared Completely independent copy Faster, less memory Safer, prevents accidental mutations Use when: only changing top-level fields Use when: modifying nested structures <pre><code># Shallow clone example\nbase_transforms = [{\"type\": \"filter\"}]\nclone1 = copy(base_transforms)\nclone1[0][\"condition\"] = \"active=true\"\n# base_transforms ALSO changed! \u26a0\ufe0f\n\n# Deep clone example\nclone2 = deepcopy(base_transforms)\nclone2[0][\"condition\"] = \"active=true\"\n# base_transforms unchanged \u2713\n</code></pre>"},{"location":"creational/prototype/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Avoid duplication of complex configs Deep copying can be slow/memory-intensive Track config evolution via provenance Extra book-keeping required Support templates and variants Shallow copy gotchas if not careful Faster than manual rebuilding Complex graphs may need custom cloning"},{"location":"creational/singleton/","title":"Singleton Pattern","text":""},{"location":"creational/singleton/#problem","title":"Problem","text":"<p>When you run microservices with multiple threads or async workers, you need a single, shared instance of expensive resources like database connections or API clients. The <code>ParagoNClientManager</code> singleton ensures all threads in a process use the same cached client instance, avoiding redundant credentials and connection overhead.</p>"},{"location":"creational/singleton/#solution","title":"Solution","text":"<p>Create a class that enforces single instantiation:</p> <pre><code>from threading import Lock\n\nclass ParagonNSingleton:\n    _instance = None\n    _lock = Lock()\n\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.token = 0\n\n    def _fetch_token(self) -&gt; int:\n        # Simulate fetching a token using the API key\n        with self._lock:\n            self.token += 1\n            return self.token\n\n    def refresh_token(self):\n        self.token = self._fetch_token()\n        return self.token\n\n\nclass ParagonNSingletonManager:\n    _instance = None \n    _lock = Lock()\n\n    def __new__(cls, *args, **kwargs):\n        raise NotImplementedError(\"Use get_instance() method to get the singleton instance.\")\n\n    @classmethod \n    def get_instance(cls, api_key: str) -&gt; \"ParagonNSingletonManager\":\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(ParagonNSingletonManager)\n                    cls._instance.client = ParagonNSingleton(api_key)\n        return cls._instance\n\n    @classmethod\n    def get_client(cls, api_key: str) -&gt; ParagonNSingleton:\n        instance = cls.get_instance(api_key)\n        return instance.client\n</code></pre>"},{"location":"creational/singleton/#key-features","title":"Key Features","text":"<ul> <li>Thread-safe: Uses double-checked locking to prevent race conditions</li> <li>Lazy initialization: Client created only on first use</li> <li>Global access: <code>ParagoNClientManager().get_client()</code> from anywhere</li> <li>Credentials managed: API keys loaded once per process</li> </ul>"},{"location":"creational/singleton/#usage-in-your-code","title":"Usage in Your Code","text":"<pre><code># Get singleton client instance\na = ParagonNSingletonManager.get_client(\"my_api_key\")\nb = ParagonNSingletonManager.get_client(\"my_api_key\")\n\nprint(a is b)  # True - both are the same instance\n\n# Thread-safe access\ndef access_client():\n    client = ParagonNSingletonManager.get_client(\"my_api_key\")\n    client.refresh_token()\n    print(f\"Token: {client.token}\")\n\nimport threading\nthreads = [threading.Thread(target=access_client) for _ in range(5)]\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</code></pre>"},{"location":"creational/singleton/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Single shared instance reduces overhead Hard to test (global state) Thread-safe token refresh Difficult to mock in unit tests Credentials loaded once Tight coupling to singleton Easy global access Hidden dependencies"},{"location":"creational/singleton/#testing-tip","title":"Testing Tip","text":"<p>For unit tests, consider a test-friendly alternative: use dependency injection or a registry pattern instead of a pure singleton.</p> <pre><code># Testable version\nclient_manager = ParagoNClientManager()\n# Pass to handlers as dependency\ndef handle_user_request(user_id, client_manager=client_manager):\n    client = client_manager.get_client()\n    # ...\n</code></pre>"},{"location":"structural/","title":"Structural Patterns","text":"<p>Structural patterns deal with object composition, creating larger structures from classes and objects through inheritance and composition.</p>"},{"location":"structural/#overview","title":"Overview","text":"<p>In microservices, React integrations, and data pipelines, structural patterns help: - Adapt incompatible interfaces: Hide third-party API quirks (Adapter) - Decouple abstraction from implementation: Support multiple backends (Bridge) - Compose hierarchies: Build tree structures of operations (Composite) - Add behavior dynamically: Wrap objects with cross-cutting concerns (Decorator) - Simplify complex subsystems: Provide unified high-level interfaces (Facade) - Share expensive objects: Reduce memory footprint of large datasets (Flyweight) - Control access: Add layers like caching and rate-limiting (Proxy)</p>"},{"location":"structural/#patterns-in-this-category","title":"Patterns in This Category","text":"Pattern Use Case Example Adapter Make incompatible interfaces work together Map ParagoN API to internal User DTO Bridge Decouple abstraction from implementation Ingestion logic + multiple storage backends Composite Treat individual and composite objects uniformly Tree of bulk operations across services Decorator Add responsibilities dynamically Wrap clients with retry/tracing/metrics Facade Provide simplified interface to complex subsystem Onboarding orchestration Flyweight Share fine-grained objects efficiently Shared metadata across Spark tasks Proxy Control access to another object Client wrapper with caching + rate-limiting"},{"location":"structural/#when-to-use","title":"When to Use","text":"<p>Choose structural patterns when you need to: - Work with external systems that have incompatible interfaces - Support multiple implementations transparently - Build flexible hierarchies of objects - Reduce memory usage for large numbers of similar objects - Add or modify behavior without changing class definitions</p>"},{"location":"structural/adapter/","title":"Adapter Pattern","text":""},{"location":"structural/adapter/#problem","title":"Problem","text":"<p>Your microservices and React frontend rely on consistent internal data models (DTOs). However, the ParagoN API (which you interact with via <code>ParagoNClient</code>) returns deeply nested JSON with inconsistent field names, optional fields, and different data types:</p>"},{"location":"structural/adapter/#paragon-api-response","title":"ParagoN API Response","text":"<pre><code>{\n  \"user_id\": \"12345\",\n  \"personal_info\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\",\n    \"contact\": {\n      \"email_addr\": \"john.doe@example.com\",\n      \"phone_num\": \"+1234567890\"\n    }\n  },\n  \"account_status\": \"ACTIVE\",\n  \"created_at\": \"2023-10-01T12:00:00Z\",\n  \"metadata\": {\n    \"tags\": [\"premium\", \"verified\"],\n    \"preferences\": {\"notifications\": true}\n  }\n}\n</code></pre>"},{"location":"structural/adapter/#your-internal-user-model","title":"Your Internal User Model","text":"<pre><code>{\n  \"id\": \"12345\",\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"phone\": \"+1234567890\",\n  \"status\": \"active\",\n  \"createdAt\": \"2023-10-01T12:00:00Z\",\n  \"tags\": [\"premium\", \"verified\"],\n  \"preferences\": {\"notifications\": true}\n}\n</code></pre> <p>Discrepancies: - Field name variations (<code>user_id</code> \u2192 <code>id</code>, <code>email_addr</code> \u2192 <code>email</code>, <code>account_status</code> \u2192 <code>status</code>) - Nested structure flattening - Type conversions (string status to lowercase) - Optional fields may be missing</p>"},{"location":"structural/adapter/#solution","title":"Solution","text":"<p>Implement bidirectional adapters that hide the complexity:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseAdapterModel(ABC):\n    \"\"\"Abstract adapter interface.\"\"\"\n\n    def __init__(self, external_data: dict):\n        self.external_data = external_data\n\n    @abstractmethod\n    def to_internal(self) -&gt; dict:\n        \"\"\"Convert external API response to internal model.\"\"\"\n        raise NotImplementedError(\"to_internal method not implemented\")\n\n    @abstractmethod\n    def to_external(self) -&gt; dict:\n        \"\"\"Convert internal model back to external API format.\"\"\"\n        raise NotImplementedError(\"to_external method not implemented\")\n\n\nclass ParagoNUserAdapter(BaseAdapterModel):\n    \"\"\"Maps ParagoN API user responses to internal User DTOs.\"\"\"\n\n    def to_internal(self) -&gt; dict:\n        \"\"\"Flatten ParagoN's nested response to internal User model.\"\"\"\n        data = self.external_data\n        internal_data = {\n            \"id\": data.get(\"user_id\"),\n            \"firstName\": data.get(\"personal_info\", {}).get(\"firstName\"),\n            \"lastName\": data.get(\"personal_info\", {}).get(\"lastName\"),\n            \"email\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"email_addr\"),\n            \"phone\": data.get(\"personal_info\", {}).get(\"contact\", {}).get(\"phone_num\"),\n            \"status\": data.get(\"account_status\", \"\").lower(),\n            \"createdAt\": data.get(\"created_at\"),\n            \"tags\": data.get(\"metadata\", {}).get(\"tags\", []),\n            \"preferences\": data.get(\"metadata\", {}).get(\"preferences\", {}),\n        }\n        return internal_data\n\n    def to_external(self) -&gt; dict:\n        \"\"\"Map internal User model back to ParagoN format.\"\"\"\n        internal_data = self.external_data\n        external_data = {\n            \"user_id\": internal_data.get(\"id\"),\n            \"personal_info\": {\n                \"firstName\": internal_data.get(\"firstName\"),\n                \"lastName\": internal_data.get(\"lastName\"),\n                \"contact\": {\n                    \"email_addr\": internal_data.get(\"email\"),\n                    \"phone_num\": internal_data.get(\"phone\"),\n                },\n            },\n            \"account_status\": internal_data.get(\"status\", \"\").upper(),\n            \"created_at\": internal_data.get(\"createdAt\"),\n            \"metadata\": {\n                \"tags\": internal_data.get(\"tags\", []),\n                \"preferences\": internal_data.get(\"preferences\", {}),\n            },\n        }\n        return external_data\n</code></pre>"},{"location":"structural/adapter/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/adapter/#basic-adaptation-paragon-response-to-internal-model","title":"Basic Adaptation: ParagoN Response to Internal Model","text":"<pre><code># Receive data from ParagoN API\nparagon_response = {\n    \"user_id\": \"12345\",\n    \"personal_info\": {\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"contact\": {\n            \"email_addr\": \"john@example.com\",\n            \"phone_num\": \"+1234567890\"\n        }\n    },\n    \"account_status\": \"ACTIVE\",\n    \"created_at\": \"2023-10-01T12:00:00Z\",\n    \"metadata\": {\n        \"tags\": [\"premium\", \"verified\"],\n        \"preferences\": {\"notifications\": True}\n    }\n}\n\n# Adapt to internal model\nadapter = ParagoNUserAdapter(paragon_response)\ninternal_user = adapter.to_internal()\n\nprint(internal_user)\n# {\n#     \"id\": \"12345\",\n#     \"firstName\": \"John\",\n#     \"lastName\": \"Doe\",\n#     \"email\": \"john@example.com\",\n#     \"phone\": \"+1234567890\",\n#     \"status\": \"active\",  # lowercased\n#     \"createdAt\": \"2023-10-01T12:00:00Z\",\n#     \"tags\": [\"premium\", \"verified\"],\n#     \"preferences\": {\"notifications\": True}\n# }\n</code></pre>"},{"location":"structural/adapter/#reverse-adaptation-internal-model-to-paragon-format","title":"Reverse Adaptation: Internal Model to ParagoN Format","text":"<pre><code># Your internal user model (from React or database)\ninternal_user = {\n    \"id\": \"12345\",\n    \"firstName\": \"Jane\",\n    \"lastName\": \"Smith\",\n    \"email\": \"jane@example.com\",\n    \"phone\": \"+9876543210\",\n    \"status\": \"active\",\n    \"createdAt\": \"2023-10-01T12:00:00Z\",\n    \"tags\": [\"vip\"],\n    \"preferences\": {\"notifications\": False}\n}\n\n# Adapt back to ParagoN format for API request\nadapter = ParagoNUserAdapter(internal_user)\nparagon_format = adapter.to_external()\n\nprint(paragon_format)\n# {\n#     \"user_id\": \"12345\",\n#     \"personal_info\": {\n#         \"firstName\": \"Jane\",\n#         \"lastName\": \"Smith\",\n#         \"contact\": {\n#             \"email_addr\": \"jane@example.com\",\n#             \"phone_num\": \"+9876543210\"\n#         }\n#     },\n#     \"account_status\": \"ACTIVE\",  # uppercased\n#     \"created_at\": \"2023-10-01T12:00:00Z\",\n#     \"metadata\": {\n#         \"tags\": [\"vip\"],\n#         \"preferences\": {\"notifications\": False}\n#     }\n# }\n</code></pre>"},{"location":"structural/adapter/#handling-missing-fields","title":"Handling Missing Fields","text":"<pre><code># Incomplete ParagoN response (missing optional fields)\nincomplete_response = {\n    \"user_id\": \"99999\",\n    \"personal_info\": {\n        \"firstName\": \"Bob\"\n        # No lastName, contact info, etc.\n    },\n    \"account_status\": \"PENDING\"\n    # No created_at, metadata\n}\n\n# Adapter safely handles missing nested fields\nadapter = ParagoNUserAdapter(incomplete_response)\ninternal = adapter.to_internal()\n\nprint(internal)\n# {\n#     \"id\": \"99999\",\n#     \"firstName\": \"Bob\",\n#     \"lastName\": None,  # Missing field\n#     \"email\": None,\n#     \"phone\": None,\n#     \"status\": \"pending\",\n#     \"createdAt\": None,\n#     \"tags\": [],  # Default empty list\n#     \"preferences\": {}  # Default empty dict\n# }\n</code></pre>"},{"location":"structural/adapter/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"Pros Cons Isolates your code from API changes Extra layer of indirection Type-safe internal models Maintenance overhead for each new API Easy to test and mock May add performance overhead Composable for complex transformations Debugging can be harder Consistent data contracts"},{"location":"structural/adapter/#performance-considerations","title":"Performance Considerations","text":"<p>For high-throughput scenarios: - Cache adapter instances if stateless - Minimize deep copying - Consider using <code>__slots__</code> for adapter classes - Profile serialization/deserialization overhead</p>"},{"location":"structural/bridge/","title":"Bridge Pattern","text":""},{"location":"structural/bridge/#problem","title":"Problem","text":"<p>Your ingestion logic must support multiple storage backends: - Production: S3 for large-scale data storage - Testing/Dev: Local filesystem for quick iteration - Multi-cloud: GCS for Google Cloud deployments</p> <p>Without Bridge, you'd have conditional logic sprinkled everywhere:</p> <pre><code># \u274c Tight coupling to specific backends\ndef ingest_data(backend_type, data):\n    if backend_type == \"s3\":\n        s3 = boto3.client(\"s3\")\n        s3.put_object(Bucket=\"my-bucket\", Key=\"data\", Body=data)\n    elif backend_type == \"local\":\n        with open(\"/data/local\", \"wb\") as f:\n            f.write(data)\n    elif backend_type == \"gcs\":\n        gcs = storage.Client()\n        bucket = gcs.bucket(\"my-bucket\")\n        blob = bucket.blob(\"data\")\n        blob.upload_from_string(data)\n</code></pre> <p>Problem: Ingestion logic is tightly coupled to storage implementation.</p>"},{"location":"structural/bridge/#solution","title":"Solution","text":"<p>Implement the bridge by separating the abstraction (ingestion logic) from implementations (storage backends):</p> <pre><code>from abc import ABC, abstractmethod\nfrom threading import Lock\n\n# Implementation interface - storage backends\nclass StorageBackend(ABC):\n    \"\"\"Abstract interface for storage operations.\"\"\"\n\n    @abstractmethod\n    def write(self, key: str, data: bytes):\n        \"\"\"Write data (idempotent - skips if exists).\"\"\"\n        raise NotImplementedError(\"write method not implemented\")\n\n    @abstractmethod\n    def update(self, key: str, data: bytes):\n        \"\"\"Update existing data (raises KeyError if not exists).\"\"\"\n        raise NotImplementedError(\"update method not implemented\")\n\n    @abstractmethod\n    def read(self, key: str) -&gt; bytes:\n        \"\"\"Read data from the backend.\"\"\"\n        raise NotImplementedError(\"read method not implemented\")\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n        \"\"\"Check if data exists in the backend.\"\"\"\n        raise NotImplementedError(\"exists method not implemented\")\n\n    @abstractmethod\n    def delete(self, key: str):\n        \"\"\"Delete data from the backend.\"\"\"\n        raise NotImplementedError(\"delete method not implemented\")\n\n\n# Concrete Implementation: S3\nclass S3Storage(StorageBackend):\n    \"\"\"AWS S3 storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        import boto3\n        self.s3_client = boto3.client(\"s3\")\n        self.bucket_name = bucket_name\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                print(f\"Data with key {key} already exists in S3. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.put_object(Bucket=self.bucket_name, Key=key, Body=data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in S3. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n        return response['Body'].read()\n\n    def exists(self, key: str) -&gt; bool:\n        try:\n            self.s3_client.head_object(Bucket=self.bucket_name, Key=key)\n            return True\n        except self.s3_client.exceptions.NoSuchKey:\n            return False\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                self.s3_client.delete_object(Bucket=self.bucket_name, Key=key)\n\n\n# Concrete Implementation: Local Filesystem\nclass LocalStorage(StorageBackend):\n    \"\"\"Local filesystem storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, base_path: str):\n        import os\n        self.base_path = base_path\n        os.makedirs(base_path, exist_ok=True)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                print(f\"Data with key {key} already exists in Local Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                with open(f\"{self.base_path}/{key}\", \"wb\") as f:\n                    f.write(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in Local Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        with open(f\"{self.base_path}/{key}\", \"rb\") as f:\n            return f.read()\n\n    def exists(self, key: str) -&gt; bool:\n        import os\n        return os.path.exists(f\"{self.base_path}/{key}\")\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                import os\n                os.remove(f\"{self.base_path}/{key}\")\n\n\n# Concrete Implementation: Google Cloud Storage\nclass GCSStorage(StorageBackend):\n    \"\"\"Google Cloud Storage backend.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self, bucket_name: str):\n        from google.cloud import storage\n        self.client = storage.Client()\n        self.bucket = self.client.bucket(bucket_name)\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                print(f\"Data with key {key} already exists in GCS Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.upload_from_string(data)\n            else:\n                raise KeyError(f\"Key {key} does not exist in GCS Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        blob = self.bucket.blob(key)\n        return blob.download_as_bytes()\n\n    def exists(self, key: str) -&gt; bool:\n        blob = self.bucket.blob(key)\n        return blob.exists()\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                blob = self.bucket.blob(key)\n                blob.delete()\n\n\n# Concrete Implementation: In-memory Mock (for testing)\nclass MockStorage(StorageBackend):\n    \"\"\"In-memory mock storage for testing.\"\"\"\n\n    update_lock = Lock()\n\n    def __init__(self):\n        self.storage = {}\n\n    def write(self, key: str, data: bytes):\n        with self.update_lock:\n            if not self.exists(key):\n                self.storage[key] = data\n            else:\n                print(f\"Data with key {key} already exists in Mock Storage. Skipping write.\")\n\n    def update(self, key: str, data: bytes):\n        with self.update_lock:\n            if self.exists(key):\n                self.storage[key] = data\n            else:\n                raise KeyError(f\"Key {key} does not exist in Mock Storage. Cannot update non-existent key.\")\n\n    def read(self, key: str) -&gt; bytes:\n        return self.storage[key]\n\n    def exists(self, key: str) -&gt; bool:\n        return key in self.storage\n\n    def delete(self, key: str):\n        with self.update_lock:\n            if self.exists(key):\n                del self.storage[key]\n\n\n# Abstraction: Ingestion Logic (independent of storage backend)\nclass IngestJob:\n    \"\"\"Ingestion job that works with any storage backend.\"\"\"\n\n    def __init__(self, backend: StorageBackend):\n        self.backend = backend\n\n    def execute(self, data_key: str, data: bytes):\n        \"\"\"Execute ingestion with idempotency check.\"\"\"\n        if not self.backend.exists(data_key):\n            self.backend.write(data_key, data)\n        else:\n            print(f\"Data with key {data_key} already exists. Skipping write.\")\n</code></pre>"},{"location":"structural/bridge/#usage-examples","title":"Usage Examples","text":""},{"location":"structural/bridge/#basic-usage-production-testing-and-development","title":"Basic Usage: Production, Testing, and Development","text":"<pre><code># Production: use S3\ns3_backend = S3Storage(bucket_name=\"prod-data-lake\")\ningest_job = IngestJob(s3_backend)\ningest_job.execute(\"2025-12-16/customer_data.parquet\", b\"production data\")\n\n# Local Development: use local filesystem\nlocal_backend = LocalStorage(\"/tmp/data\")\ningest_job_local = IngestJob(local_backend)\ningest_job_local.execute(\"example_key\", b\"sample data\")\n\n# Testing: use in-memory mock storage\nmock_backend = MockStorage()\ningest_job_test = IngestJob(mock_backend)\ningest_job_test.execute(\"example_key\", b\"sample data in mock\")\n\n# Multi-cloud: use Google Cloud Storage\ngcs_backend = GCSStorage(bucket_name=\"multi-cloud-data\")\ningest_job_gcs = IngestJob(gcs_backend)\ningest_job_gcs.execute(\"2025-12-16/customer_data.parquet\", b\"gcs data\")\n</code></pre>"},{"location":"structural/bridge/#demonstrating-the-bridge-same-logic-different-backends","title":"Demonstrating the Bridge: Same Logic, Different Backends","text":"<pre><code># The key benefit: IngestJob logic is identical regardless of backend\ndef run_ingestion_pipeline(backend: StorageBackend, data_items: list):\n    \"\"\"Generic pipeline - works with ANY storage backend.\"\"\"\n    job = IngestJob(backend)\n    for item in data_items:\n        job.execute(item[\"key\"], item[\"data\"])\n    print(\"Pipeline complete!\")\n\n# Use the same function with different backends\ndata = [\n    {\"key\": \"file1.txt\", \"data\": b\"content1\"},\n    {\"key\": \"file2.txt\", \"data\": b\"content2\"},\n]\n\n# Try with mock first\nmock_backend = MockStorage()\nrun_ingestion_pipeline(mock_backend, data)\n\n# Deploy with S3\ns3_backend = S3Storage(bucket_name=\"my-bucket\")\nrun_ingestion_pipeline(s3_backend, data)\n\n# No code changes! Just swap the backend.\n</code></pre>"},{"location":"structural/bridge/#thread-safe-updates-with-write-and-update-methods","title":"Thread-Safe Updates with Write and Update Methods","text":"<pre><code># Write: idempotent, skips if exists (used for initial writes)\nbackend = MockStorage()\nbackend.write(\"user:123\", b'{\"name\": \"Alice\"}')  # Succeeds\nbackend.write(\"user:123\", b'{\"name\": \"Bob\"}')    # Skips (already exists)\n\n# Update: requires existing key (used for updates/patches)\ntry:\n    backend.update(\"user:123\", b'{\"name\": \"Bob\"}')  # Succeeds\n    backend.update(\"user:999\", b'{\"name\": \"Charlie\"}')  # Raises KeyError\nexcept KeyError as e:\n    print(f\"Update failed: {e}\")\n</code></pre>"},{"location":"structural/bridge/#concurrent-access-with-lock-protection","title":"Concurrent Access with Lock Protection","text":"<pre><code>import threading\n\ndef ingest_in_thread(backend: StorageBackend, key: str, data: bytes):\n    job = IngestJob(backend)\n    job.execute(key, data)\n\n# Test concurrent access with mock backend\nmock_backend = MockStorage()\nthreads = []\n\nfor i in range(5):\n    t = threading.Thread(\n        target=ingest_in_thread, \n        args=(mock_backend, \"concurrent_key\", f\"data_{i}\".encode())\n    )\n    threads.append(t)\n    t.start()\n\nfor t in threads:\n    t.join()\n\nprint(f\"Final data in mock: {mock_backend.read('concurrent_key')}\")\n# All threads are synchronized via Lock - no race conditions\n</code></pre>"},{"location":"structural/bridge/#testing-with-mock-backend","title":"Testing with Mock Backend","text":"<pre><code># Unit test example - no need for real AWS/GCS credentials\ndef test_ingest_job_with_mock_storage():\n    backend = MockStorage()\n    ingest_job = IngestJob(backend)\n\n    key = \"ingest_key\"\n    data = b\"ingest data\"\n\n    # Execute ingestion\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Verify data integrity\n    read_data = backend.read(key)\n    assert read_data == data\n\n    # Test idempotency: second execute should skip\n    ingest_job.execute(key, data)\n    assert backend.exists(key) == True\n\n    # Cleanup\n    backend.delete(key)\n    assert backend.exists(key) == False\n</code></pre>"},{"location":"structural/bridge/#benefits","title":"Benefits","text":"<p>// Add performance considerations | Pros | Cons | |------|------| | Decouple job logic from storage | Extra abstraction layers | | Add new backends without changing job code | More code upfront | | Easy to test with mock storage | Potential performance overhead | | Swap backends at runtime | Interface mismatches between backends | | Support multi-cloud deployments | | |------|------| | Decouple job logic from storage | Extra abstraction layers | | Add new backends without changing job code | More code upfront | | Easy to test with mock storage | Potential performance overhead | | Swap backends at runtime | Interface mismatches between backends | | Support multi-cloud deployments | |</p>"},{"location":"structural/bridge/#advanced-storage-factory","title":"Advanced: Storage Factory","text":"<pre><code>def create_storage(backend_type: str, **config) -&gt; StorageImplementation:\n    \"\"\"Factory for creating storage backends.\"\"\"\n    backends = {\n        \"s3\": lambda: S3Storage(**config),\n        \"local\": lambda: LocalStorage(**config),\n        \"gcs\": lambda: GCSStorage(**config),\n    }\n\n    if backend_type not in backends:\n        raise ValueError(f\"Unknown storage backend: {backend_type}\")\n\n    return backends[backend_type]()\n\n# Configuration-driven backend selection\nbackend_type = os.getenv(\"STORAGE_BACKEND\", \"s3\")\nstorage = create_storage(backend_type, bucket_name=\"my-bucket\")\njob = IngestJob(storage, {\"name\": \"my_job\"})\n</code></pre>"},{"location":"structural/composite/","title":"Composite Pattern","text":""},{"location":"structural/composite/#problem","title":"Problem","text":"<p>Your orchestration layer must execute hierarchical operations composed of many smaller sub-operations:</p> <ul> <li>Bulk updates spanning multiple microservices</li> <li>Fan-out workflows (user-service \u2192 billing \u2192 inventory \u2192 notifications)</li> <li>Long-running batches with partial progress tracking</li> <li>Parallel or sequential execution depending on the operation</li> </ul> <p>Each operation may:</p> <ul> <li>Succeed or fail independently</li> <li>Run in parallel or sequence</li> <li>Be cancelled mid-flight</li> <li>Report partial progress and aggregated errors</li> </ul>"},{"location":"structural/composite/#without-composite","title":"Without Composite","text":"<p>You end up with special-case orchestration logic everywhere:</p> <pre><code># \u274c Hard-coded orchestration logic\ndef bulk_update():\n    user_result = update_user()\n    if not user_result.success:\n        return failure()\n\n    inventory_result = update_inventory()\n    billing_result = update_billing()\n\n    if inventory_result.failed or billing_result.failed:\n        return partial_failure()\n</code></pre> <p>Problems:</p> <ul> <li>Tight coupling between orchestration and execution logic</li> <li>No recursive composition</li> <li>Hard to parallelize</li> <li>Difficult to track progress or aggregate failures</li> <li>Adding nested workflows becomes unmanageable</li> </ul>"},{"location":"structural/composite/#solution","title":"Solution","text":"<p>Use the Composite Pattern to model operations as a tree:</p> <ul> <li>LeafOperation \u2192 Executes a single RPC / task</li> <li>CompositeOperation \u2192 Executes and aggregates child operations</li> <li>Operation interface \u2192 Uniform execution, cancellation, and status tracking</li> </ul> <p>This allows:</p> <ul> <li>Recursive composition</li> <li>Parallel or sequential execution</li> <li>Aggregated success/failure</li> <li>Partial execution and progress reporting</li> <li>Transparent orchestration</li> </ul>"},{"location":"structural/composite/#core-design","title":"Core Design","text":"<pre><code>from abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import List, Optional\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\n</code></pre>"},{"location":"structural/composite/#operation-status","title":"Operation Status","text":"<pre><code>class OperationStatus(Enum):\n    PENDING = \"PENDING\"\n    IN_PROGRESS = \"IN_PROGRESS\"\n    SUCCESS = \"SUCCESS\"\n    FAILURE = \"FAILURE\"\n</code></pre>"},{"location":"structural/composite/#operation-result","title":"Operation Result","text":"<pre><code>class OperationResult:\n    def __init__(self, status=OperationStatus.PENDING, errors=None):\n        self.status = status\n        self.errors = errors if errors else []\n\n    @property\n    def is_complete(self):\n        return self.status in {OperationStatus.SUCCESS, OperationStatus.FAILURE}\n</code></pre>"},{"location":"structural/composite/#component-interface","title":"Component Interface","text":"<pre><code>class Operation(ABC):\n    @abstractmethod\n    def execute(self) -&gt; OperationResult:\n        pass\n\n    @abstractmethod\n    def cancel(self) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_status(self) -&gt; OperationStatus:\n        pass\n</code></pre>"},{"location":"structural/composite/#leaf-operation","title":"Leaf Operation","text":"<p>Represents a single RPC / microservice call.</p> <pre><code>class LeafOperation(Operation):\n    def __init__(self, name: str, payload: dict, fail: bool = False):\n        self.name = name\n        self.payload = payload\n        self.fail = fail\n        self.result = OperationResult()\n\n    def execute(self) -&gt; OperationResult:\n        try:\n            self.result.status = OperationStatus.IN_PROGRESS\n            time.sleep(0.5)  # simulate RPC latency\n\n            if self.fail:\n                raise Exception(f\"{self.name} failed\")\n\n            self.result.status = OperationStatus.SUCCESS\n            return self.result\n\n        except Exception as e:\n            self.result.status = OperationStatus.FAILURE\n            self.result.errors.append(e)\n            return self.result\n\n    def cancel(self):\n        if self.result.status == OperationStatus.IN_PROGRESS:\n            self.result.status = OperationStatus.FAILURE\n\n    def get_status(self):\n        return self.result.status\n</code></pre>"},{"location":"structural/composite/#composite-operation","title":"Composite Operation","text":"<p>Executes and aggregates child operations.</p> <pre><code>class CompositeOperation(Operation):\n    def __init__(self, name: str, children: Optional[List[Operation]] = None, parallel=False):\n        self.name = name\n        self.children = children or []\n        self.parallel = parallel\n        self.result = OperationResult()\n\n    def add_operation(self, operation: Operation):\n        self.children.append(operation)\n</code></pre>"},{"location":"structural/composite/#sequential-execution","title":"Sequential Execution","text":"<pre><code>    def _execute_sequential(self):\n        self.result.status = OperationStatus.IN_PROGRESS\n        success = True\n\n        for child in self.children:\n            result = child.execute()\n            if result.status == OperationStatus.FAILURE:\n                success = False\n                self.result.errors.extend(result.errors)\n\n        self.result.status = OperationStatus.SUCCESS if success else OperationStatus.FAILURE\n        return self.result\n</code></pre>"},{"location":"structural/composite/#parallel-execution","title":"Parallel Execution","text":"<pre><code>    def _execute_parallel(self):\n        self.result.status = OperationStatus.IN_PROGRESS\n        success = True\n\n        with ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [executor.submit(child.execute) for child in self.children]\n\n            for future in as_completed(futures):\n                result = future.result()\n                if result.status == OperationStatus.FAILURE:\n                    success = False\n                    self.result.errors.extend(result.errors)\n\n        self.result.status = OperationStatus.SUCCESS if success else OperationStatus.FAILURE\n        return self.result\n</code></pre>"},{"location":"structural/composite/#unified-execution-api","title":"Unified Execution API","text":"<pre><code>    def execute(self):\n        return (\n            self._execute_parallel()\n            if self.parallel\n            else self._execute_sequential()\n        )\n\n    def cancel(self):\n        for child in self.children:\n            child.cancel()\n        self.result.status = OperationStatus.FAILURE\n\n    def get_status(self):\n        if all(c.get_status() == OperationStatus.SUCCESS for c in self.children):\n            return OperationStatus.SUCCESS\n        if any(c.get_status() == OperationStatus.FAILURE for c in self.children):\n            return OperationStatus.FAILURE\n        if any(c.get_status() == OperationStatus.IN_PROGRESS for c in self.children):\n            return OperationStatus.IN_PROGRESS\n        return OperationStatus.PENDING\n</code></pre>"},{"location":"structural/composite/#progress-tracking","title":"Progress Tracking","text":"<pre><code>    def get_progress(self) -&gt; float:\n        if not self.children:\n            return 100.0\n        completed = sum(1 for c in self.children if c.result.is_complete)\n        return (completed / len(self.children)) * 100\n</code></pre>"},{"location":"structural/composite/#usage-example-bulk-update-workflow","title":"Usage Example: Bulk Update Workflow","text":"<pre><code>bulk_update = CompositeOperation(\n    name=\"bulk-update\",\n    parallel=True,\n    children=[\n        LeafOperation(\"user-service\", {\"user_id\": 1}),\n        LeafOperation(\"inventory-service\", {\"sku\": \"A1\"}, fail=True),\n        LeafOperation(\"billing-service\", {\"invoice\": 123}),\n    ],\n)\n\nbulk_update.execute()\n\nprint(\"Status:\", bulk_update.get_status().value)\nprint(\"Progress:\", bulk_update.get_progress(), \"%\")\nprint(\"Errors:\", bulk_update.result.errors)\n</code></pre>"},{"location":"structural/composite/#recursive-composition-nested-workflows","title":"Recursive Composition (Nested Workflows)","text":"<pre><code>billing_flow = CompositeOperation(\n    \"billing-flow\",\n    children=[\n        LeafOperation(\"invoice\"),\n        LeafOperation(\"payment\"),\n    ]\n)\n\nroot = CompositeOperation(\n    \"root-workflow\",\n    parallel=True,\n    children=[\n        LeafOperation(\"user-service\"),\n        billing_flow,\n        LeafOperation(\"notification-service\"),\n    ],\n)\n\nroot.execute()\n</code></pre>"},{"location":"structural/composite/#concurrency-orchestration","title":"Concurrency &amp; Orchestration","text":"<ul> <li>Parallel execution via <code>ThreadPoolExecutor</code></li> <li> <p>Compatible with:</p> </li> <li> <p>Worker pools</p> </li> <li>Background job schedulers</li> <li>Async orchestration layers</li> <li>Supports best-effort execution (all children run even if some fail)</li> </ul>"},{"location":"structural/composite/#testing-example","title":"Testing Example","text":"<pre><code>def test_partial_failure():\n    op = CompositeOperation(\n        \"test\",\n        children=[\n            LeafOperation(\"ok\"),\n            LeafOperation(\"fail\", fail=True),\n        ],\n    )\n\n    result = op.execute()\n    assert result.status == OperationStatus.FAILURE\n    assert len(result.errors) == 1\n</code></pre>"},{"location":"structural/composite/#benefits","title":"Benefits","text":"Pros Cons Uniform interface for simple &amp; complex operations More abstraction Recursive composition Requires careful state handling Parallel &amp; sequential execution Thread cancellation is cooperative Aggregated errors Retry logic not built-in Progress tracking"},{"location":"structural/composite/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Retry Decorator for operations</li> <li>Fail-fast vs best-effort policies</li> <li>Compensation / rollback operations</li> <li>Timeout-aware execution</li> <li>Tracing &amp; metrics decorators</li> <li>Async (<code>asyncio</code>) implementation</li> </ul>"},{"location":"structural/composite/#when-to-use-composite","title":"When to Use Composite","text":"<p>\u2705 Use when:</p> <ul> <li>Operations naturally form trees</li> <li>You need uniform handling of single and grouped actions</li> <li>Orchestration logic must remain clean</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Execution order is strictly linear</li> <li>No hierarchical grouping exists</li> </ul>"},{"location":"structural/decorator/","title":"Decorator Pattern \u2013 Cross-Cutting Features for API Clients","text":""},{"location":"structural/decorator/#problem","title":"Problem","text":"<p>You want to add cross-cutting features to API clients like <code>ParagoNClient</code> without modifying their code. Typical features include:</p> <ul> <li>\ud83d\udd01 Retries</li> <li>\ud83e\uddf5 Tracing</li> <li>\ud83d\udcca Metrics / Logging</li> </ul>"},{"location":"structural/decorator/#challenges","title":"Challenges","text":"<ul> <li>Features must be chainable and order-sensitive</li> <li>Wrapper overhead must be minimal for high-frequency calls</li> <li>Ability to enable/disable features via configuration</li> </ul>"},{"location":"structural/decorator/#without-decorators","title":"\u274c Without Decorators","text":"<p>Embedding retries or tracing directly in the client:</p> <pre><code>class ParagoNClient:\n    def get_user(self, user_id):\n        print(\"Tracing start\")\n        for attempt in range(3):\n            try:\n                print(\"Calling API\")\n                return {\"user_id\": user_id}\n            except Exception:\n                print(\"Retrying...\")\n        print(\"Tracing end\")\n</code></pre> <p>Problems:</p> <ul> <li>Violates Single Responsibility Principle</li> <li>Hard to test or extend</li> <li>Cannot selectively enable/disable features</li> </ul>"},{"location":"structural/decorator/#solution-decorator-pattern","title":"Solution: Decorator Pattern","text":"<p>Wrap the client with decorators for each feature:</p> <pre><code>ParagoNClient\n     \u2193\nRetryingClient\n     \u2193\nTracingClient\n     \u2193\nApplication Code\n</code></pre>"},{"location":"structural/decorator/#advantages","title":"Advantages:","text":"<ul> <li>Each feature is isolated</li> <li>Decorators are composable</li> <li>Order can change behavior (e.g., trace every retry vs retry traced calls)</li> <li>Supports configuration-driven enable/disable</li> </ul>"},{"location":"structural/decorator/#implementation","title":"Implementation","text":""},{"location":"structural/decorator/#base-client","title":"Base Client","text":"<pre><code>class ParagoNClient:\n    def get_user(self, user_id: str) -&gt; dict:\n        print(f\"Fetching user {user_id}\")\n        return {\"user_id\": user_id, \"name\": \"John Doe\"}\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        print(f\"Updating user {user_id}\")\n        return True\n</code></pre>"},{"location":"structural/decorator/#base-decorator","title":"Base Decorator","text":"<pre><code>class BaseDecorator:\n    def __init__(self, client: ParagoNClient):\n        self.client = client\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.client.get_user(user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.client.update_user(user_id, data)\n</code></pre>"},{"location":"structural/decorator/#retrying-decorator","title":"Retrying Decorator","text":"<pre><code>class RetryingClient(BaseDecorator):\n    def __init__(self, client: ParagoNClient, retries: int = 3):\n        super().__init__(client)\n        self.retries = retries\n\n    def retry_func(self, func, *args, **kwargs):\n        for attempt in range(self.retries):\n            try:\n                print(f\"Attempt {attempt+1} for {func.__name__}\")\n                if random.random() &lt; 0.3:  # simulate failure\n                    raise Exception(\"Simulated network error\")\n                return func(*args, **kwargs)\n            except Exception as e:\n                print(f\"Error on attempt {attempt+1}: {e}\")\n                if attempt == self.retries - 1:\n                    raise e\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.retry_func(super().get_user, user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.retry_func(super().update_user, user_id, data)\n</code></pre>"},{"location":"structural/decorator/#tracing-decorator","title":"Tracing Decorator","text":"<pre><code>class TracingClient(BaseDecorator):\n    def __init__(self, client: ParagoNClient):\n        super().__init__(client)\n\n    def trace_func(self, func, *args, **kwargs):\n        print(f\"Tracing start: {func.__name__} with args: {args}, kwargs: {kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"Tracing end: {func.__name__} with result: {result}\")\n        return result\n\n    def get_user(self, user_id: str) -&gt; dict:\n        return self.trace_func(super().get_user, user_id)\n\n    def update_user(self, user_id: str, data: dict) -&gt; bool:\n        return self.trace_func(super().update_user, user_id, data)\n</code></pre>"},{"location":"structural/decorator/#configuration-driven-client","title":"Configuration-Driven Client","text":"<pre><code>class ParagonClientConfig:\n    def __init__(self, enable_retries=True, enable_tracing=True, retries=3):\n        self.enable_retries = enable_retries\n        self.enable_tracing = enable_tracing\n        self.retries = retries\n\n    def get_client(self) -&gt; ParagoNClient:\n        client = ParagoNClient()\n        if self.enable_retries:\n            client = RetryingClient(client, retries=self.retries)\n        if self.enable_tracing:\n            client = TracingClient(client)\n        return client\n</code></pre>"},{"location":"structural/decorator/#usage-example","title":"Usage Example","text":"<pre><code>config = ParagonClientConfig(enable_retries=True, enable_tracing=True, retries=3)\nclient = config.get_client()\n\nclient.get_user(\"12345\")\nclient.update_user(\"12345\", {\"name\": \"Jane Doe\"})\n</code></pre> <p>Order-sensitive behavior:</p> <pre><code># Trace every retry\nclient = TracingClient(RetryingClient(ParagoNClient()))\n\n# Retry traced calls\nclient = RetryingClient(TracingClient(ParagoNClient()))\n</code></pre>"},{"location":"structural/decorator/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_decorated_client():\n    config = ParagonClientConfig(True, True, retries=2)\n    client = config.get_client()\n    user_data = client.get_user(\"12345\")\n    assert user_data[\"user_id\"] == \"12345\"\n    update_status = client.update_user(\"12345\", {\"name\": \"Jane Doe\"})\n    assert update_status is True\n</code></pre> <pre><code>def test_tracing_logic(capfd):\n    client = TracingClient(ParagoNClient())\n    client.get_user(\"12345\")\n    out, _ = capfd.readouterr()\n    assert \"Tracing start\" in out\n    assert \"Tracing end\" in out\n</code></pre>"},{"location":"structural/decorator/#benefits","title":"Benefits","text":"Pros Cons No changes to core client Slight overhead per call Features are composable Some method duplication Order-sensitive behavior Requires interface consistency Easy to test Supports configuration-driven enable/disable"},{"location":"structural/decorator/#summary","title":"Summary","text":"<p>This implementation provides a clean, extensible, and testable approach to adding cross-cutting features to API clients using the Decorator Pattern.</p> <p>It allows runtime composition, configurable behavior, and ensures SRP adherence.</p>"},{"location":"structural/facade/","title":"Facade Pattern","text":""},{"location":"structural/facade/#problem","title":"Problem","text":"<p>Your application includes a complex onboarding subsystem that must coordinate multiple internal and external services:</p> <ul> <li>Identity service (user creation)</li> <li>Billing service (subscription setup)</li> <li>Third-party provider (ParagoN) for provisioning</li> <li>Error handling and compensation when partial failures occur</li> <li>Retry safety for frontend and service retries</li> </ul> <p>Each onboarding attempt may:</p> <ul> <li>Call multiple subsystems in a strict order</li> <li>Partially succeed and then fail</li> <li>Be retried due to network issues or client timeouts</li> <li>Require rollback of previously completed steps</li> </ul>"},{"location":"structural/facade/#without-facade","title":"Without Facade","text":"<p>Controllers and callers end up containing orchestration logic:</p> <pre><code># \u274c Orchestration leaks into controllers\ndef onboard_user_controller(req):\n    try:\n        create_identity(req.user_id, req.email)\n        create_subscription(req.user_id, req.plan_id)\n        provision_paragon(req.user_id)\n        return success()\n    except Exception:\n        rollback_billing(req.user_id)\n        rollback_paragon(req.user_id)\n        return failure()\n</code></pre> <p>Problems:</p> <ul> <li>Controllers become complex and fragile</li> <li>Orchestration logic is duplicated across callers</li> <li>Error recovery is inconsistent</li> <li>Retry behavior is unsafe without idempotency</li> <li>Changing onboarding steps requires touching many callers</li> </ul>"},{"location":"structural/facade/#solution","title":"Solution","text":"<p>Use the Facade Pattern to provide a single high-level API for onboarding:</p> <ul> <li>OnboardingFacade \u2192 Orchestrates the full onboarding workflow</li> <li>Subsystem services \u2192 Identity, Billing, ParagoN</li> <li>Idempotency store \u2192 Ensures safe retries</li> </ul> <p>The Facade:</p> <ul> <li>Hides orchestration details</li> <li>Centralizes error handling and rollback</li> <li>Returns a simple success/failure result</li> <li>Keeps controllers and frontends thin</li> </ul>"},{"location":"structural/facade/#core-design","title":"Core Design","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Optional, Dict\n</code></pre>"},{"location":"structural/facade/#request-dto","title":"Request DTO","text":"<p>Encapsulates all data needed for onboarding.</p> <pre><code>@dataclass(frozen=True)\nclass OnboardUserRequest:\n    user_id: str\n    email: str\n    plan_id: str\n    idempotency_key: str\n</code></pre>"},{"location":"structural/facade/#result-dto","title":"Result DTO","text":"<p>Simple outcome returned to callers.</p> <pre><code>@dataclass\nclass OnboardUserResult:\n    success: bool\n    user_id: Optional[str] = None\n    error: Optional[str] = None\n</code></pre>"},{"location":"structural/facade/#subsystem-services-hidden-behind-the-facade","title":"Subsystem Services (Hidden Behind the Facade)","text":"<p>These represent internal or external dependencies. The Facade coordinates them but callers never interact with them directly.</p> <pre><code>class IdentityService:\n    def create_user(self, user_id: str, email: str) -&gt; None:\n        ...\n</code></pre> <pre><code>class BillingService:\n    def create_subscription(self, user_id: str, plan_id: str) -&gt; None:\n        ...\n    def cancel_subscription(self, user_id: str) -&gt; None:\n        ...\n</code></pre> <pre><code>class ParagoNClient:\n    def provision_account(self, user_id: str) -&gt; None:\n        ...\n    def deprovision_account(self, user_id: str) -&gt; None:\n        ...\n</code></pre>"},{"location":"structural/facade/#idempotency-store","title":"Idempotency Store","text":"<p>Ensures safe retries by caching onboarding results.</p> <pre><code>class IdempotencyStore:\n    def __init__(self):\n        self._store: Dict[str, OnboardUserResult] = {}\n\n    def get(self, key: str) -&gt; Optional[OnboardUserResult]:\n        return self._store.get(key)\n\n    def save(self, key: str, result: OnboardUserResult) -&gt; None:\n        self._store[key] = result\n</code></pre>"},{"location":"structural/facade/#facade","title":"Facade","text":"<p>Provides a single entry point for onboarding.</p> <pre><code>class OnboardingFacade:\n    \"\"\"\n    Facade that hides onboarding orchestration, retries, and recovery\n    behind a single high-level API.\n    \"\"\"\n</code></pre>"},{"location":"structural/facade/#orchestration-flow","title":"Orchestration Flow","text":"<pre><code>    def onboard_user(self, request: OnboardUserRequest) -&gt; OnboardUserResult:\n</code></pre>"},{"location":"structural/facade/#1-idempotency-check","title":"1. Idempotency Check","text":"<pre><code>cached = self.idempotency_store.get(request.idempotency_key)\nif cached:\n    return cached\n</code></pre> <ul> <li>Prevents duplicate onboarding</li> <li>Allows safe retries from frontend or services</li> </ul>"},{"location":"structural/facade/#2-execute-subsystems-in-order","title":"2. Execute Subsystems in Order","text":"<pre><code>self.identity.create_user(request.user_id, request.email)\nself.billing.create_subscription(request.user_id, request.plan_id)\nself.paragon.provision_account(request.user_id)\n</code></pre> <ul> <li>Strict sequencing</li> <li>All orchestration is hidden from callers</li> </ul>"},{"location":"structural/facade/#3-error-handling-compensation","title":"3. Error Handling &amp; Compensation","text":"<pre><code>except Exception as e:\n    self._rollback(request)\n</code></pre> <p>Rollback is best-effort and isolated:</p> <pre><code>def _rollback(self, request: OnboardUserRequest) -&gt; None:\n    self.paragon.deprovision_account(request.user_id)\n    self.billing.cancel_subscription(request.user_id)\n</code></pre>"},{"location":"structural/facade/#4-persist-result","title":"4. Persist Result","text":"<pre><code>self.idempotency_store.save(request.idempotency_key, result)\n</code></pre> <ul> <li>Guarantees consistent results on retries</li> </ul>"},{"location":"structural/facade/#usage-example","title":"Usage Example","text":""},{"location":"structural/facade/#controller-caller","title":"Controller / Caller","text":"<pre><code>result = facade.onboard_user(\n    OnboardUserRequest(\n        user_id=\"user-123\",\n        email=\"user@example.com\",\n        plan_id=\"pro\",\n        idempotency_key=\"req-001\",\n    )\n)\n\nif result.success:\n    return 200\nreturn 500\n</code></pre>"},{"location":"structural/facade/#idempotent-retry","title":"Idempotent Retry","text":"<pre><code>result = facade.onboard_user(request)  # returns cached result\n</code></pre>"},{"location":"structural/facade/#execution-characteristics","title":"Execution Characteristics","text":"<ul> <li>Sequential execution of subsystems</li> <li>Centralized rollback logic</li> <li>Idempotent retries</li> <li>Single return type for callers</li> </ul>"},{"location":"structural/facade/#testing-example","title":"Testing Example","text":"<pre><code>def test_idempotent_onboarding():\n    result1 = facade.onboard_user(request)\n    result2 = facade.onboard_user(request)\n    assert result1 == result2\n</code></pre>"},{"location":"structural/facade/#benefits","title":"Benefits","text":"Pros Cons Simple API for callers Facade can grow large Centralized orchestration Requires careful design Consistent error handling May hide subsystem details Idempotent retry support Rollbacks are best-effort Keeps controllers thin"},{"location":"structural/facade/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Persistent onboarding state machine</li> <li>Async / event-driven onboarding</li> <li>Saga-based compensation</li> <li>Retry &amp; circuit breaker decorators</li> <li>Metrics and tracing inside Facade</li> </ul>"},{"location":"structural/facade/#when-to-use-facade","title":"When to Use Facade","text":"<p>\u2705 Use when:</p> <ul> <li>Multiple subsystems must be coordinated</li> <li>Callers only care about success/failure</li> <li>Orchestration logic is growing in controllers</li> <li>You need a stable API over volatile internals</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Logic is trivial or single-service</li> <li>Callers need fine-grained control over steps</li> </ul>"},{"location":"structural/flyweight/","title":"Flyweight Pattern: Metadata Sharing in Spark","text":""},{"location":"structural/flyweight/#problem","title":"Problem","text":"<p>Your Spark cluster processes millions of records that reference a small set of schema descriptors and lookup metadata:</p> <ul> <li>Each record may carry references to the same descriptors repeatedly</li> <li>Large-scale duplication of metadata increases memory usage</li> <li>Serialization of redundant metadata adds overhead during task shipping across executors</li> <li>Immutable metadata can be shared safely, but naive instantiation wastes resources</li> </ul>"},{"location":"structural/flyweight/#without-flyweight","title":"Without Flyweight","text":"<pre><code># \u274c Every record creates a new metadata object\nfor row in rows:\n    descriptor = MetadataDescriptor(id=row.descriptor_id, fields=row.fields)\n    process(row, descriptor)\n</code></pre> <p>Problems:</p> <ul> <li>High memory consumption</li> <li>Frequent serialization/deserialization in distributed tasks</li> <li>Redundant object creation for identical descriptors</li> <li>Difficult to maintain consistent references</li> </ul>"},{"location":"structural/flyweight/#solution","title":"Solution","text":"<p>Use the Flyweight Pattern to share immutable metadata objects across tasks:</p> <ul> <li>MetadataFlyweight \u2192 Represents a single, immutable descriptor object</li> <li>Central registry \u2192 Ensures one instance per descriptor id</li> <li>Thread-safe creation \u2192 Prevents race conditions when registering flyweights</li> <li>Distributed-safe serialization \u2192 Reuse the same metadata instance during pickling where possible</li> </ul> <p>This allows:</p> <ul> <li>Minimal memory footprint</li> <li>Fast serialization</li> <li>Safe sharing across threads and tasks</li> <li>Centralized management of metadata objects</li> </ul>"},{"location":"structural/flyweight/#core-design","title":"Core Design","text":"<pre><code>from threading import Lock\n</code></pre>"},{"location":"structural/flyweight/#flyweight-class","title":"Flyweight Class","text":"<pre><code>class MetadataFlyweight:\n    _registry = {}\n    _lock = Lock()\n</code></pre> <ul> <li><code>_registry</code> \u2192 stores unique flyweight instances keyed by <code>descriptor_id</code></li> <li><code>_lock</code> \u2192 ensures thread-safe creation</li> </ul>"},{"location":"structural/flyweight/#object-creation-new","title":"Object Creation (new)","text":"<pre><code>def __new__(cls, descriptor_id: str, **attributes):\n    with cls._lock:\n        if descriptor_id not in cls._registry:\n            instance = super(MetadataFlyweight, cls).__new__(cls)\n            instance.descriptor_id = descriptor_id\n            instance.attributes = attributes\n            cls._registry[descriptor_id] = instance\n        return cls._registry[descriptor_id]\n</code></pre> <p>Highlights:</p> <ul> <li>Ensures singleton per descriptor id</li> <li>Safe to use in multi-threaded environments</li> <li>Additional attributes are stored only once</li> <li>Returns existing object if descriptor already exists</li> </ul>"},{"location":"structural/flyweight/#representation","title":"Representation","text":"<pre><code>def __repr__(self):\n    return f\"MetadataFlyweight(id={self.descriptor_id}, attributes={self.attributes})\"\n</code></pre> <ul> <li>Useful for debugging and logging</li> <li>Shows descriptor id and attributes of shared object</li> </ul>"},{"location":"structural/flyweight/#usage-example-spark-pipeline","title":"Usage Example: Spark Pipeline","text":"<pre><code># Imagine multiple tasks processing rows with the same descriptor\ntasks = [\n    {\"descriptor_id\": \"user\", \"fields\": [\"id\", \"name\"]},\n    {\"descriptor_id\": \"order\", \"fields\": [\"id\", \"amount\"]},\n    {\"descriptor_id\": \"user\", \"fields\": [\"id\", \"name\"]},\n]\n\nflyweights = [MetadataFlyweight(**t) for t in tasks]\n\nfor fw in flyweights:\n    print(fw)\n</code></pre> <p>Output:</p> <pre><code>MetadataFlyweight(id=user, attributes={'fields': ['id', 'name']})\nMetadataFlyweight(id=order, attributes={'fields': ['id', 'amount']})\nMetadataFlyweight(id=user, attributes={'fields': ['id', 'name']})\n</code></pre> <p>Notice that the <code>user</code> descriptor is shared across tasks.</p>"},{"location":"structural/flyweight/#benefits","title":"Benefits","text":"Pros Cons Reduces memory usage by sharing objects Requires careful immutability management Faster serialization/deserialization Central registry can become a bottleneck Thread-safe creation Slight overhead on first creation Simplifies metadata consistency Not suitable for mutable objects"},{"location":"structural/flyweight/#advanced-considerations","title":"Advanced Considerations","text":"<ul> <li>Pickling in Spark: Ensure descriptors are pickle-friendly; avoid complex references that cannot be serialized</li> <li>Distributed caching: Could extend the registry to use a distributed cache (Redis, Broadcast variables) for cross-node sharing</li> <li>Lazy attribute computation: Flyweight can defer heavy initialization until first access</li> <li>Immutable objects: Flyweights must remain immutable to safely share across threads and tasks</li> </ul>"},{"location":"structural/flyweight/#when-to-use-flyweight","title":"When to Use Flyweight","text":"<p>\u2705 Use when:</p> <ul> <li>Many objects share identical read-only metadata</li> <li>Memory usage or serialization overhead is significant</li> <li>You want centralized management for consistent references</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>Objects are mutable or change per task</li> <li>Metadata is unique per record or rarely reused</li> <li>Overhead of registry outweighs memory savings</li> </ul>"},{"location":"structural/proxy/","title":"Proxy Pattern: HTTP API Client with Caching, Rate-Limiting, and Circuit Breaker","text":""},{"location":"structural/proxy/#problem","title":"Problem","text":"<p>Your HTTP API client (<code>ParagoNClient</code>) is used in edge services and frontends to fetch and update user data:</p> <ul> <li>Repeated API calls for the same user lead to redundant network requests</li> <li>High request rates can exceed third-party API limits</li> <li>Transient API failures may propagate to clients, causing downtime</li> <li>Managing caching, rate-limits, and circuit-breaking separately makes code complex and scattered</li> </ul>"},{"location":"structural/proxy/#without-proxy","title":"Without Proxy","text":"<pre><code># \u274c Direct client calls without policies\nuser = client.get_user(\"user123\")\nclient.update_user(\"user123\", {\"plan\": \"pro\"})\n</code></pre> <p>Problems:</p> <ul> <li>No caching \u2192 repeated fetches always hit the API</li> <li>No rate-limiting \u2192 risk of hitting API quotas</li> <li>No circuit breaker \u2192 failures propagate to callers</li> <li>Hard to add new policies without modifying client</li> </ul>"},{"location":"structural/proxy/#solution","title":"Solution","text":"<p>Use the Proxy Pattern to wrap the original client and provide additional behaviors transparently:</p> <ul> <li>Caching \u2192 store recent API responses to reduce repeated network calls</li> <li>Rate-limiting \u2192 prevent excessive API requests</li> <li>Circuit breaker \u2192 stop requests when failures exceed a threshold and allow recovery after cooldown</li> <li>Transparent interface \u2192 same method signatures as <code>ParagoNClient</code>, so callers can swap easily</li> </ul> <p>This allows:</p> <ul> <li>Centralized policy enforcement</li> <li>Reduced API calls and latency</li> <li>Safe handling of transient failures</li> <li>Simplified client usage for frontends</li> </ul>"},{"location":"structural/proxy/#core-design","title":"Core Design","text":"<pre><code>import time\n</code></pre>"},{"location":"structural/proxy/#proxy-class","title":"Proxy Class","text":"<pre><code>class ParagoNClientProxy:\n    def __init__(self, client: ParagoNClient, cache_ttl: int = 60,\n                 rate_limit: int = 10, breaker_threshold: int = 5):\n        self.client = client\n        self.cache = {}\n        self.cache_ttl = cache_ttl\n        self.rate_limit = rate_limit\n        self.breaker_threshold = breaker_threshold\n        self.failure_count = 0\n        self.last_failure_time = None\n</code></pre> <ul> <li><code>client</code> \u2192 the real API client being wrapped</li> <li><code>cache</code> \u2192 stores cached responses keyed by user ID</li> <li><code>cache_ttl</code> \u2192 cache expiration time in seconds</li> <li><code>rate_limit</code> \u2192 max number of concurrent requests in the cache</li> <li><code>breaker_threshold</code> \u2192 number of failures before opening the circuit</li> </ul>"},{"location":"structural/proxy/#caching-logic","title":"Caching Logic","text":"<pre><code>if user_id in self.cache:\n    cached_entry = self.cache[user_id]\n    if current_time - cached_entry['timestamp'] &lt; self.cache_ttl:\n        print(f\"Returning cached data for user {user_id}\")\n        return cached_entry['data']\n</code></pre> <ul> <li>Reduces repeated API calls for the same user</li> <li>TTL ensures cache freshness</li> </ul>"},{"location":"structural/proxy/#rate-limiting-logic","title":"Rate-Limiting Logic","text":"<pre><code>if len(self.cache) &gt;= self.rate_limit:\n    raise Exception(\"Rate limit exceeded. Try again later.\")\n</code></pre> <ul> <li>Simple policy based on number of cached entries</li> <li>Prevents excessive load on the API</li> </ul>"},{"location":"structural/proxy/#circuit-breaker-logic","title":"Circuit Breaker Logic","text":"<pre><code>if self.failure_count &gt;= self.breaker_threshold:\n    if current_time - self.last_failure_time &lt; 60:  # cooldown\n        raise Exception(\"Circuit breaker is open. Request blocked.\")\n    else:\n        self.failure_count = 0  # reset after cooldown\n</code></pre> <ul> <li>Stops requests when failures exceed threshold</li> <li>Allows recovery after a cooldown period</li> </ul>"},{"location":"structural/proxy/#unified-proxy-api","title":"Unified Proxy API","text":"<pre><code>def get_user(self, user_id: str) -&gt; dict:\n    # Implements caching, rate-limiting, and circuit breaker\n    ...\n\ndef update_user(self, user_id: str, data: dict) -&gt; bool:\n    # Implements circuit breaker and invalidates cache on update\n    ...\n</code></pre> <ul> <li>Methods mirror the real client API</li> <li>Transparent for callers</li> </ul>"},{"location":"structural/proxy/#usage-example","title":"Usage Example","text":"<pre><code>client = ParagoNClient()\nproxy = ParagoNClientProxy(client, cache_ttl=60, rate_limit=10, breaker_threshold=5)\n\nuser = proxy.get_user(\"user123\")  # Fetches from API\nuser_cached = proxy.get_user(\"user123\")  # Returns cached\nproxy.update_user(\"user123\", {\"plan\": \"pro\"})  # Invalidates cache\n</code></pre>"},{"location":"structural/proxy/#testing-example-fast-circuit-breaker","title":"Testing Example (Fast Circuit Breaker)","text":"<pre><code># Test caching, rate-limiting, and circuit breaker without real waits\ntest_proxy_fast_circuit_breaker()\n</code></pre> <ul> <li>Uses cache TTL of 2s and breaker threshold of 2</li> <li>Simulates circuit breaker cooldown by fast-forwarding last_failure_time</li> <li>Tests all proxy behaviors deterministically</li> </ul>"},{"location":"structural/proxy/#benefits","title":"Benefits","text":"Pros Cons Transparent API proxy Adds some overhead per call Reduces redundant network requests Simple rate-limiting logic only Protects against cascading failures Circuit breaker cooldown is fixed Centralizes policy enforcement Not suitable for complex policies"},{"location":"structural/proxy/#advanced-extensions","title":"Advanced Extensions","text":"<ul> <li>Configurable cache backends (Redis, memcached)</li> <li>Pluggable rate-limiting policies (token bucket, leaky bucket)</li> <li>Metrics collection (success/failure counts, cache hits/misses)</li> <li>Async-friendly implementation using <code>asyncio</code></li> <li>Dynamic circuit breaker policies per endpoint</li> </ul>"},{"location":"structural/proxy/#when-to-use-proxy","title":"When to Use Proxy","text":"<p>\u2705 Use when:</p> <ul> <li>You want to add cross-cutting concerns like caching, throttling, or fault-tolerance</li> <li>You want transparent client swapping without changing callers</li> <li>You need centralized handling of third-party API calls</li> </ul> <p>\u274c Avoid when:</p> <ul> <li>No additional behavior is required beyond the client</li> <li>Overhead of proxy outweighs benefits for low-traffic services</li> </ul>"}]}